```{r, include=FALSE}
set.seed(42)
```

# Dispersion models

Modelling dispersion means that we describe how the expected variance changes as a function of the mean or predictor variables. The residual problem that we address by that is know as heteroskedasticity.

## Heteroskedasticity in the linear model

The easiest way to understand heteroskedasticity is in the linear model. Let's look at an extreme example that we create by simulating some data

```{r chunk_chapter4_chunk8, echo=TRUE, eval=TRUE}
set.seed(125)

data = data.frame(treatment = factor(rep(c("A", "B", "C"), each = 15)))
data$observation = c(7, 2 ,4)[as.numeric(data$treatment)] +
  rnorm( length(data$treatment), sd = as.numeric(data$treatment)^2 )
boxplot(observation ~ treatment, data = data)
```

Especially p-values and confidence intervals of `lm()`{.R} and ANOVA can react quite strongly to such differences in residual variation. So, running a standard `lm()`{.R} / ANOVA on this data is not a good idea - in this case, we see that all regression effects are not significant, as is the ANOVA, suggesting that there is no difference between groups.

```{r chunk_chapter4_chunk9, echo=TRUE, eval=TRUE}
fit = lm(observation ~ treatment, data = data)
summary(fit)
summary(aov(fit))
```

::: callout-important
A subtlety about heteroskedasticity in the normal model is that it maily affects p-values for the variable that we are

``` r

# error sd is variable, but uncorrelated with predictor

getP <- function(effect = 0, n = 1000){
  sd = rgamma(n, shape = 2)
  #hist(sd)
  x = runif(n)
  y = effect * x + rnorm(n, sd = sd)  
  fit <- lm(y ~ x)
  table <- summary(fit)
  return(table$coefficients[2,4])
}

out = replicate(1000, getP())
hist(out, breaks = 20)


# error sd is variable and correlated with predictor (identical in this case)

getP2 <- function(effect = 0, n = 1000){
  sd = rgamma(n, shape = 2)
  y = effect * sd + rnorm(n, sd = sd)  
  fit <- lm(y ~ sd)
  table <- summary(fit)
  return(table$coefficients[2,4])
}

out2 = replicate(1000, getP2())
hist(out2, breaks = 20)
```
:::

So, what can we do?

### Transformation

One option is to search for a transformation of the response that improves the problem - If heteroskedasticity correlates with the mean value, one can typically decrease it by some sqrt or log transformation, but often difficult, because this may also conflict with keeping the distribution normal.

### Modelling the variance / dispersion

The second, more general option, is to model the variance - Modelling the variance to fit a model where the variance is not fixed. We will discuss two packages in R that allow to model the dispersion

The first (traditional) option is to use `nlme::gls`{.R}. GLS = *Generalized Least Squares*. In the `gls` function, you can specify a dependency of the residual variance on a predictor or the response via the weight argument. There are different types of dependencies that you can specify, see `?varFunc`{.R}. In our case, we will use the `varIdent` function, which allows to specify a different variance per treatment.

```{r chunk_chapter4_chunk10, echo=TRUE, eval=TRUE}
library(nlme)

fit = gls(observation ~ treatment, data = data, 
          weights = varIdent(form = ~ 1 | treatment))
summary(fit)
```

If we check the ANOVA, we see that, unlike before, we get a significant effect of the treatment

```{r chunk_chapter4_chunk11, echo=TRUE, eval=TRUE}
anova(fit)
```

The second option for modeling variances is to use the `glmmTMB` package. Here, you can specify an extra regression formula for the dispersion (= residual variance). If we fit this:

```{r chunk_chapter4_chunk12, echo=TRUE, eval=TRUE}
library(glmmTMB)

fit = glmmTMB(observation ~ treatment, data = data, 
              dispformula = ~ treatment)
summary(fit)
```

We get 2 regression tables as outputs - one for the effects, and one for the dispersion (= residual variance). We see, as expected, that the dispersion is higher in groups B and C compared to A. An advantage over gls is that we get confidence intervals and p-values for these differences on top!

::: {.callout-caution icon="false"}
#### Exercise variance modelling

Take this plot of Ozone \~ Solar.R using the airquality data. Clearly there is heteroskedasticity in the relationship:

```{r chunk_chapter4_chunk14, echo=TRUE, eval=TRUE}
plot(Ozone ~ Solar.R, data = airquality)
```

We can also see this when we fit the regression model:

```{r chunk_chapter4_chunk15, echo=TRUE, eval=F}
m1 = lm(Ozone ~ Solar.R, data = airquality)
par(mfrow = c(2, 2))
plot(m1)
```

We could of course consider other predictors, but let's say we want to fit this model specifically

1.  Try to get the variance stable with a transformation.
2.  Use the `gls` function (package `nlme`) with the untransformed response to make the variance dependent on Solar.R. Hint: Read in `varClasses` and decide how to model this.
3.  Use `glmmTMB` to model heteroskedasticity.
:::

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution

1.  **Transformation**

```{r}
m3 = lm(sqrt(Ozone) ~ Solar.R, data = airquality)
```

2.  **GLS**

```{r}
m3 = nlme::gls(Ozone ~ Solar.R, weights = varPower(form = ~Solar.R), data = airquality[complete.cases(airquality),])
```

3.  **glmmTMB**

```{r}
m4 = glmmTMB(Ozone ~ Solar.R, dispformula = ~Solar.R, data = airquality)
```
:::

## Heteroskedasticity in GLMMs

GLM(M)s can be heteroskedastic as well, i.e. dispersion depends on some predictors. In `glmmTMB`, you can make the dispersion of the negative Binomial dependent on a formula via the `dispformula` argument, in the same way as in `nlme` for the linear model.

Variance problems would show up when plotting residuals against predicted and predictors. On the previous page, we saw some variance problems in the Salamander model. We could add a variable dispersion model via

```{r chunk_chapter5_chunk21, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
m3 = glmmTMB(count ~ spp + mined + (1|site), family = nbinom1,
             dispformula = ~ spp + mined ,  data = Salamanders)
summary(m3)
library(DHARMa)
res = simulateResiduals(m3, plot = T)

par(mfrow = c(1, 2))
plotResiduals(res, Salamanders$spp)
plotResiduals(res, Salamanders$mined)
```

## Excursion: outliers, robust and quantile regression

What can we do if, after accounting for the functional relationship, response transformation and variance modelling, residual diagnostic 2 shows non-normality, in particular strong outliers? Here simulated example data with strong outliers / deviations from normality:

```{r chunk_chapter4_chunk16, echo=TRUE, eval=TRUE}
set.seed(123)

n = 100
concentration = runif(n, -1, 1)
growth = 2 * concentration + rnorm(n, sd = 0.5) +
  rbinom(n, 1, 0.05) * rnorm(n, mean = 6*concentration, sd = 6)
plot(growth ~ concentration)
```

Fitting the model, we see that the distribution is to wide:

```{r chunk_chapter4_chunk17, echo=TRUE, eval=TRUE}
fit = lm(growth ~ concentration)
par(mfrow = c(2, 2))
plot(fit)
```

What can we do to deal with such distributional problems and outliers?

-   **Removing** - Bad option, hard to defend, reviewers don't like this - if at all, better show robustness with and without outlier, but result is sometimes not robust.
-   **Change the distribution** - Fit a model with a different distribution, i.e. GLM or other.
-   **Robust regressions**.
-   **Quantile regression** - A special type of regression that does not assume a particular residual distribution.

### Robust regression

Robust methods generally refer to methods that are robust to violation of assumptions, e.g. outliers. More specifically, standard robust regressions typically downweight datap oints that have a too high influence on the fit. See <a href="https://cran.r-project.org/web/views/Robust.html" target="_blank" rel="noopener">https://cran.r-project.org/web/views/Robust.html</a> for a list of robust packages in R.

```{r chunk_chapter4_chunk18, echo=TRUE, eval=TRUE}
# This is the classic method.
library(MASS)

fit = rlm(growth ~ concentration) 
summary(fit)
# No p-values and not sure if we can trust the confidence intervals.
# Would need to boostrap by hand!

# This is another option that gives us p-values directly.
library(robustbase)

fit = lmrob(growth ~ concentration) 
summary(fit)
```

### Quantile regression

Quantile regressions don't fit a line with an error spreading around it, but try to fit a quantile (e.g. the 0.5 quantile, the median) regardless of the distribution. Thus, they work even if the usual assumptions don't hold.

```{r chunk_chapter4_chunk19, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(qgam)

dat = data.frame(growth = growth, concentration = concentration)

fit = qgam(growth ~ concentration, data = dat, qu = 0.5) 
summary(fit)
```
