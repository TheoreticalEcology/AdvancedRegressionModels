---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
set.seed(42)
```

# GL(M)Ms

## Introduction to GLMs

Generalized linear models (GLMs) extend the linear model (LM) to other (i.e. non-normal) distributions. The idea is the following:

1.  We want to keep the regression formula $y \sim f(x)$ of the lm with all it's syntax, inlcuding splines, random effects and all that. In the GLM world, this is called the "linear predictor".

2.  However, in the GLM, we now allow other residual distributions than normal, e.g. binomial, Poisson, Gamma, etc.

3.  Some families require a particular range of the predictions (e.g. binomial requires predictions between zero and one). To achieve this, we use a so-called link function to bring the results of the linear predictor to the desired range.

In R, the distribution is specified via the `family()` function, which distinguishes the glm from the lm function. If you look at the help of family, you will see that the link function is an argument of `family()`. If no link is provided, the default link for the respective family is chosen.

```{r}
?family
```

A full GLM structure is thus:

$$
y \sim family[link^{-1}(f(x) + RE)]
$$

::: callout-note
As you see, in noted the link function as $link^{-1}$ in this equation. The reason is that traditionally, the link function is applied to the left hand side of the equation, i.e.

$$
link(y) \sim x
$$

This is important to keep in mind when interpreting the names of the link function - the log link, for example, means that $log(y) = x$, which actually means that we assume $y = exp(x)$, i.e. the result of the linear predictor enters the exponential function, which assures that we have strictly positive predictions.
:::

The function $f(x)$ itself can have all components that we discussed before, in particular

-   You can add random effects as before (using functions `lme4::glmer` or `glmmTMB::glmmTMB`)

-   You can also use splines using `mgcv::gam`

## Important GLM variants

The most important are

-   Bernoulli / Binomial family = logistic regression with logit link
-   Poisson family = Poisson regression with log link
-   Beta regresion for continous 0/1 data

Of course, there are many additional distributions that you could consider for your response. Here an overview of the common choices:

```{r chunk_chapter4_0, echo=FALSE, out.width="150%", out.height="150%"}
knitr::include_graphics(c("images/linkFunctions.jpg"))
```

```{=html}
<p><small>Screenshot taken from Wikipedia: <a href="https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function</a>. Content licensed under the <a href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License" target="_blank" rel="noopener">Creative Commons Attribution-ShareAlike License 3.0</a>.</small></p>
```
### Count data - Poisson regression

The standard model for count data (1,2,3) is the Poisson regression, which implements

-   A log-link function -\> $y = exp(ax + b)$

-   A Poisson distribution

As an example, we use the data set birdfeeding from the EcoData package - the dataset consists of observations of foods given to nestlings to parents as a function of the attractiveness of the nestling.

```{r}
library(EcoData)
#str(birdfeeding)
plot(feeding ~ attractiveness, data = birdfeeding)
```

To fit a Poisson regression to this data, we use the `glm()` function, specifying `family = "poisson"`. Note again that the log link function is the default (see `?family`), so it does not have to be specified.

```{r}
fit = glm(feeding ~ attractiveness, data = birdfeeding, family = "poisson")
summary(fit)
```

The output is very similar to the `lm()`, however, as the residuals are not any more assumed to scatter normally, all statistics based on R^2^ have been replaced by the deviance (deviance = - 2 logL(saturated) - logL(fitted)). So, we have

-   Deviance residuals on top

-   Instead of R^2^, we get null vs. residual deviance, and AIC. Based on the deviance, we can calculate a pseudo R^2^, e.g. McFadden, which is 1-\[LogL(M)/LogL(M~0~))\]

::: callout-note
As already mentioned, the deviance is a generalization of the sum of squares and plays, for example, a similar role as the residual sum of squares in ANOVA.

The deviance of our model M~1~ is defined as $Deviance_{M_1} = 2 \cdot (logL(M_{saturated}) - logL(M_1))$

**Example:**

Deviance of M~1~

```{r}
fit1 = glm(feeding ~ attractiveness, data = birdfeeding, family = "poisson")
summary(fit1)$deviance
fitSat = glm(feeding ~ as.factor(1:nrow(birdfeeding)), data = birdfeeding, family = "poisson")
2*(logLik(fitSat) - logLik(fit1))
```

Deviance of the null model (null model is for example needed to calculate the pseudo-R^2^ ):

```{r}
summary(fit1)$null.deviance
fitNull = glm(feeding ~ 1, data = birdfeeding, family = "poisson")
2*(logLik(fitSat) - logLik(fitNull))
```

**Calculation of Pseudo-R^2^** **with deviance or logL**

Common pseudo-R^2^ such as McFadden or Nagelkerke use the the logL instead of the deviance, whereas the pseudo-R^2^ by Cohen uses the deviance. However, R^2^ calculated based on deviance or logL differs considerably, as the former is calculated in reference to the maximal achievable fit (saturated model) while the latter just compares the improvement of fit compared to the null model:

```{r}
# Based on logL
McFadden = 1-(logLik(fit1))/(logLik(fitNull))
print(McFadden)

# Based on deviance
Cohen = 1- (logLik(fitSat) - logLik(fit1)) / (logLik(fitSat) - logLik(fitNull))
print(Cohen)
```

*Note: Unfortunately, there is some confusion about the exact definition, as deviance is sometimes defined (especially outside of the context of GL(M)Ms) simply as* $Deviance = -2LogL(M_1)$
:::

If we want to calculate model predictions, we have to transform to the response scale. Here we have a log link, i.e. we have to transform with exp(linear response).

```{r chunk_chapter5_chunk17, echo=TRUE, eval=TRUE}
exp(1.47459 + 3 * 0.14794)
```

Alternatively (and preferably), you can use the predict() function with `type = "response"`

```{r}
dat = data.frame(attractiveness = 3)
predict(fit, newdata = dat) # linear predictor
predict(fit, newdata = dat, type = "response") # response scale
```

Effect plots work as before. Note that the effects package always transforms the y axis according to the link, so we have log scaling on the y axis, and the effect lines remain straight

```{r, warning=FALSE, message=FALSE}
library(effects)
plot(allEffects(fit))
```

#### Notes on the Poisson regression

**Poisson vs. log transformed count data:** For count data, even if the distribution is switched, the log link is nearly always the appropriate link function. Before GLMs were widely available, was common to it lm with log transformed counts, which is basically log link + normal distribution

**Log offset:** If there is a variable that that controls the number of counts (e.g. time, area), this variable is usually added as a offset in the following form

```{r, eval = F}
fit = glm(y ~ x + offset(log(area)), family = "poisson")
```

As the log-link connects the linear predictor as in $y = exp(x)$, and $exp(x + log(area)) = exp(x) \cdot area$, this makes the expected counts proportional to area, or whatever variable is added as a log offset.

**Interactions:** As for all GLMs with nonlinear link functions, interpretation of the interactions is more complicated. See notes on this below.

### 0/1 or k/n data - logistic regression

The standard model to fit binomial (0/1 or k/n) data is the logistic regression, which combines the binomial distribution with a logit link function. To get to know this model, let's have a look at the titanic data set in EcoData:

```{r}
library(EcoData)
#str(titanic)
#mosaicplot( ~ survived + sex + pclass, data = titanic)
titanic$pclass = as.factor(titanic$pclass)
```

We want to analyze how survival in the titanic accident depended on other predictors. We could fit an lm, but the residual checks make it very evident that the data with a 0/1 response don't fit to the assumption of an lm:

```{r}
fit = lm(survived ~ sex * age, data = titanic)
summary(fit)
par(mfrow = c(2, 2))
plot(fit)
```

Thus, let's move to the logistic regression, which assumes a 0/1 response + logit link. In principle, this is distribution is called Bernoulli, but in R both 0/1 and k/n are called "binomial", as Bernoulli is the special case of binomial where n = 1.

```{r}
m1 = glm(survived ~ sex*age, family = "binomial", data = titanic)
summary(m1)
```

::: callout-note
The syntax here is for 0/1 data. If you have k/n data, you can either specify the response as cbind(k, n-k), or you can fit the glm with k \~ x, weights = n
:::

The interpretation of the regression table remains unchanged. To transform to predictions, we have to use the inverse logit, which is in R:

```{r chunk_chapter5_chunk5, echo=TRUE, eval=TRUE}
plogis(0.493381 + 0.022516 * 20)  # Women, age 20.
plogis(0.493381 -1.154139 + 20*(0.022516-0.046276)) # Men, age 20
```

Alternatively, we can again use the predict function

```{r}
newDat = data.frame(sex = as.factor(c("female", "male")), age = c(20,20))
predict(m1, newdata = newDat) # Linear predictor.
predict(m1, newdata = newDat, type = "response")  # Response scale.
```

Finally, the effect plots - note again the scaling of the y axis, which is now logit.

```{r}
library(effects)
plot(allEffects(m1))
```

If you do an ANOVA on a glm, you should take care that you perform a Chisq and not an F-test. You notice that you have the right one if the ANOVA doesn't supply SumSq statistics, but deviance. In the anova function, we have to set this by hand

```{r}
anova(m1, test = "Chisq")
```

note that anova works for glm and glmmTMB, but not for lme4::glmer.

The function car::Anova works for all models and uses a ChiSq test automatically. Given that you probably want to use a type II or III Anova anyway, you should prefer it.

```{r}
car::Anova(m1)
```

Of course, when using this with random effects, the caveats that we discussed when introducing random effects apply: this function does not account for changes in the degrees of freedom created by changing the fixed effect structure. Something like the lmerTest package which uses a df approximation does not exist for GLMMs. Thus, the values that you obtain here are calculated under the assumption that the RE structure is the same.

If you see large changes in your RE structure, or if you want to select on the REs, you can move to a simulated LRT.

#### Notes on the logistic regression

**Offset:** there is no exact solution for making 0/1 data dependent on a scaling factor via an offset, which is often desirable, for example in the context of survival analysis with different exposure times. An approximate solution is to use an offset together with the log-log link (instead of logit).

**Interactions:** As for all GLMs with nonlinear link functions, interpretation of the interactions is more complicated. See notes in this below.

**Overdispersion:** 0/1 poisson responses cannot be overdispersed, but k/n responses can be. However, 0/1 responses can show overdispersion if grouped to k/n. Note next section on residual checks, as well as comments on testing binomial GLMs in the\
<a href="https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html#binomial-data" target="_blank" rel="noopener">DHARMa vignette</a>.

::: {.callout-caution icon="false"}
#### Example - Elk Data

You will be given a data set of habitat use of Elks in Canada. Measured is the presence of Elks (0/1), and a number of other predictors. Description of variables:

-   dist_roads - distance of the location to the next road

-   NDVI - normalized difference vegetation index, essentially greeness of vegetation on the site

-   ruggedness of the terrain

-   dem - digital eleveation model = elevation above sea level

-   presence - presence of the elk

-   habitat - open or forest

Perform either:

a)  A predictive analysis, i.e. a model to predict where Elks can be found.
b)  A causal analysis, trying to understand the effect of roads on Elk presence.
:::

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution

A. Predictive analysis

```{r}
load(file = "hiddenData/elk_data.RData")

library(MASS)
fit <- glm(presence ~ dist_roads  + dem + ruggedness, data = elk_data, family = "binomial")
predictive_model = MASS::stepAIC(fit, direction = "both")
summary(predictive_model)

```

B. Causal analysis

The predictive model has actually dropped the variable of interest (distance to roads) which shows the risks of tools that select for the best predictive model such as AIC selection: Collinear variables that we need to adjust our effects, are often dropped.

For the causal model, we really need to think about the causal relationships between the variables:

We are interested in the effect of dist_roads on presence:

```{r}
summary(glm(presence ~ dist_roads, data = elk_data, family = "binomial"))
```

Positive effect of dist_roads on elk, or in other words, more elks closer to the roads? Does that make sense? No, we expect a negative effect!

Altitude (dem) and the ruggedness probably affect both variables, presence and dist_roads, and thus they should be considered as confounders:

```{r}
fit = glm(presence ~ dist_roads+ dem + ruggedness, data = elk_data, family = "binomial")
```

The effect of dist_roads is now negative!

Let's check the residuals:

```{r}
library(DHARMa)
res <- simulateResiduals(fit, plot = TRUE)
plot(res, quantreg = TRUE)
plotResiduals(res, form = elk_data$dem, quantreg = TRUE)
plotResiduals(res, form = elk_data$ruggedness, quantreg = TRUE)
```

The functional forms of our confounders are not perfect.

Since we are not really interested in them, a cool trick is to use a GAM which automatically adjusts the functional for of the fitted curve to flexibly take care of the confounders. Our main predictor dist_roads is still modelled as a linear effect.

```{r}
library(mgcv)
fit2 <- gam(presence ~ dist_roads + s(dem) + s(ruggedness), data = elk_data, family = "binomial")
summary(fit2)
```

Let's take another look at the residual plots, in particular for the confounders.

```{r}
res <- simulateResiduals(fit2, plot = TRUE)
plot(res, quantreg = TRUE)
plotResiduals(res, form = elk_data$dem, quantreg = TRUE)
plotResiduals(res, form = elk_data$ruggedness, quantreg = TRUE)
```

Now, everything looks perfect
:::

### Continous positive response - Gamma regression

```{r}
library(faraway)
```

```{r}
fit <- lm(log(resist) ~ x1 + x2 + x3 + x4, data = wafer)
summary(fit)

library(DHARMa)
simulateResiduals(fit, plot = T)
```

Alternative: Gamma regresision

```{r}
fit <- glm(formula = resist ~ x1 + x2 + x3 + x4,
           family  = Gamma(link = "log"),
           data    = wafer)
summary(fit)
```

::: callout-note
For Gamma regression, different link functions are used. The canonical link, which is also the default in R, is the inverse link 1/x. However, also log and identity link are commonly used.
:::

### Continous proportions - Beta regression and other options

We already covered discrete proportions, which can be modelled with a logistic regression. For continous proportions, this model is not suitable, because we don't know how many "trials" we have. There are a few other options to model this data, in particular the beta regression. Let's have a look.

There are two main ways to fit this data:

1.  Transform the response, or fit the GLM with a logit link or the arcsine transformation on the response
2.  A Beta regression

For further options, see [here](https://github.com/florianhartig/Statistics/blob/master/Examples/ProportionalData.md). I would generally recommend that the Beta regression is currently the preferred way and the first thing I would try. A good way to fit it is the package glmmTMB.

::: callout-tip
#### Case study

Here a case study, using the EcoData dataset

```{r}
?elemental
```

traditional lm, arc sine transformed response of proportions, see critique in https://esajournals.onlinelibrary.wiley.com/doi/10.1890/10-0340.1

```{r}
m1 <- lm(N_arc ~ Year + Site , data = elemental[elemental$Species == "ABBA", ])
summary(m1)
```

Now a beta regression

```{r}
library(glmmTMB)
m2 <- glmmTMB(N_dec ~ Year + Site, family = beta_family, data = elemental[elemental$Species == "ABBA", ] )
summary(m2)
```
:::

## Residual and their solutions in GLMs

First of all: everything we said about model selection and residual checks for LMs also apply for GLMs, with only very few additions, so you should check your model in principle as before. However, there are a few tweaks you have to be aware of.

Let's look again at the titanic example

```{r}
m1 = glm(survived ~ sex*age, family = "binomial", data = titanic)
```

How can we check the residuals of this model? Due to an unfortunate programming choice in R (Nerds: Check `class(m1)`), the standard residual plots still work

```{r chunk_chapter5_chunk10, echo=TRUE, eval=TRUE}
par(mfrow = c(2, 2))
plot(m1)
```

but they look horrible, because they still check for normality of the residuals, while we are interested in the question of whether the residuals are binomially distributed.

### DHARMA residual plots for GL(M)Ms

The `DHARMa` package that we already introduced solves this problem

```{r chunk_chapter5_chunk12, echo=TRUE, eval=TRUE}
library(DHARMa)
res = simulateResiduals(m1)
```

Standard plot:

```{r chunk_chapter5_chunk13, echo=TRUE, eval=TRUE}
plot(res)
```

Out of the help page: The function creates a plot with two panels. The left panel is a uniform Q-Q plot (calling <a href="https://rdrr.io/cran/DHARMa/man/plotQQunif.html" target="_blank" rel="noopener">plotQQunif</a>), and the right panel shows residuals against predicted values (calling <a href="hhttps://rdrr.io/cran/DHARMa/man/plotResiduals.html" target="_blank" rel="noopener">plotResiduals</a>), with outliers highlighted in red.

Very briefly, we would expect that a correctly specified model shows:

a)  A straight 1-1 line, as well as not significant of the displayed tests in the Q-Q-plot (left) -\> Evidence for a correct overall residual distribution (for more details on the interpretation of this plot, see help).

b)  Visual homogeneity of residuals in both vertical and horizontal direction, as well as no significance of quantile tests in the Residual vs. predicted plot (for more details on the interpretation of this plot, see help).

Deviations from these expectations can be interpreted similarly to a linear regression. See the vignette for detailed examples.

With that in mind, we can say that there is nothing special to see here. Also residuals against predictors shows no particular problem:

```{r chunk_chapter5_chunk14, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
par(mfrow = c(1, 2))
plotResiduals(m1, form = model.frame(m1)$age)
plotResiduals(m1, form = model.frame(m1)$sex)
```

However, residuals against the missing predictor pclass show a clear problem:

```{r chunk_chapter5_chunk15, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
dataUsed = as.numeric(rownames(model.frame(m1)))
plotResiduals(m1, form = titanic$pclass[dataUsed])
```

Thus, I should add passenger class to the model

```{r}
m2 = glm(survived ~ sex*age + pclass, family = "binomial", data = titanic)
summary(m2)

plotResiduals(m2, form = model.frame(m2)$pclass)
```

Now, residuals look fine. Of course, if your model gets more complicated, you may want to do additional checks, for example for the distribution of random effects etc.

### Dispersion Problems in GLMs

One thing that is different between GLMs and LM is that GLMs can display overall dispersion problems. The most common GLMs to show overdispersion are the Poisson and the logistic regression.

The reason is that simple GLM distributions such as the Poisson or the Binomial (for k/n data) do not have a parameter for adjusting the spread of the observed data around the regression line (dispersion), but their variance is a fixed as function of the mean.

There are good reasons for why this is the case (Poisson and Binomial describe particular processes, e.g. coin flip, for which the variance is a fixed function of the mean), but the fact is that when applying these GLMs on real data, we often find overdispersion (more dispersion than expected), and more rarely, underdispersion (less dispersion than expected).

To remove the assumptions of a fixed dispersion, there are three options, of which you should definitely take the third one:

1.  **Quasi-distributions**, which are available in glm. Those add a term to the likelihood that corrects the p-values for the dispersion, but they are not distributions .-\> Can't check residuals, no AIC. -\> Discouraged.
2.  **Observation-level random effect (OLRE)** - Add a separate random effect per observation. This effectively creates a normal random variate at the level of the linear predictor, increases variance on the responses.
3.  A **GLM distribution with variable dispersion**, for Poisson usually the negative binomial.

The reason why we should prefer the 3rd option is that it allows better residual checks and to model the dispersion as a function of the predictors, see next section.

::: callout-note
Overdispersion is often created by model misfit. Thus, before moving to a variable dispersion GLM, you should check for / correct model misfit.
:::

#### Recognizing overdispersion

To understand how to recognize overdispersion, let's look at an example. We'll use the Salamanders dataset from the package glmmTMB, staring with a simple Poisson glm:

```{r}
library(glmmTMB)
library(lme4)
library(DHARMa)

m1 = glm(count ~ spp + mined, family = poisson, data = Salamanders)
```

Overdispersion will be automatically highlighted in the standard DHARMa plots

```{r}
res = simulateResiduals(m1, plot = T)
```

You see the dispersion problem by:

-   Dispersion test in the left plot significant

-   QQ plot S-shaped

-   Quantile lines in the right plots outside their expected quantiles

You can get a more detailed output with the testDispersion function, which also displays the direction of the dispersion problem (over or underdispersion)

```{r}
testDispersion(res)
```

OK, often the dispersion problem is caused by structural problems. Let's add a random effect for site, which makes sense. We can do so using the function lme4::glmer, which adds the same extensions to glm as lmer adds to lm. This means we can use the usual random effect syntax we have used before.

```{r}
m2 = glmer(count ~ spp + mined + (1|site), 
           family = poisson, data = Salamanders)
```

The standard dispersion test is OK

```{r}
res = simulateResiduals(m2, plot = T)
```

But when random effects are added, you should prefer to calcualte conditional residuals, because this test is more powerful. For lme4 models, we can switch via re.form = T

```{r}
res = simulateResiduals(m2, plot = T, re.form = NULL)
```

This test shows that there is still some overdispersion. Actually, what the plots also show is heteroskedasticity, and we should probably deal with that as well, but we will only learn how in the next chapter. For now, let's switch to a negative binomial model. This could be fit with lme4, but it is more convenient to use the package glmmTMB, which has the same syntax as lme4, but more advanced options.

```{r}
m4 = glmmTMB(count ~ spp + mined + (1|site), family = nbinom2, data = Salamanders)
res = simulateResiduals(m4, plot = T)
```

Unfortunately, glmmTMB doesn't allow to calculate conditional residuals, so we have to be satisfied with the fact that the unconditional residuals look great.

### Zero-inflation

Another common problem in count data (Poisson / negative binomial), but also other GLMs (e.g. binomial, beta) is that the observed data has more zeros than expected by the fitted distribution. For the beta, 1-inflation, and for the k/n binomial, n-inflation is also common, and tested for / addressed in the same way.

To deal with this **zero-inflation**, one usually adds an additional model component that controls how many zeros are produced. The default way to do this is assuming two separate processes which act after one another:

1.  First, we have the normal GLM, predicting what values we would expect
2.  On top of that, we have a logistic regression, which decides whether the GLM prediction or a zero should be observed

Note that the result of 1 can also be zero, so there are two explanations for a zero in the data. Zero-inflated GLMMs can, for example, be fit with glmmTMB, using the ziformula argument.

#### Recognizing zero-inflation

::: callout-caution
The fact that you have a lot of zeros in your data does not indicate zero-inflation. Zero-inflation is with respect to the fitted model. You can only check for zero-inflation after fitting a model.
:::

Let's look at our last model - DHARMa has a special function to check for zero-inflation

```{r chunk_chapter5_chunk22, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
testZeroInflation(res)
```

This shows no sign of zero-inflation. There are, however, two problems with this test:

1.  glmmTMB models only allow unconditional residuals, which means that dispersion and zero-inflation tests are less powerfull
2.  When there is really zero-inflation, variable dispersion models such as the negative Binomial often simply increase the dispersion to account for the zeros, leading to no apparent zero-inflation in the residuals, but rather underdispersion.

Thus, for zero-inflation, model selection, or simply fitting a ZIP model is often more reliable than residual checks. You can compare a zero-inflation model via AIC or likelihood ratio test to your base model, or simply check if the ZIP term in glmmTMB is significant.

```{r}
m5 = glmmTMB(count ~ spp + mined + (1|site), family = nbinom2, ziformula = ~1,  data = Salamanders)
summary(m5)
```

In this case, we have no evidence for zero-inflation. To see an example where you can find zero-inflation, do the Owl case study below.

## Interpreting interactions in GLMs

A significant problem with interpreting GLMs is the interpretation of slopes in the presence of other variables, in particular interactions. To understand this problem, let's first confirm to ourselves: if we simulate data under the model assumptions, parameters will be recovered as expected.

```{r}
library(effects)
set.seed(123)
trt = as.factor(sample(c("ctrl", "trt"), 5000, replace= T))
concentration =  runif(5000)

response = plogis(0 + 1 * (as.numeric(trt) - 1) + 1*concentration)
survival = rbinom(5000, 1, prob = response)

dat = data.frame(trt = trt, 
                 concentration = concentration,
                 survival = survival)

m1 = glm(survival ~ trt * concentration, data = dat, family = "binomial")
summary(m1)
plot(allEffects(m1))
```

The problem with this, however, is the condition that we "simulate data under model assumptions", which includes the nonlinear link function. Let's have a look what happens if we simulate data differently: in this case, we just assume that treatment changes the overall probability of survival (from 45% to 90%), and the concentration increases the survival by up to 10% for each group. We may think that we don't have an interaction in this case, but the model finds one

```{r}
response = 0.45 * as.numeric(trt) + 0.1*concentration
survival = rbinom(5000, 1, response)

dat = data.frame(trt = trt, 
                 concentration = concentration,
                 survival = survival)

m2 = glm(survival ~ trt * concentration, 
            data = dat, family = "binomial")
summary(m2)
plot(allEffects(m2))
```

It looks in the effect plots as if the slope is changing as well, but note that this because the effect plots scale the y axis according to the link - absolutely, the effect of concentration is 10% for both groups.

The reason is simple: if we plot the plogis function, it becomes obvious that at different base levels (which would be controlled by trt in our case), moving a unit in concentration has a different effect.

```{r, echo= F}
curve(plogis, -6, 6, xlab = "concentration")
abline(v = 0)
abline(v = 1, lty = 2)
abline(v = 3, col = "red")
abline(v = 4, col = "red", lty =2)
```

If we turn this around, this means that if want the model to have the same effect of concentration at the response scale for both treatments, we must implement an interaction.

Whether this is a feature or a bug of GLMs depends a bit on the viewpoint. One could argue that, looking at survival, for example, it doesn't make sense that the concentration should have an effect of absolute 10% on top of the baseline created by trt for either 45% and 90% survival, and if we see such an effect, we should interpret this as an interaction, because relatively speaking, and increase of 45% to 55% is less important than an increase of 90% to 100%.

Still, this also means that main effects and interactions can change if you change the link function, and default links are not always natural from a biological viewpoint.

There are several ways to get better interpretable interactions. One option is that we could fit the last model with a binomial distribution, but with an identity link

```{r, warnings = F}
m3 = glm(survival ~ trt * concentration, 
            data = dat, family = binomial(link = "identity"))
summary(m3)
plot(allEffects(m3))
```

Now, we can interpret effects and interactions exactly like in a linear regression.

Another option is to look at the predicted effects at the response scale, e.g. via the effect plots, and interpret from there if we have an interaction according to what you would define as one biologically. One option to do this is the margins package.

::: callout-note
If effect directions change in sign, they will do so under any link function (as they are always monotonous), so changes in effect direction are robust to this problem.
:::

## New considerations for GLMMs

As we saw, in principle, random effects can be added to GLMs very much in the same way as before. Note, however, that there is a conceptual difference:

1.  In an LMM, we had y = f(x) + RE + residual, where both RE and residual error were normal distributions

2.  In a GLMM, we have y = dist \<- link\^-2 \<- f(x) + RE, so the normally distributed RE goes into another distribution via the link function

This has a number of consequences that may be unexpected if you don't think about it.

### Unconditinal vs. marginal predictions

One of those is that, if you have a nonlinear link function, predictions based on the fixed effects (unconditional) will not correspond to the mean of your data, or the average of the model predictions including the random effect (marginal predictions). To understand this, imagine we have a logistic regression, and our fixed effects predict a mean of 1. Then the unconditional prediction at the response scale

```{r}
plogis(1)
```

Now, let's try to see what the average (marginal) prediction over all random effects is. We add a random effect with 20 groups, sd = 1. In this case, there will be additional variation around the random intercept.

```{r}
mean(plogis(1 + rnorm(20, sd = 1)))
```

The value is considerably lower than the unconditional prediction, and this value is also what you would approximately get if you take the mean of your data.

Whether this is a problem or not is a matter of perspective. From statistical viewpoint, assuming that your model assumptions correspond to the data-generating model, there is nothing wrong with the unconditional (fixed effect) prediction not corresponding to the mean of the data. From a practical side, however, many people are unsatisfied with this, because they want to show predictions against data. Unfortunately (to my knowledge), there is no function to automatically create marginal predictions. A quick fix would be to create conditional predictions, add random effecots on top as above (with the estimated variances) and push them through the corresponding link function.

## Case Studies

### Binomial GLM - Snails

The following dataset contains information about the occurrences of 3 freshwater snail species and their infection rate with schistosomiasis (feshwater snails are intermediate hosts). The data and the analysis is from [Rabone et al. 2019](https://link.springer.com/article/10.1186/s13071-019-3745-8).

```{r chunk_chapter4_chunk1, echo=TRUE, eval=T}
library(EcoData)
?snails
```

The dataset has data on three snail species: *Biomphalaria pfeifferi* (BP), *Bulinus forskalii* (BF), and *Bulinus truncatus* (BT). For this task, we will focus only on *Bulinus truncatus* (BT).

We want to answer two questions:

1.  What are the variables that explain the occurrence of the *Bulinus truncatus* species (=Species distribution model)
2.  What are the variables that influence the infection of the snails with Schistosomiasis

Tasks:

-   Build two binomial GLMs (see hints) to explain the presence and the infection rate of *Bulinus truncatus* (BT)

-   Add environmental variables and potential confounders (see ?snails for an overview of the variables and if you want, you can also take a loot at the original paper)

-   Check residuals with DHARMa, check for misfits (plot residuals against variables)

-   Which variables explain the presence of BT and which have an effect on the infection rates?

::: callout-tip
#### Hints

Prepare dataset before the analysis:

```{r}
data = EcoData::snails
data$sTemp_Water = scale(data$Temp_Water)
data$spH = scale(data$pH)
data$swater_speed_ms = scale(data$water_speed_ms)
data$swater_depth = scale(data$water_depth)
data$sCond = scale(data$Cond)
data$swmo_prec = scale(data$wmo_prec)
data$syear = scale(data$year)
data$sLat = scale(data$Latitude)
data$sLon = scale(data$Longitude)
data$sTemp_Air = scale(data$Temp_Air)
# Remove NAs
rows = rownames(model.matrix(~sTemp_Water + spH + sLat + sLon + sCond + seas_wmo+ swmo_prec + swater_speed_ms + swater_depth +sTemp_Air+ syear + duration + locality + site_irn + coll_date, data = data))
data = data[rows, ]
```

1.  Factors which drive the occurrence of BT

    Minimal model:

    ```{r}
    model1 = glm(bt_pres~ sTemp_Water,
                 data = data,  family = binomial)
    ```

    Add environmental variables such as Temperature, pH, water metrics (e.g. water speed etc). Which variables could be potential confounders (year? locality? Season?....)

2.  Factors which affect the infection rate

    We use a k/n binomial model:

    ```{r}
    model2 = glm(cbind(BT_pos_tot, BT_tot - BT_pos_tot )~ sTemp_Water ,
                     data = data[data$BT_tot > 0, ],  family = binomial)
    ```

    Add environmental variables such as Temperature, pH, water metrics (e.g. water speed etc). Which variables could be potential confounders (year? locality? Season?....)
:::

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution - Species distribution model

Environmental variables (part of our hypothesis): Temp_water, Temp_Air, pH, Cond, swmo_prec, water_speed_ms, and water_depth.

Potential confounders: site_type, year, seas_wmo (season) (, and locality)

Our model:

```{r}
model1 = glm(bt_pres~ site_type + sTemp_Water + spH +
               sCond + swmo_prec + swater_speed_ms  + duration + 
               sTemp_Air + seas_wmo + syear + swater_depth,
               data = data,  family = binomial)
summary(model1)
```

Intercept corresponds to seas_wmo == wet and site_type == can.2

Year (our confounder) has a very strong effect on the occurrence rate. Conducitivity has a strong positive effect. Let's look at the effects:

```{r,warning=FALSE}
plot(allEffects(model1))
```

it appears that site_type and conductivity have the largest positive effects on occurrence rates. It is reasonable to assume that there are interactions between freshwater (site_type) and factors such as conductivity:

```{r,warning=FALSE}
model1 = glm(bt_pres~ site_type * sCond -sCond + swater_speed_ms+spH + sTemp_Water + duration + 
                 sTemp_Air + seas_wmo + syear + swater_depth,
             data = data,  family = binomial)
summary(model1)
```

"-sCond" allows us to test the slopes for the different site_types against 0 (no reference level)

Residual checks:

```{r}
library(DHARMa)
sim = simulateResiduals(model1, plot = TRUE)
```

Check misfits by plotting predictors against residuals:

```{r}
plotResiduals(sim, data$sCond)
plotResiduals(sim, data$sTemp_Air)
```

They look okay!

**Bonus - Spatial autocorrelation:**

A common problem with spatial data is spatial autocorrelation. We can use DHARMa to test if our residuals are spatially autocorrelated

```{r}
res2 = recalculateResiduals(sim, group = c(data$site_irn))
groupLocations = aggregate(cbind(data$sLat, data$sLon ), list( data$site_irn), mean)
testSpatialAutocorrelation(res2, x = groupLocations$V1, y = groupLocations$V2)
```

The test is significant, i.e. we should assume that the data is spatially autocorrelated. Addressing this issue, however, is part of the Advanced course
:::

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution - Infection rate

Environmental variables (part of our hypothesis): Temp_water, Temp_Air, pH, Cond, swmo_prec, water_speed_ms, and water_depth.

Potential confounders: site_type, year, seas_wmo (season), duration, ( and locality)

Our model:

```{r}
data_inf = data[data$BT_tot > 0 , ] # only observations 

model2 = glm(cbind(BT_pos_tot, BT_tot - BT_pos_tot )~ site_type + sTemp_Water + spH +
               sCond + log(swmo_prec+1) + swater_speed_ms  + log(duration) + 
               sTemp_Air + seas_wmo + syear + swater_depth,
               data = data_inf,  family = binomial)
summary(model2)
```

Intercept corresponds to seas_wmo == wet and site_type == can.2

Year (our confounder) has a very strong effect on the occurrence rate. Syte_type, swmo_prec (Precipitation), sTemp_Water, and sTemp_Air have strong effects:

```{r,warning=FALSE}
plot(allEffects(model2))
```

With partial residuals:

```{r,warning=FALSE}
plot(allEffects(model2, partial.residuals = TRUE))
```

Reminder: The partial residuals don't tell us anything because the variance of the residuals (remember binomial) depends on the probability of the binomial process AND on k (k/n binomial model). We need DHARMa to check the residuals:

```{r}
library(DHARMa)
sim = simulateResiduals(model2, plot = TRUE)

```

Residuals look okayish for a k/n model.

Check misfits by plotting predictors against residuals:

```{r}
plotResiduals(sim, log(data_inf$swmo_prec+1))
plotResiduals(sim, data_inf$sTemp_Water)
```

swmo_prec doesn't look good but this is caused by the many zeros in the variable
:::

### Elks

The data for this example is not in the ecodata package, but will be distributed in the course.

The data contains observations of elk presence / absence in various places in Canada. The ecological question is if roads disturb the elks.

Task: perform an analysis of the data to understand if roads have a causal effect on elk presence. Correct for appropriately for confounders! Check the residuals of your model to make sure it's properly specified!


### Making spatial predictions with a logistic regression - Elephant SDM

To demonstrate how to fit a species distribution model and extrapolate it in space, let's have a look at the elephant dataset, which is contained in EcoData

```{r, echo=TRUE, eval=TRUE}
library(EcoData)
?elephant
```

The object elephant contains two subdatasets

1.  elephant\$occurenceData contains presence / absence data as well as bioclim variables (environmental predictors) for the African elephant

2.  elephant\$predictionData data with environmental predictors for spatial predictions

The environmental data consists of 19 environmental variables, called bio1 through bio19, which are public and globally available bioclimatic variables (see <https://www.worldclim.org/data/bioclim.html> for a description of the variables). For example, bio1 is the mean annual temperature. No understanding of these variables is required for the task, the only difficulty is that many of them are highly correlated because they encode similar information (e.g. there are several temperature variables).

The goal of this exercise is to fit a logistic regression model based on the observed presence / absences, and then make new predictions of habitat suitability in space across Africa based on the fitted model. Thus, our workflow consists of two steps:

1.  building and optimizing the predictive model, and
2.  using the predictive model to make predictions on new data and visualizing the results.

Here an example of how you could do this

**Build predictive model:**


```{r}
model = glm(Presence~bio1, data = elephant$occurenceData, family = binomial()) # minimum model
```

To check the predictive power of the model, you can either use

```{r, message=F}
AIC(model) 
library(pROC)
auc(elephant$occurenceData$Presence, predict(model, type = "response"))
```

The AUC is a common measure of goodness of fit for binary classification. However, it doesn't penalize for complexity, as the AIC. Better to look at it under cross-validaton

```{r, results=F, message = F}
library(boot)
res = cv.glm(elephant$occurenceData, model, cost = auc, K=5)
```

```{r}
res$delta
```

Currently, fit and validation AUC are nearly identical, because we have a very simple model. However, for more complex models, this could change. 

-   Drop some of the highly correlated variables (don't use all of them).

-   Use quadratic effects

-   Use `stepAIC` or the `dredge` function to optimize the predictive model

**Make new predictions**

The data for making spatial predictions is in elephant$predictionData. This new dataset is not a data.frame but a raster object, which is a special data class for spatial data. You can plot one of the predictors in the following way.

```{r, message = F}
library(sp)
library(raster)
plot(elephant$predictionData$bio1)
```

As our new_data object is not a typical data.frame, we are not using the standard predict function for a glm, which is ?predict.glm, but the predict function from the raster object (which internally transforms the new_data into a classical data.frame, pass then the data.frame to our model, and then transforms the output back to a raster object). Therefore, the syntax is slightly different to how we previously used predict().

```{r}
predictions =  predict(elephant$predictionData, model = model, type = "response")
head(as.data.frame(predictions))
```

The advantage of the raster object is that we can directly use it to create a map (the raster object has coordinates for each observation):

```{r}
spplot(predictions, colorkey = list(space = "left") )
```

**Task:** play around with the logistic regression to improve predictive accuracy. You can check predictive accuracy by looking at AIC or by taking out a bit of the data for validation, e.g. by AUC. When improving the predictive power of the model, does the map change?
