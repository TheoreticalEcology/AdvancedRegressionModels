---
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r, include=FALSE}
set.seed(42)
```

# ANOVA and other tests on the LM

## The idea of ANOVA

ANOVA stands for ANalysis Of VAriance. The basic idea is to find out how much of the signal (variance) is explained by different factors. We had already short introduced ANOVA in the section on categorical predictors.

The problem with explaining ANOVA is that the term is overfraught with historical meanings and explanations that are no further relevant. It used to be that ANOVA is a stand-alone method that you use for experimental designs with different treatments, that ANOVA assumes normal distribution and partitions sum of squares, that there are repeated-measure ANOVAS and all that, and those varieties of ANOVA partly still exist, but in general, there is a much simpler and general explanation of ANOVA:

Modern explanation: ANOVA is not a statistical model, but a hypothesis test that can be performed on top of any regression model. What this test is doing is to measure how much model fit improves when a predictor is added, and if this improvement is significant.

### An example

As an example, here is a ANOVA (function `aov()`) performed on the fit of a linear model

```{r}
fit = lm(Ozone ~ Wind + Temp, data = airquality)
summary(aov(fit))
```

In the standard ANOVA for the LM, model fit is measured by the reduction in residual sum of squares. Let's look at the example above. In the ANOVA table above, we (virtually) start with an intercept only model. Now, what the table tells us is that adding Wind to the model reduces the Sum Sq. by 45284, and adding also Temp reduces the Sum Sq by another 25886, which leaves us with 53973 residual sum sq. From this, we can also conclude that the total variance of the response is 53973 + 25886 + 45284 = 125143. Let's check this:

```{r}
sum((fit$model$Ozone - mean(fit$model$Ozone))^2)
```

Thus, we can conclude that the R2 explained by each model component is

```{r}
53973/125143 # Wind
25886/125143 # Temp
45284/125143 # Residual
```

Moreover, the ANOVA table performs tests to see if the improvement of model fit is significant against a null model. This is important because, as mentioned before (particular in the chapter on model selection), adding a predictor always improves model fit.

To interpret the p-values, consider that H0 = the simpler model is true, thus we test if the improvement of model fit is higher than what we would expect if the predictor has no effect.

### Customizing the partitioning

There are a number of cases where it can make sense to perform an ANOVA for larger parts of the model. Consider, for example, the following regression:

```{r}
fit = lm(Ozone ~ Wind + I(Wind^2) + Temp + I(Temp^2), data = airquality)
```

Maybe, we would like to ask how much variance is explained by Wind + Wind\^2, and how much by Temp + Temp\^2. In this case, we can perform custom ANOVA, using the `anova()` function that we already introduced in the section on model selection via likelihood ratio tests (LRTs).

```{r}
m0 = lm(Ozone ~ 1, data = airquality)
m1 = lm(Ozone ~ Wind + I(Wind^2) , data = airquality)
m2 = lm(Ozone ~ Wind + I(Wind^2) + Temp + I(Temp^2), data = airquality)
anova(m0, m1, m2)
```

## Fundamental issues in ANOVA

There are three basic problems that we will come back again when generalizing this principle across a range of models:

1.  How should we measure "improvement of model fit". Traditionally, improvement is measured by the reduction of the residual sum of squares, but for GLMs, we will have to expand this definition
2.  How should we test if the improvement in fit is significant? For simple models, this is not so much a problem
3.  How should we partition variance if predictors are collinear, and thus the order in which predictors are included matters
4.  Should we correct for complexity?

Let's look at the problem one by one:

### Measuring model fit

The definition of model fit via sum of squares makes sense as long as we work with linear models.

For GLMs, this definition doesn't make sense any more. A number of so-called pseudo-R^2^ metrics have been proposed. Most of them are based on the likelihood (as a measure of model fit) and try to recover as far as possible the properties of an R^2^ for the linear model.

A common metrics is McFadden pseudo-R^2^, which is defined as 1-\[LogL(M)/LogL(M~0~))\], where M is our model, and M0 is an intercept only model.

### Testing if the improvement is significant

The test used in our example before is an F-test. The F-test is used for models that assume normal distribution. The F-test can be interpreted as a special case of a likelihood ratio test (LRT), which we already used in the chapter on model selection. An LRT can be used on two nested models, with M0 being the simpler model, and makes the following assumptions:

1.  H~0~ = M~0~ is true
2.  Test statistic = likelihood ratio -2 \[MLE(M~0~)/MLE(H~1~)\]
3.  Then, under relatively broad conditions, the test static will be chi-squared distributed, with df = difference residual df (parameters) of the models M~1~, M~0~

This setup works for LMs and GLMs, but runs into problems when the definition how many df a model has is unclear. This is in particular the case for mixed models. In this case, one can resort to simulated LRTs. Simulated LRTs are a special case of the boostrap, which is a very general and popular method to generate nonparametric confidence intervals and null distributions. We will talk in detail about these methods in the chapter on nonparametric methods, but because this method is crucial for mixed models, I want to shortly explain it already here:

The difference between a parametric and a nonparametric test is that the latter does not make assumptions about the test statistic (point 3 above), but somehow generates the latter from the data. The parametric bootstrap does this in the following way:

1.  Simulate data from H~0~ (= the fitted model M~0~)
2.  Re-fit M~0~ and M~1~, and calculate likelihood ratios
3.  Repeat n times to get an idea about the expected increase in likelihood when moving to M~1~ under the assumption that M~0~ is correct

```{r}
library(DHARMa)
m0 = lm(Ozone ~ Wind , data = airquality)
m1 = lm(Ozone ~ Wind + Temp, data = airquality)
simulateLRT(m0, m1)
# for comparison
anova(m0, m1)
```

### Partitioning variance if order matters

Another key problem in ANOVA, and a source of much confusion, is how to deal with the fact that often, the order in which model components are added matters. Compare the results of our previous ANOVA with this one, where I only flipped the order of Temp and Wind:

```{r chunk_chapter3_chunk64, echo=TRUE, eval=TRUE}
fit = lm(Ozone ~ Temp + Wind, data = airquality)
summary(aov(fit))
```

The result is markedly different, and reason is that the `aov`{.R} function performs a so-called type I ANOVA. The type I ANOVA adds variables in the order in which they are in the model formula, and because Temp and Wind are collinear, the variable that is added first to the model will absorb variation from the other, and thus seems to explain more of the response.

There are other types of ANOVA that avoid this problem. The so-called type II ANOVA shows for each variable only the part that is uniquely attributable to the respective variable

```{r chunk_chapter3_chunk65, echo=TRUE, eval=TRUE}
car::Anova(fit, type = "II")
```

There is also type III, which is as type II, but avoids a similar problem for interactions, i.e. it discards the variance that is shared between main effects and interactions. Note that numeric variables should be centered and categorical variables should have orthogonal contrast when running a type III ANOVA (see below).

```{r chunk_chapter3_chunk66, echo=TRUE, eval=TRUE}
car::Anova(fit, type = "III")
```

Here is an overview of the situation for 2 predictors A and B and their interaction. The upper left figure corresponds to the case where we have no collinearity between either of those variables. The figure on the top right (and similarly types I - III) are the three possible types of ANOVA for variables with collinearity. The "overlap" between the circles depicts the shared part, i.e. the variability that can be expressed by either variable (due to collinearity). Note that the shares in Type II, III do not add up to 1, as there is a kind of "dark variation" that we cannot securely add to either variable.

```{r chunk_chapter3_67, echo=FALSE}
knitr::include_graphics(c("images/ANOVA.jpg"))
```

Which type of ANOVA is most appropriate? Unfortunately, the answer is both "it depends on what you want to know" and "there is a lot of discussion about this topic". I would say the following:

-   If you have an orthogonal, balanced design, it doesn't matter.

-   If not, type I is really only appropriate if you explicitly want to have the order-dependence. There may be some rare cases for this, e.g. if you want to say my regression should first explain everything it can by variable A, and then I only want to see if variable B adds something.

-   If you want to test for significance and partition R2 in a fair way, you should either choose type II or type III ANOVA. Discussions about which is preferable are ongoing, see e.g. [@hector2010analysis]. As you will see when reading this, most of these discussion evolve around your exact definition of the "R2 of an interaction" and what exactly you want to test.

::: callout-caution
Note that, apart from collinearity and balance, also centering of the variables can affect the shared component between main effects and interactions, and thus the variance partitioned in a type III ANOVA. To understand this, consider the following: in a regression x1\*x2, where both numeric predictors x1, x2 are centered, the interaction cannot create an average main effect on either x1 or x2, because either predictor has a equal amount of positive and negative values. If x1,x2 are uncentered, however, it is possible to make the regression predict increasing values with increasing x1 on average by setting an intercept value alone.

For that reason, working with uncentered predictors will lead to an increased shared variance between x1, x2 and the interaction x1:x2, and thus to lower SumSq values for the main effects in the type III ANOVA. You can see this by running the following code:

```{r, results='hide'}
fit = lm(Ozone ~ scale(Temp) * scale(Wind), data = airquality)
fit2 = lm(Ozone ~ Temp * Wind, data = airquality)

car::Anova(fit, type = "II")
car::Anova(fit2, type = "II")


car::Anova(fit, type = "III")
car::Anova(fit2, type = "III")
```

For categorical variables, the same thing can happen - the choice of contrasts (see section on contrasts) will influence the amount of shared variation between main effects and the interactions. When using orthogonal contrasts, this shared component is minimized, because the interaction cannot explain . Let's do a small example, where we compare a regression with treatment contrasts (which are not orthogonal) to Helmert contrasts (which are orthogonal):

```{r, results='hide'}
airquality$fMonth = as.factor(airquality$Month)
fit1 = lm(Ozone ~ Temp * fMonth , data = airquality)
fit2 = lm(Ozone ~ Temp * fMonth , data = airquality,
          contrasts = list(fMonth = "contr.helmert"))

car::Anova(fit1, type = "III")
car::Anova(fit2, type = "III")
```

Let's compare the results to an ANOVA without an interaction (note that in this case, type II / III) are identical for all sensible contrasts.

```{r, results='hide'}
fit1 = lm(Ozone ~ Temp + fMonth , data = airquality)
car::Anova(fit1, type = "III")
```

If you run this, you will see that the Helmert contrasts (any other orthogonal contrasts would do as well) match much better with the results of the model without interaction, because they de-correlate the variance explained by the main effects and the interactions. Because of this, it is usually advised to use orthogonal contrasts (note: the default treatment contrasts are not orthogonal) when running a type III ANOVA with 2-way interactions between two categorical variables.
:::

::: {.callout-caution icon="false"}
###### Excercise

Try out the difference between type I, II, III ANOVA for the airquality data set, either for the simple Wind + Temp model, or for more complicated models. If you want to see the effects of Type III Anova, you need to add an interaction (see next section).
:::

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution

Let's use the model with Wind, Temp, Solar.R, and their interactions. Wind and Temp, and Solar.R and Temp are collinear.

```{r chunk_chapter3_68, echo=FALSE}
fit = lm(Ozone ~ (scale(Wind) + scale(Temp) + scale(Solar.R))^2 , data = airquality)
summary(fit)

```

**Difference type I and II ANOVA:**

```{r chunk_chapter3_69, echo=FALSE}
summary(aov(fit))
print(car::Anova(fit, type = "II"))
```

Only the Temp:Solar.R interaction is not affected!

**Difference type II and III ANOVA:**

```{r chunk_chapter3_70, echo=FALSE}
print(car::Anova(fit, type = "III"))
```

Identical when variables are scaled.
:::

### Correcting partitions for complexity

A last problem is that model components with more complexity (df) will always tend to explain more variance. We can see this when a model with a separate mean per day:

```{r}
m = lm(Ozone ~ as.factor(Day) , data = airquality)
summary(aov(m))
```

The ANOVA tells us that this variable Day explains around 1/3 of the variation in the data, although it is not even significant. Because of this problem, the `summary.lm()` function reports a raw and an adjusted R^2^ for each model. The adjusted R^2^ tries to correct the R^2^ for the complexity of the model.

```{r}
summary(m)
```

Here, we see that the adjusted R^2^ is considerably lower than the raw R^2^, even though not zero.

In general, reliably adjusting R^2^ components for complexity in an ANOVA is very complicated. I would recommend that

-   If you report raw components (which is the default in most papers), consider that this is a description of your model, not an inference about the true variance created by the respective factor, and include in your interpretation that variables or model components with more df will always tend to explain more variance

-   Alternatively, if the true variance is really crucial for your study, you can adjust R^2^ by a null expectation, similar to the parametric bootstrap for the LRT. However, because you subtract what is expected under H~0~, this will be a conservative estimate.

## Relationship between lm and other hypotheses tests

ANOVA is just one example of the close relationship between the lm and other null hypothesis significance tests. ANOVA can either be seen as a special test (traditional view), or as a special evaluation of a fitted lm (modern view). I would recommend taking the modern view, as it easily generalizes to GLMs and other more advanced regression models such as GLMMs.

It is interesting to consider more generally which hypothesis tests can be obtained by looking at certain results or tests on the linear model. As you can see in the table below, there are many classical hypothesis tests that are equivalently or nearly equivalently tested by a linear model.

+----------------------------------+---------------------------------------+---------------------------------------------------+
| Hypothesis                       | R function                            | Equivalent linear model                           |
+==================================+=======================================+===================================================+
| Two groups differ in their means | `t.test(y_1, y_2)`                    | `lm(c(y_1, y_2)~c(g_1, g_2))`                     |
|                                  |                                       |                                                   |
| **unpaired t.test**              |                                       |                                                   |
+----------------------------------+---------------------------------------+---------------------------------------------------+
| Two groups differ in their means | `t.test(y_1, y_2, paired = TRUE)`     | `lm(y_2-y_1~1)`                                   |
|                                  |                                       |                                                   |
| **paired t.test**                |                                       |                                                   |
+----------------------------------+---------------------------------------+---------------------------------------------------+
| y depends on continuous x        | `cor.test(x, y)`                      | `lm(y~x)` note: only near-equivalent              |
|                                  |                                       |                                                   |
| **Pearson correlation**          |                                       |                                                   |
+----------------------------------+---------------------------------------+---------------------------------------------------+
| y depends on continuous x        | `cor.test(x, y, method = "spearman")` | `lm(rank(y)~rank(x))` note: only near-equivalent. |
|                                  |                                       |                                                   |
| **Spearman correlation**         |                                       |                                                   |
+----------------------------------+---------------------------------------+---------------------------------------------------+
| y differs within a group         | `aov(y~group)`                        | `lm(y~group)` with subsequent ANOVA               |
|                                  |                                       |                                                   |
| **One-way ANOVA**                |                                       |                                                   |
+----------------------------------+---------------------------------------+---------------------------------------------------+

: Common statistical tests with linear models

Based on this, one could think it doesn't matter then if we use a regression model or a hypothesis test. However, the advantage of the regression model is that

-   It provides a richer output

-   It provides the same output for all cases, with clear ideas about how to check residuals etc.

-   It allows to adjust for the effects of covariates

## Case studies

### Plant Height

Let's look at our plant height case study from the previous chapter. Which type of ANOVA should you run to analyze the model? Compare results to "less appropriate" alternatives.

```{r}
library(EcoData)
fit = lm(loght ~ temp * lat, data = plantHeight)
summary(fit)
```

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution

We have correlations between variables and an interaction, so per default, we should probably run a type II or III Anova. Note that for type III, you should center predictors to reduce collinearities (and thus shared fractions between main effects and interactions):

```{r}
plantHeight$stemp = scale(plantHeight$temp, scale = FALSE) # only center variable
plantHeight$slat = scale(plantHeight$lat, scale = FALSE)
fit = lm(loght ~ stemp * slat, data = plantHeight)
summary(fit)

```

The type II ANOVA discards shared fractions between main effects. Note that because of the high collinearity, R2 assigned to each predictor is extremely low.

```{r}
print(car::Anova(fit, type = "II"))
```

The type III ANOVA also discards the shared fractions between the interactions and main effects. For this reason, the main effects are even lower than in type II.

```{r}
print(car::Anova(fit, type = "III"))
```

Generally, because of its order dependence under collinearity, you should be hesitant to run a type I ANOVA. However, there can be reasons to do it. For example, in this case, you may argue that temperature is a more plausible or mechanistic predictor that latitude. For that reason, you could say that you want to explain everything by temperature first, and then see if latitude can explain something on top of temperature. If that is your goal, it can also run a type I ANVOA.

```{r}
summary(aov(fit))
```

As a last, add-on, let's see let's see what happens when we use the non centered variables:

```{r}
fit2 = lm(loght ~ temp * lat, data = plantHeight)
print(car::Anova(fit2, type = "II"))
print(car::Anova(fit2, type = "III"))
```

Note that in this case, main effects drop stronger between II and III, because there are stronger correlations between the main and interaction effects. This is an example of a more general point: if you run type II / III ANOVAs, you should do everything to reduce correlations between the variables. For categorical variables, one way to do so is to use orthogonal contrasts.
:::
