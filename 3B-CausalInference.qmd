---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Causal inference

## Correlations != Causality

The most fundamental distinction in strategies for model choice is if we want to estimate effects, or if we want to predict. If we want to estimate effects, we nearly always want to estimate causal effects.

Let me first define what we mean by "a causal effect": if we have a system with a number of variables, the causal effect of A on B is the change in B that would happen if we changed A, but kept all other aspects of this system constant.

Assume we look at the effect of coffee consumption on Lung Cancer. Assume further that there is no such effect. However, there is an effect of smoking on lung cancer, and for some reason, smoking also affects coffee consumption.

It is common to visualize and analyze such relationships in a causal graph. Here, I use the ggdag

```{r}
library(ggdag)
library(ggplot2)
theme_set(theme_dag())
dag = confounder_triangle(x = "Coffee", y = "Lung Cancer", z = "Smoking") 
ggdag(dag, text = FALSE, use_labels = "label")
```

We can use the ggdag package (or to be more exact: the underlying daggitty package) to explore the implications of this graph. The way I created the graph already includes the assumption that we are interested in the effect of Coffee on Lung Cancer. I can use the function ggdag_dconnected() to explore if those two are d-connected, which is just a fancy word for correlated.

```{r}
ggdag_dconnected(dag, text = FALSE, use_labels = "label")
```

In this case, the plot highlights me to the fact that Coffee and Lung cancer d-connected. What that means is: if I plot coffee against lung cancer, I will see a correlation between them, even though coffee consumption does not influence lung cancer at all. We can easily confirm via a simulation that this is true:

```{r}
smoking <- runif(50)
Coffee <- smoking + rnorm(50, sd = 0.2)
LungCancer <- smoking + rnorm(50, sd =0.2)
fit <- lm(LungCancer ~ Coffee)
plot(LungCancer ~ Coffee)
abline(fit)
```

So, if we would fit a regression of LungCancer \~ Coffee, we would at least conclude that there is a correlation between the two variables. Many people would even go a step further, and conclude that Coffee consumption affects lung cancer. This is a classical misinterpretation, the is spurious, created by the confounder smoking. Realizing this, I could ask the ggdag_dconnected function what would happen if I control for the effect of smoking.

```{r}
ggdag_dconnected(dag, text = FALSE, use_labels = "label", controlling_for = "z")
```

The result conforms with our intuition - once we control for smoking, there will be no correlation between Coffee and Lung Cancer (because there is no causal effect between them). In causal lingo, we say the two are now d-separated.

## How to control?

But how do we control for the confounder smoking?

### Experimental control

The conceptually easiest approach is: we perform a controlled experiment. The entire idea experiments is that we vary only one (or a few) factors, keeping the others constant, and thus we are able to "see" causal effects directly. So, if we did an experiment with people that all smoke the same, varying coffee, we would get the right result:

```{r}
smoking <- rep(0.5, 50)
Coffee <- smoking + rnorm(50, sd = 0.2)
LungCancer <- smoking + rnorm(50, sd =0.2)
fit <- lm(LungCancer ~ Coffee)
plot(LungCancer ~ Coffee)
abline(fit)
```

Which us to an important point: people that run controlled experiments always estimate causal effects, unless something went wrong with the control. Actually, all advice regarding experimental design (control, randomization) is aimed are removing any possible confounding with other factors, so that we can isolate causal effects.

### Synthetic control

In observational data, we have to use statistical methods to achieve synthetic control. Luckily, we have a great tool for that: the multiple regression.

The multiple regression can estimate the effect of coffee, corrected for the effect of smoking. Thus, by including smoking in the multiple regression, we are "virtually" holding smoking constant, thus allowing us to estimate the causal effect of coffee. Let's try this out:

```{r}
smoking <- runif(100)
Coffee <- smoking + rnorm(100, sd = 0.2)
LungCancer <- smoking + rnorm(100, sd =0.2)
fit1 <- lm(LungCancer ~ Coffee)
fit2 <- lm(LungCancer ~ Coffee + smoking)
plot(LungCancer ~ Coffee)
abline(fit1)
abline(fit2, col = "red")
legend("topleft", c("simple regression", "multiple regression"), col = c(1,2), lwd = 1)
```

## A framework for causal analysis

What we have just seen is an example of a causal analysis. The goal of a causal analysis is to control for other variables, in such a way that we estimate the same effect size we would obtain if only the target predictor was manipulated (as in a randomized controlled trial). If we are after causal effects, the correct selection of variables is crucial, while it isn't if we just want to predict.

You probably have learned in your intro stats class that, to do so, we have to control for **confounders.** I am less sure, however, if everyone is clear about what a confounder is. In particular, confounding is more specific than having a variable that correlates with predictor and response. The direction is crucial to identify true confounders. Imagine that there is a third variable that is included

```{r}
dag = collider_triangle(x = "Coffee", y = "Lung Cancer", m = "Nervousness") 
ggdag(dag, text = FALSE, use_labels = "label")
```

Let's simulate some data according to this structure

```{r}
set.seed(123)
Coffee <- runif(100)
LungCancer <- runif(100)
nervousness = Coffee + LungCancer + rnorm(100, sd = 0.1)
```

If we fit a multiple regression on this structure, we erroneously conclude that there is an effect of coffee on lung cancer

```{r}
fit1 <- lm(LungCancer ~ Coffee + nervousness)
summary(fit1)
```

this time, the univariate regression gets it right:

```{r}
fit1 <- lm(LungCancer ~ Coffee)
summary(fit1)
```

What we've seen here is a collider bias - a collider is a variable that is influenced by predictor and response. Although it correlates with predictor and response, correcting for it (or including it) in a multiple regression will create a bias on the causal link we are interested in (Corollary: Including all variables is not always a good thing).

There is an entire framework to analyze which variables you need to include in the regression to achieve proper control (Pearl 2000, 2009). In the following picture, I have summarized the three basic structures. The one that we haven't discussed yet is a mediation structure. A mediator is an intermediate variable that "mediates" an effect between exposure and response.

```{r chunk_chapter3_72, echo=FALSE, out.width="150%", out.height="150%"}
knitr::include_graphics(c("images/CausalStructures.jpg"))
```

How do we deal with the basic causal structures colliders, mediators and confounders in a regression analysis? Start by writing down the hypothesis / structure that you want to estimate causally (for example, in A, B "Plant diversity" -\> Ecosystem productivity). Then

1.  Control for all confounding structures
2.  Do not control for colliders and other similar relationships, e.g. "M-Bias" (red paths).
3.  It depends on the question whether we should control for **mediators** (yellow paths).

Note: If other variables are just there to correct our estimates, they are **nuisance parameters** (= we are not interested in them), and we should later not discuss them, as they were not themselves checked for confounding (Table 2 fallacy).

::: callout-tip
The best practical guidance paper I know on estimating causal effects is Lederer et al., 2018, "Control of Confounding and Reporting of Results in Causal Inference Studies. Guidance for Authors from Editors of Respiratory, Sleep, and Critical Care Journals" which is available <a href="https://www.atsjournals.org/doi/full/10.1513/AnnalsATS.201808-564PS" target="_blank" rel="noopener">here</a>.
:::

::: {.callout-caution icon="false"}
#### Case study 1

Take the example of the past exercise (airquality) and assume, the goal is to understand the causal effect of Temperature on Ozone (primary hypothesis). Draw a causal diagram to decide which variables to take into the regression (i.e. noting which are confounders, mediators or colliders), and fit the model.
:::

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution

-   Solar.R could affect both Temp, Ozone -\> Coufounder, include
-   Wind could affect Temp, Ozone -\> Coufounder, include. Alternatively, one could assume that Temp is also affecting Wind, then it's a mediator
-   I would not include Month, as the Month itself should not affect Ozone, it's the Temp, Solar.R of the month that must have the effect. It's more like a placeholder, but if you include it it will nearly act as a collider, because it can snitch away some of the effects of the other variables.
:::

## Graphs and models on graphs

https://www.andrewheiss.com/blog/2020/02/25/closing-backdoors-dags/

Let's take a more complicated example. We will use data from Grace & Keeley (2006), who try to understand plant diversity following wildfires in fire-prone shrublands of California. The authors have measured various variables, and they have a specific hypothesis how those are related (see below).

```{r}
library(piecewiseSEM)
theme_set(theme_dag())

dag <- dagify(rich ~ distance + elev + abiotic + age + hetero + firesev + cover,
  firesev ~ elev + age + cover,
  cover ~ age + elev + abiotic ,
  exposure = "cover",
  outcome = "rich"
  )

ggdag(dag)
```

For this exercise, I want to assume that we are particularly interested in the causal effect of cover on species richness. I have specified this in the dag above already. With the ggdag_paths() command, I can isolate all paths that would create a correlation between cover and richness

```{r}
ggdag_paths(dag)
```

Looking at the paths, I can see confounding structures, for example for age, abiotic and elevation. So I should definitely control for them. The function

```{r}
ggdag_adjustment_set(dag)
```

helps me by suggesting which confounders I should control for. Note that the function doesn't suggest to adjust for mediation structures per default, but keeps them in the graph, so you have to decide what to do with them. You can directly ask to include adjustment for mediation if you run

```{r}
ggdag_adjustment_set(dag, effect="direct")
```

in this case, plot suggest to adjust for fireseverity, to isolate the direct effect of cover on richness.

So, the model we should fit is

```{r}
fit = lm(rich ~ cover + hetero + distance, data = keeley)
summary(fit)
```

### Table II fallacy

When reporting this, note that we have only corrected the relationship rich \~ cover for confounding. Consequently, we should not report the entire regression table above, or at least make a clear distinction about which variables have been corrected, and which haven't.

The blind reporting of the entire regression table is known as Table II fallacy (because regression tables are often Table II in a paper).

### Structural equation models (SEMs)

But what if we are interested in all the causal relationships? If we have a graph already, the best option is to fit a structural equation model.

Structural equation models (SEMs) are models that are designed to estimate entire causal diagrams. For GLMs responses, you will currently have to estimate the DAG (directed acyclic graph) piece-wise, e.g. with <a href="https://cran.r-project.org/web/packages/piecewiseSEM/vignettes/piecewiseSEM.html" target="_blank" rel="noopener">https://cran.r-project.org/web/packages/piecewiseSEM/vignettes/piecewiseSEM.html</a>. For linear SEMs, we can estimate the entire DAG in one go. This also allows to have unobserved variables in the DAG. One of the most popular packages for this is `lavaan`. I will show here only an example using piecewiseSEM.

```{r chunk_chapter7_chunk20, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
library(piecewiseSEM)

mod = psem(
  lm(rich ~ distance + elev + abiotic + age + hetero + firesev + cover, data = keeley),
  lm(firesev ~ elev + age + cover, data = keeley), 
  lm(cover ~ age + elev + hetero + abiotic, data = keeley)
)

summary(mod)
plot(mod)
```

## Case studies

::: {.callout-caution icon="false"}
#### Case study: Swiss fertility

Perform a causal, a predictive and an exploratory analysis of the Swiss fertility data set called "swiss", available in the standard R data sets. Target for the causal analysis is to estimate the causal (separate direct and indirect effects) of education on fertility, i.e. `lm(Fertility ~ Education, data = swiss)`{.R}.
:::

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution

-   Agriculture, Catholic could be seen as confounders or mediators, depending on whether you think Education affects the number of people being in Agriculture or Catholic, or vice versa
-   Infant mortality could be a mediator or a collider, depeding on whether you think fertility -\> infant mortality or infant mortality -\> fertility. I would tend to see it as a mediator.

For all mediators: remember that if you want to get the total (indirect + direct) effect of education on fertility, you should not include mediators. If you want to get the direct effect only, they should be included.
:::

::: callout-tip
GAMs are particularly useful for confounders. If you have confounders, you usually don't care that the fitted relationship is a bit hard to interpret, you just want the confounder effect to be removed. So, if you want to fit the causal relationship between Ozone \~ Wind, account for the other variables, a good strategy might be:

```{r, echo=TRUE, eval=TRUE}
library(mgcv)
fit = gam(Ozone ~ Wind + s(Temp) + s(Solar.R) , data = airquality)
summary(fit)
```

In this way, you still get a nicely interpretable linear effect for Wind, but you don't have to worry about the functional form of the other predictors.
:::

::: {.callout-caution icon="false"}
#### Case study: Life satisfaction

The following data set contains information about life satisfaction (lebensz_org) in Germany, based on the socio-economic panel.

```{r chunk_chapter4_chunk1, echo=TRUE, eval=T}
library(EcoData)
?soep
```

Perform a causal analysis of the effect of income on life satisfaction, considering possible confounding / mediation / colliders.
:::

::: {.callout-tip collapse="true" appearance="minimal" icon="false"}
#### Solution

-   Nearly all other variables are confounders, gesund_org could als be a collider
-   Might consider splitting data into single households, families, as effects could be very different. Alternatively, could add interactions with single, families and / or time to see if effects of income are different

A possible model is

```{r chunk_chapter4_task_0, message=FALSE, warning=FALSE}
fit <- lm(lebensz_org ~ sqrt(einkommenj1) + syear + sex + alter + anz_pers + 
            bildung + erwerb + gesund_org, data = soep)
summary(fit)
```

Note that you shouldn't interpret the other variables (Table II fallacy) in a causal analysis, unless the other variables are analyzed / corrected for confounders / colliders.
:::

## 
