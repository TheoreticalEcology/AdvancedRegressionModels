[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced Regression Models with R",
    "section": "",
    "text": "Preface\nIf we have a response variable and want to understand how this response variable is influenced by one or several factors, we will typically use a regression model. The aim of this course is to enable you to run such a regression model in the quality expected for a “real” scientific study. To do so, you will have to master a number of skills, in particular:\n\nUnderstanding the fundamental statistical indicators in regression analysis (p-value, estimator) and their quality (power, bias, error, coverage),\nUnderstanding what a causal effect means in a regression context, and what this means for experimental design and model selection,\nKnowing all building blocks of the “advanced GLMM framework” that helps us to correctly model our data, e.g. GLMs, random effects, GAMs, correlation structures, …\nKnowledge of standard non-parametric evaluation methods for regression models, such as parametric and non-parametric bootstrap, cross-validation,\nand the ability to use all of these methods in an applied data analysis.\n\nThis is what we will mainly train in this course. Don’t worry if you think that this sounds too simple. We could spend an entire week on understanding the p-value alone, and still only scratch the surface. If you have an overview of what can be done with regression models, and are confident to run a realistic scientific analysis on your own, we have achieved a lot.\nThis course assumes basic prior knowledge of statistical methods (tests, regressions, p-value, power, CIs, …) and the ability to apply those in R. At the University of Regensburg, this knowledge would be taught in the Bachelors Biology Lecture “Statistik und Bioinformatik” (lecture notes in German here), and the block course “Introduction to statistics in R”. If you didn’t take those or comparable courses, you should at least try to get some basic understanding of R before proceeding with this book. This lecture series from MarinStatsLectures could be a good start. There is also an appendix in this book which covers the most common functions for data manipulation and plotting in R.\nIf you have comments, questions or suggestions regarding this book, please submit them here.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Note that some elements of this work (embedded videos, graphics) may be under a seperate licence and are thus not included in this licence."
  },
  {
    "objectID": "1A-GettingStarted.html",
    "href": "1A-GettingStarted.html",
    "title": "1  Getting Started",
    "section": "",
    "text": "In this course, we work with the combination of R + RStudio.\n\nR is the calculation engine that performs the computations.\nRStudio is the editor that helps you sending inputs to R and collect outputs.\n\nMake sure you have a recent version of R + RStudio installed on your computer. If you have never used RStudio, here is a good video introducing the basic system and how R and RStudio interact."
  },
  {
    "objectID": "1A-GettingStarted.html#libraries-that-you-will-need",
    "href": "1A-GettingStarted.html#libraries-that-you-will-need",
    "title": "1  Getting Started",
    "section": "1.3 Libraries that you will need",
    "text": "1.3 Libraries that you will need\nThe R engine comes with a number of base functions, but one of the great things about R is that you can extend these base functions by libraries that can be programmed by anyone. In principle, you can install libraries from any website or file. In practice, however, most commonly used libraries are distributed via two major repositories. For statistical methods, this is CRAN, and for bioinformatics, this is Bioconductor.\n\n\n\n\n\n\nClick to see more on installing libraries in R\n\n\n\n\n\nTo install a package from a library, use the command\n\ninstall.packages(LIBRARY)\n\nExchange “LIBRARY” with the name of the library you want to install. The default is to search the package in CRAN, but you can specify other repositories or file locations in the function. For Windows / Mac, R should work out of the box. For other UNIX based systems, may also need to install\nbuild-essential\ngfortran\nlibmagick++-dev\nr-base-dev\ncmake\nIf you are new to installing packages on Debian / Ubuntu, etc., type the following:\nsudo apt update && sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev cmake\n\n\n\nIn this book, we will often use data sets from the EcoData package, which is not on CRAN, but on a GitHub page. To install the package, if you don’t have the devtools package installed already, first install devtools from CRAN by running\n\ninstall.packages(\"devtools\")\n\nThen install the EcoData package via\n\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\",\n                         dependencies = T, build_vignettes = T)\n\nFor your convenience, the EcoData installation also forces the installation of most of the packages needed in this book, so this may take a while. If you want to load only the EcoData package, or if you encounter problems during the install, set dependencies = F, build_vignettes = F."
  },
  {
    "objectID": "1A-GettingStarted.html#assumed-r-knowledge",
    "href": "1A-GettingStarted.html#assumed-r-knowledge",
    "title": "1  Getting Started",
    "section": "1.4 Assumed R knowledge",
    "text": "1.4 Assumed R knowledge\nAs mentioned in the preface, this book assumes that you have basic knowledge about data manipulation (reading in data, removing or selecting columns or rows, calculating means per group etc.) and plotting in R. Note that for both purposes, there are currently two main schools in the R environment which do the same things, but with very different syntax:\n\nbase R, which uses functions such as plot(), apply(), aggregate()\ntidyverse, with packages such as dplyr and ggplot2, which provide functions such as mutate(), filter() and heavily rely on the %&gt;% pipe operator.\n\nThere are many opinions about advantages and disadvantages of the two schools. I’m agnostic about this, or more precisely, I think you should get to know both schools and then decide based on the purpose. I see advantages of tidyverse in particular for data manipulation, while I often prefer baseR plots over ggplot2. To keep it simple, however, all code in this course uses base R.\n\n\n\n\n\n\nNote\n\n\n\nThe tidyverse framework is currently trying to expand to the tasks of statistical / machine learning models as well, trying to streamline statistical workflows. While this certainly has a lot of potential, I don’t see it as general / mature enough to recommend it as a default for the statistical workflow.\n\n\nIn the following box, you will find an exercise that asks you to perform basic plots and data manipulations. To text yourself, please check that you can perform these operations. If you have problems, you should study an introductory R course (for example here) before continuing with this text.\n\n\n\n\n\n\nExercise - Data wrangling\n\n\n\nWe work with the airquality dataset:\n\ndat = airquality\n\n\nBefore working with a dataset, you should always get an overview of it. Helpful functions for this are str(), View(), summary(), head(), and tail(). Apply them to dat and make sure to understand what they do.\nWhat is the data type of the variable ‘Month’? Transform it to a factor\nScale the variable Wind and save it as a new variable in dat\nTransform the variable ‘Temp’ (log-transform) and save it as a new variable in dat\nExtract the first 100 rows of dat and remove the NAs from the subsetted dataset\nPlot the variables of the dataset, and against each other (e.g. Wind, Wind vs Temp, Temp vs Month, all simultaneously)\nCalculate correlation indices between the numerical variables (e.g. Wind and Temp, Temp and Ozone). What is the difference between Spearman and Pearson correlation?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nstr() helps us to check the data types of the variables, ensure that they are correct, e.g. categorical variables should be factors and continuous variables should be either num (numeric) or int (integer). summary()returns important summary statistics of our variables and informs us about NAs in the data\n\nstr(dat)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\nsummary(dat)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n\n\n\nThere are NAs in Ozone and Solar.R! Also, Month is not a factor!\nWe have to transform Month into a factor:\n\ndat$Month = as.factor(dat$Month)\nstr(dat)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : Factor w/ 5 levels \"5\",\"6\",\"7\",\"8\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n\nScaling means that the variables are centered and standardized (divided by their standard deviation):\n\ndat$sWind = scale(dat$Wind)\nsummary(dat)\n\n     Ozone           Solar.R           Wind             Temp       Month \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00   5:31  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00   6:30  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00   7:31  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88   8:31  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00   9:30  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00         \n NA's   :37       NA's   :7                                              \n      Day             sWind.V1      \n Min.   : 1.0   Min.   :-2.3438868  \n 1st Qu.: 8.0   1st Qu.:-0.7259482  \n Median :16.0   Median :-0.0730957  \n Mean   :15.8   Mean   : 0.0000000  \n 3rd Qu.:23.0   3rd Qu.: 0.4378323  \n Max.   :31.0   Max.   : 3.0492420  \n\n\n\nUse logfunction to transform the variable (be aware of NAs!)\n\ndat$logTemp = log(dat$Temp)\n\nUse [rows, cols] to subset the data and complete.cases() to remove observations with NAs\n\ndat_sub = dat[1:100,]\nsummary(dat_sub)\n\n     Ozone           Solar.R           Wind            Temp       Month \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.70   Min.   :56.00   5:31  \n 1st Qu.: 16.00   1st Qu.:101.0   1st Qu.: 7.40   1st Qu.:69.00   6:30  \n Median : 34.00   Median :223.0   Median : 9.70   Median :79.50   7:31  \n Mean   : 41.59   Mean   :193.3   Mean   :10.07   Mean   :76.87   8: 8  \n 3rd Qu.: 63.00   3rd Qu.:274.0   3rd Qu.:12.00   3rd Qu.:84.00   9: 0  \n Max.   :135.00   Max.   :334.0   Max.   :20.70   Max.   :93.00         \n NA's   :31       NA's   :7                                             \n      Day              sWind.V1          logTemp     \n Min.   : 1.00   Min.   :-2.3438868   Min.   :4.025  \n 1st Qu.: 7.00   1st Qu.:-0.7259482   1st Qu.:4.234  \n Median :14.50   Median :-0.0730957   Median :4.376  \n Mean   :14.93   Mean   : 0.0313607   Mean   :4.334  \n 3rd Qu.:23.00   3rd Qu.: 0.5797567   3rd Qu.:4.431  \n Max.   :31.00   Max.   : 3.0492420   Max.   :4.533  \n\n\ndat_sub = dat_sub[complete.cases(dat_sub),]\nsummary(dat_sub)\n\n     Ozone          Solar.R            Wind            Temp       Month \n Min.   :  1.0   Min.   :  7.00   Min.   : 4.00   Min.   :57.00   5:24  \n 1st Qu.: 16.0   1st Qu.: 97.25   1st Qu.: 7.40   1st Qu.:67.75   6: 9  \n Median : 33.0   Median :223.00   Median : 9.70   Median :81.00   7:26  \n Mean   : 41.5   Mean   :192.53   Mean   :10.15   Mean   :76.61   8: 5  \n 3rd Qu.: 61.5   3rd Qu.:274.25   3rd Qu.:12.00   3rd Qu.:84.25   9: 0  \n Max.   :135.0   Max.   :334.00   Max.   :20.70   Max.   :92.00         \n      Day              sWind.V1          logTemp     \n Min.   : 1.00   Min.   :-1.6910344   Min.   :4.043  \n 1st Qu.: 7.75   1st Qu.:-0.7259482   1st Qu.:4.216  \n Median :15.50   Median :-0.0730957   Median :4.394  \n Mean   :14.97   Mean   : 0.0550798   Mean   :4.330  \n 3rd Qu.:21.00   3rd Qu.: 0.5797567   3rd Qu.:4.434  \n Max.   :31.00   Max.   : 3.0492420   Max.   :4.522  \n\n\nSingle continuous variables can be visualized using a histogram (hist) , for two variables, it depends on their data types:\n\n\n\n\n\n\n\n\nScenario\nWhich plot\nR command\n\n\n\n\nNumeric\nHistogram or boxplot\nhist() andboxplot\n\n\nNumeric with numeric\nScatterplot\nplot\n\n\nNumeric with categorical\nBoxplot\nboxplot(numeric~categorical)\n\n\nCategorical with categorical\nmosaicplot or grouped barplot\nmosaicplot(table(categorical, categorical)) or barplot(data,        beside=TRUE)\n\n\n\n\n# Numeric\nhist(dat$Wind, main = \"Wind\")\n\n\n\n# Numeric vs numeric\nplot(dat$Wind, dat$Solar.R)\n\n\n\n# Numeric with categorical\nboxplot(Wind~Month, data = dat)\n\n\n\n# All with all\npairs(dat)\n\n\n\n\nSpearman is a rank correlation factor, less sensitive against outliers and non-linearity:\n\n# Pearson\ncor(dat$Wind, dat$Temp, use = \"complete.obs\")\n\n[1] -0.4579879\n\n# Spearman\ncor(dat$Wind, dat$Temp, use = \"complete.obs\", method = \"spearman\")\n\n[1] -0.4465408"
  },
  {
    "objectID": "1A-GettingStarted.html#organization-of-this-book",
    "href": "1A-GettingStarted.html#organization-of-this-book",
    "title": "1  Getting Started",
    "section": "1.1 Organization of this book",
    "text": "1.1 Organization of this book\nThe aim of this book is to introduce you to the main techniques and concepts that are used when performing regression analyses in applied settings. This book is organized in three parts:\n\nModelling the mean: The first part of this book is focusing on how to model the mean response as a function of the one or several predictor variables. For this chapter, we will mainly stick to the assumptions of the linear regression, which is that residuals are i.i.d. normally distributed. Topics we will cover here are linear regression, ANOVA and mixed models\nModel Choice The second part covers model choice, including how to handle missing data, model selection, causal inference and non-parametric methods\nModelling the Distribution The third part of the book is about modelling different residual distributions. We will relax the iid normal assumptions of the LM, and move to GLMs and modelling variance and correlation of residuals."
  },
  {
    "objectID": "2A-LinearRegression.html",
    "href": "2A-LinearRegression.html",
    "title": "2  Linear Regression",
    "section": "",
    "text": "Let’s start with the basics. For this chapter, we will rely a lot on the airquality data, which is one of the built-in datasets in R.\n\n\n\n\n\n\nMore info on the airquality data\n\n\n\n\n\nMost datasets in R have a help file. Thus, if you don’t know the data set, have a look at the description via pressing F1 with the curses on the word, or via typing\n\n?airquality\n\nAs you can read, the datasets comprises daily air quality measurements in New York, May to September 1973. You can see the structure of the data via\n\nstr(airquality)\n\nVariables are described in the help. Note that Month / Day are currently coded as integers, would be better coded as date or (ordered) factors (something you could do later, if you use this dataset).\n\n\n\nLet’s say we want to examine the relationship between Ozone and Wind in these data.\n\nplot(Ozone ~ Wind, data = airquality)\n\n\n\n\nOK, I would say there is some dependency here. To quantify this numerically, we want to fit a linear regression model through the data with the lm() function of R. We can do this in R by typing\n\nfit = lm(Ozone ~ Wind, data = airquality)\n\nThis command creates a linear regression model that fits a straight line, adjusting slope and intercept to get the best fit through the data. We can see the fitted coefficients if we print the model object\n\nfit\n\n\nCall:\nlm(formula = Ozone ~ Wind, data = airquality)\n\nCoefficients:\n(Intercept)         Wind  \n     96.873       -5.551  \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function name lm is short for “linear model”. Remember from your basic stats course: This model is not called linear because we necessarily fit a linear function. It’s called linear as long as we express the response (in our case Ozone) as a polynomial of the predictor(s). A polynomial ensures that when estimating the unknown parameters (we call them “effects”), they all affect predictions linearly, and can thus be solved as a system of linear equations.\nSo, an lm is any regression of the form \\(y = \\operatorname{f}(x) + \\mathcal{N}(0, \\sigma)\\), where \\(\\operatorname{f}\\) is a polynomial, e.g. \\({a}_{0} + {a}_{1} \\cdot x + {a}_{2} \\cdot {x}^{2}\\), and \\(\\mathcal{N}(0, \\sigma)\\) means that we assume the data scattering as a normal (Gaussian) distribution with unknown standard deviation \\(\\sigma\\) around \\(\\operatorname{f}(x)\\)."
  },
  {
    "objectID": "2A-LinearRegression.html#interpreting-the-fitted-model",
    "href": "2A-LinearRegression.html#interpreting-the-fitted-model",
    "title": "2  Linear Regression",
    "section": "2.2 Interpreting the fitted model",
    "text": "2.2 Interpreting the fitted model\nWe can get a more detailed summary of the statistical estimates with the summary() function.\n\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Wind, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.572 -18.854  -4.868  15.234  90.000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  96.8729     7.2387   13.38  &lt; 2e-16 ***\nWind         -5.5509     0.6904   -8.04 9.27e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.47 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.3619,    Adjusted R-squared:  0.3563 \nF-statistic: 64.64 on 1 and 114 DF,  p-value: 9.272e-13\n\n\nIn interpreting this, recall that:\n\n“Call” repeats the regression formula.\n“Residuals” gives you an indication about how far the observed data scatters around the fitted regression line / function.\nThe regression table (starting with “Coefficients”) provides the estimated parameters, one row for each fitted parameter. The first column is the estimate, the second is the standard error (for 0.95 confidence interval multiply with 1.96), and the fourth column is the p-value for a two-sided test with \\({H}_{0}\\): “Estimate is zero”. The t-value is used for calculation of the p-value and can usually be ignored.\nThe last section of the summary provides information about the model fit.\n\nResidual error = Standard deviation of the residuals,\n114 df = Degrees of freedom = Observed - fitted parameters.\nR-squared \\(\\left({R}^{2}\\right)\\) = How much of the signal, respective variance is explained by the model, calculated by \\(\\displaystyle 1 - \\frac{\\text{residual variance}}{\\text{total variance}}\\).\nAdjusted R-squared = Adjusted for model complexity.\nF-test = Test against intercept only model, i.e. is the fitted model significantly better than the intercept only model (most relevant for models with &gt; 1 predictor).\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\nWhat is the meaning of “An effect is not significant”?\nIs an effect with three *** more significant / certain than an effect with one *?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nYou should NOT say that the effect is zero, or that the null hypothesis has been accepted. Official language is “there is no significant evidence for an effect(p = XXX)”. If we would like to assess what that means, some people do a post-hoc power analysis (which effect size could have been estimated), but better is typically just to discuss the confidence interval, i.e. look at the confidence interval and say: if there is an effect, we are relatively certain that it is smaller than X, given the confidence interval of XYZ.\nMany people view it that way, and some even write “highly significant” for *** . It is probably true that we should have a slightly higher confidence in a very small p-value, but strictly speaking, however, there is only significant, or not significant. Interpreting the p-value as a measure of certainty is a slight misinterpretation. Again, if we want to say how certain we are about the effect, it is better to look again at the confidence interval, i.e. the standard error and use this to discuss the precision of the estimate (small confidence interval / standard error = high precision / certainty)."
  },
  {
    "objectID": "2A-LinearRegression.html#visualizing-the-results",
    "href": "2A-LinearRegression.html#visualizing-the-results",
    "title": "2  Linear Regression",
    "section": "2.3 Visualizing the results",
    "text": "2.3 Visualizing the results\nFor a simple lm and one predictor, we can visualize the results via\n\nplot(Ozone ~ Wind, data = airquality)\nabline(fit)\n\n\n\n\nA more elegant way to visualize the regression, which also works once we move to generalized linear models (GLM) and multiple regression, is using the effects package. The base command to visualize the fitted model is:\n\nlibrary(effects)\nplot(allEffects(fit, partial.residuals = T))\n\n\n\n\nThe blue line is the fitted model (with confidence interval in light blue). When using the argument partial.residuals = T, purple circles are the data, and the purple line is a nonparametric fit to the data. If the two lines deviate strongly, this indicates that we may have a misspecified model (see next section on residual checks)."
  },
  {
    "objectID": "2A-LinearRegression.html#residual-checks",
    "href": "2A-LinearRegression.html#residual-checks",
    "title": "2  Linear Regression",
    "section": "2.4 Residual checks",
    "text": "2.4 Residual checks\nThe regression line of an lm, including p-values and all that, is estimated under the assumptions that the residuals scatter iid normal. If that is not (approximately) the case, the results may be nonsense. Looking at the results of the effect plots above, we can already see a first indication that there may be a problem.\nWhat we see highlighted here is that the data seems to follow a completely different curve than the fitted model. The conclusion here would be: The model we are fitting does not fit to the data, we should not interpret its outputs, but rather say that we reject it, it’s the wrong model, we have to search for a more appropriate description of the data (see next section, “Adjusting the functional form”).\n\n2.4.1 LM residual plots\nTo better analyse these residuals (and potential problems), R offers a function for residual plots. It produces 4 plots. I think it’s most convenient plotting them all into one figure, via the command par(mfrow = c(2, 2)), which produces a figure with 2 x 2 = 4 panels.\n\npar(mfrow = c(2, 2))\nplot(fit)\n\n\n\n\nInterpretation of the panels:\n\nResiduals vs Fitted: Shows misfits and wrong functional form. Scattering should be uniformly distributed.\nNormal Q-Q: Checks if residuals follow an overall normal distribution. Bullets should lie on the line in the middle of the plot and may scatter a little bit at the ends.\nScale - Location: Checks for heteroskedasticity. Does the variance change with predictions/changing values? Scattering should be uniformly distributed.\nResiduals vs Leverage: How much impact do outliers have on the regression? Data points with high leverage should not have high residuals and vice versa. Bad points lie in the upper right or in the lower right corner. This is measured via the Cook’s distance. Distances higher than 0.5 indicate candidates for relevant outliers or strange effects.\n\n\n\n\n\n\n\nNote\n\n\n\nThe plot(model) function should ONLY be used for linear models, because it explicitly tests for the assumptions of normally distributed residuals. Unfortunately, it also works for the results of the glm() function, but when used on a GLM beyond the linear model, it is not interpretable.\n\n\n\n\n2.4.2 DHARMa residual plots\nThe residual plots above only work for testing linear models, because they explicitly test for the assumptions of a normal distribution, which we will relax once we go to more complex models. The DHARMa package provides a general way to perform residual checks for practically any model from the generalized linear mixed-effect model (GLMM) family.\n\nlibrary(DHARMa)\nres &lt;- simulateResiduals(fit, plot = T)\n\n\n\n\nThe simulateResiduals() function uses simulations to generate standardized residuals for any GLMM. Standardized means that a uniform distribution of residuals signifies perfect fit. Here, the residuals are saved in the variable res.\nThe argument plot = T creates a plot with two panels (alternatively, you could also type plot(res)). The left panel is a uniform qq plot (calling plotQQunif), and the right panel shows residuals against predicted values (calling plotResiduals), with outliers highlighted in red.\nVery briefly, we would expect that a correctly specified model shows:\n\na straight 1-1 line, as well as n.s. of the displayed tests in the qq-plot (left) -&gt; evidence for an the correct overall residual distribution (for more details on the interpretation of this plot, see plotQQunif)\nvisual homogeneity of residuals in both vertical and horizontal direction, as well as n.s. of quantile tests in the res ~ predictor plot (for more details on the interpretation of this plot, see ?plotResiduals)\n\nDeviations from these expectations can be interpreted similar to a linear regression. See the DHARMa vignette for detailed examples."
  },
  {
    "objectID": "2A-LinearRegression.html#adjusting-the-functional-form",
    "href": "2A-LinearRegression.html#adjusting-the-functional-form",
    "title": "2  Linear Regression",
    "section": "2.5 Adjusting the Functional Form",
    "text": "2.5 Adjusting the Functional Form\n\n2.5.1 Mean vs. variance problems\nLooking at the residual plots of our regression model, there are two types of problems, and there is a clear order in which you should solve them:\n\nSystematic misfit of the mean: the first thing we should worry about is if the model describes the mean of the data well, i.e. if our function form (we assumed a straight line) fits to the data. You can see misfit for our model in a) the effects plot, b) the res ~ fitted plot, and c) the DHARMa res ~ fitted plot.\nDistributional problems: only if your model fits the mean of the data well should you look at distributional problems. Distributional problems means that the residuals do not scatter as assumed by the regression (for an lm iid normal). The reason why we look only after solving 1) at the distribution is that a misfit can easily create distributional problems.\n\nHere, and in this part of the book in general, we will first be concerned with adjusting the model to describe the mean correctly. In the second part of the book, we will then also turn to advanced options to model the variance\n\n\n\n\n\n\nCaution\n\n\n\nImportant: Residuals are always getting better for more complex models. They should therefore NOT solely be used for automatic model selection, i.e. you shouldn’t assume that the model with the best residuals is necessarily preferable (on how to select models, see section on model selection). The way you should view residual checks is as a rejection test: the question you are asking is if the residuals are so bad that the model needs to be rejected!\n\n\n\n\n2.5.2 R regression syntax\nIf we see a misfit of the mean, we should adjust the functional form of the model. Here a few options (see Table B.1 for more details):\n\nfit = lm(Ozone ~ Wind, data = airquality) # Intercept + slope.\nfit = lm(Ozone ~ 1, data = airquality) # Only intercept.\nfit = lm(Ozone ~ Wind - 1 , data = airquality) # Only slope.\nfit = lm(Ozone ~ log(Wind), data = airquality) # Predictor variables can be transformed.\nfit = lm(Ozone^0.5 ~ Wind, data = airquality) # Output variables can also be transformed.\nfit = lm(Ozone ~ Wind + I(Wind^2), data = airquality) # Mathematical functions with I() command.\n\nBasically, you can use all these tools to search for a better fit through the data.\nNote that if you transform the response variable, you change both the functional form and the spread of the residuals. Thus, this option can change (and often improve) both the mean and distributional problems, and is a common strategy to deal with non-symmetric residuals etc. There is a small convenience function in the package MASS that automatically searches for the best-fitting power transformation of the response.\n\nlibrary(MASS)\nboxcox(fit)\n\n\n\n\nIn the result, you can see, that the best fit assuming a power transformation is Ozone^k is delivered by k ~= 0.35.\n\n\n\n\n\n\nTask\n\n\n\nModify the formula with the tools above to get (as far as possible) an acceptable fit to the data.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPossible solution, adding a quadratic predictor and chosing a power of 0.35 transformation based on the boxcox function:\n\nfit1 = lm(Ozone^0.35 ~ Wind + I(Wind^2), data = airquality)\nplot(allEffects(fit1, partial.residuals = T), selection = 1)\n\n\n\n\nNote that if you plot(fit1), there seems to be still a bit of a pattern in the scale-location plot (heteroskedasticity). I would say that this is not particularly concerning, but if you are concerned and don’t manage to address it via changing the functional form, you could fit a regression with variable variance. We will discuss this in a later chapter.\nYou could get an even better fit by adding more and more predictors, for example:\n\nfit2 = lm(Ozone^0.35 ~ Wind + I(Wind^2) + I(Wind^3) + I(Wind^4) + I(Wind^5) +\n           I(Wind^6) + I(Wind^7) + I(Wind^8), data = airquality)\nplot(allEffects(fit2, partial.residuals = T), selection = 1)\n\n\n\n\nHowever, as noted above, and as we will further discuss in the section on model selection, this is not a good idea, because while a more complicated model always improves residuals, it has other disadvantages. This model is likely too complex for the data (aka it overfits). We can see this by looking at common model selection indicators (again, more in the section on model selection).\nAIC comparison (lower = better)\n\nAIC(fit1)\n\n[1] 270.2059\n\nAIC(fit2)\n\n[1] 274.7512\n\n\nLikelihood ratio test (is there evidence for the more complex model?)\n\nanova(fit1, fit2)\n\nAnalysis of Variance Table\n\nModel 1: Ozone^0.35 ~ Wind + I(Wind^2)\nModel 2: Ozone^0.35 ~ Wind + I(Wind^2) + I(Wind^3) + I(Wind^4) + I(Wind^5) + \n    I(Wind^6) + I(Wind^7) + I(Wind^8)\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    113 65.112                           \n2    107 61.059  6    4.0528 1.1837 0.3205\n\n\n\n\n\n\n\n2.5.3 Generalized additive models (GAMs)\nAnother options to get the fit right are Generalized Additive Models (GAM). The idea is to fit a smooth function to data, to automatically find the “right” functional form. The smoothness of the function is automatically optimized.\n\nlibrary(mgcv)\n\nfit = gam(Ozone ~ s(Wind) , data = airquality)\n\n# allEffects doesn't work here.\nplot(fit, pages = 0, residuals = T, pch = 20, lwd = 1.8, cex = 0.7,\n     col = c(\"black\", rep(\"red\", length(fit$residuals))))\n\n\n\n\nIn the summary(), you still get significance for the smoothing term (i.e. a p-value), but given that you fit a line that could go up and down, you don’t get an effect direction.\n\nsummary(fit)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nOzone ~ s(Wind)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   42.129      2.194    19.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n          edf Ref.df     F p-value    \ns(Wind) 2.995  3.763 28.76  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.487   Deviance explained =   50%\nGCV = 578.45  Scale est. = 558.53    n = 116\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nFit Ozone ~ Temp, and look at summary, residuals and visualizations. What would you conclude?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfit = lm(Ozone ~ Temp, data = airquality)\nplot(allEffects(fit, partial.residuals = T))\n\n\n\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n\n\nTemperature has a significant positive effect of Ozone. However, effect seems nonlinear, could consider adding a quadratic term of a GAM to improve the fit. The model explains nearly 49% of the variance of the given data (R2). 37 observations have missing data and are omitted. The model explains the data significantly better compared a model with only an intercept (F-statistic)."
  },
  {
    "objectID": "2A-LinearRegression.html#categorical-predictors",
    "href": "2A-LinearRegression.html#categorical-predictors",
    "title": "2  Linear Regression",
    "section": "2.6 Categorical Predictors",
    "text": "2.6 Categorical Predictors\nThe lm() function can handle both numerical and categorical variables. To understand what happens if the predictor is categorical, we’ll use another data set here, PlantGrowth (type ?PlantGrowth or F1 help if you want details). We visualize the data via:\n\nboxplot(weight ~ group, data = PlantGrowth)\n\n\n\n\n\n2.6.1 Understanding contrasts\nLet’s fit an lm() now with the categorical explanatory variable group. They syntax is the same as before\n\nfit = lm(weight ~ group, data = PlantGrowth)\nsummary(fit)\n\n\nCall:\nlm(formula = weight ~ group, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.0320     0.1971  25.527   &lt;2e-16 ***\ngrouptrt1    -0.3710     0.2788  -1.331   0.1944    \ngrouptrt2     0.4940     0.2788   1.772   0.0877 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.2641,    Adjusted R-squared:  0.2096 \nF-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\n\nbut the interpretation of the results often leads to confusion. We see effects for trt1 and trt2, but where is ctrl? The answer is: the R is calculating so-called treatment contrasts for categorical predictors. In treatment contrasts, we have a reference level (in our case ctrl), which is the intercept of the model, and then we estimate the difference to the reference for all other factor levels (in our case trt1 and trt2).\n\n\n\n\n\n\nTip\n\n\n\nBy default, R orders factors levels alphabetically, and thus the factor level that is first in the alphabet will be used as reference level in lm(). In our example, ctrl is before trt in the alphabet and thus used as reference, which also makes sense, because it is scientifically logical to compare treatments to the control. In other cases, it may be that the logical reference level does not correspond to the alphabetical order of the factors. If you want to change which factor level is the reference, you can re-order your factor using the relevel() function. In this case, estimates and p-values will change, because different comparisons (contrasts) are being made, but model predictions will stay the same!\n\n\nThis also becomes clear if we plot the results.\n\nplot(allEffects(fit))\n\n\n\n\nNote that in this case, the effects package depicts the CI by purple error bars.\nSo, in our example, the intercept estimates the mean weight in the ctr group (with a p-value tested against zero, usually not interesting), and the effects of trt1 and trt2 are the differences of weight compared to the control, with a p-value for a test against an effect size of zero.\n\n\n\n\n\n\nMore details on contrasts\n\n\n\n\n\nTreatment contrasts are not the only option to deal with categorical variables. There are many other options to set up contrasts, which may be appropriate in certain situations. A simple way to change contrasts is the following syntax\n\nfit = lm(weight ~ 0 + group, data = PlantGrowth)\nsummary(fit)\n\n\nCall:\nlm(formula = weight ~ 0 + group, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n          Estimate Std. Error t value Pr(&gt;|t|)    \ngroupctrl   5.0320     0.1971   25.53   &lt;2e-16 ***\ngrouptrt1   4.6610     0.1971   23.64   &lt;2e-16 ***\ngrouptrt2   5.5260     0.1971   28.03   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.9867,    Adjusted R-squared:  0.9852 \nF-statistic: 665.5 on 3 and 27 DF,  p-value: &lt; 2.2e-16\n\n\nwhich results in fitting the mean for each factor level. Unfortunately, this syntax option doesn’t generalize to multiple regressions. There are a number of further options for specifying contrasts. You can tell R by hand how the levels should be compared or use some of the pre-defined contrasts. Here is an example:\n\nPlantGrowth$group3 = PlantGrowth$group\ncontrasts(PlantGrowth$group3) = contr.helmert\nfit = lm(weight ~ group3, data = PlantGrowth)\nsummary(fit)\n\n\nCall:\nlm(formula = weight ~ group3, data = PlantGrowth)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.0710 -0.4180 -0.0060  0.2627  1.3690 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.07300    0.11381  44.573  &lt; 2e-16 ***\ngroup31     -0.18550    0.13939  -1.331  0.19439    \ngroup32      0.22650    0.08048   2.814  0.00901 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6234 on 27 degrees of freedom\nMultiple R-squared:  0.2641,    Adjusted R-squared:  0.2096 \nF-statistic: 4.846 on 2 and 27 DF,  p-value: 0.01591\n\n\nWhat we are using here is Helmert contrasts, which contrast the second level with the first, the third with the average of the first two, and so on. Which contrasts make most sense depends on the question. For more details, see chapter 10 in Stéphanie M. van den Berg’s book “Analysing Data using Linear Models” and also this paper.\n\n\n\n\n\n2.6.2 ANOVA\nFor categorical predictors, one nearly always performs an ANOVA on top of the linear regression estimates. The underlying ideas behind this will be explained in more detail in our section on ANOVA, but just very quickly:\nAn ANOVA starts with a base model (in this case intercept only) and adds the variable group. It then measures:\n\nHow much the model improves in terms of \\({R}^{2}\\) (this is in the column Sum Sq).\nIf this increase of model fit is significant.\n\nWe can run an ANOVA on the fitted model object, using the aov() function:\n\nanov = aov(fit)\nsummary(anov)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)  \ngroup3       2  3.766  1.8832   4.846 0.0159 *\nResiduals   27 10.492  0.3886                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this case, we would conclude that the variable group (3 levels) significantly improves model fit, i.e. the group overall has a significnat effect, even though the individual contrasts in the original model where not significant.\n\n\n\n\n\n\nNote\n\n\n\nThe situation that the ANOVA is significant, but the linear regression contrasts is often perceived as confusing, but this is perfectly normal and not a contradiction.\nIn general, the ANOVA is more sensitive, because it tests for an overall effect (thus also has a larger n), whereas the individual contrasts test with a reduced n. Moreover, as we have seen, in the summary of a lm(), for &gt;2 levels, not all possible contrasts are tested. In our example, we estimate p-values for crtl-trt1 and ctrl-trt2, which are both n.s., but we do not test for differences of trt1-trt2 (which happens to be significant).\nThus, the general idea is: 1. ANOVA tells us if there is an effect of the variable at all, 2. summary() tests for specific group differences.\n\n\n\n\n2.6.3 Post-Hoc Tests\nAnother common test performed on top of an ANOVA or a categorical predictor of an lm() is a so-called post-hoc test. Basically, a post-hoc test computes p-values for all possible combinations of factor levels, usually corrected for multiple testing:\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = fit)\n\n$group3\n            diff        lwr       upr     p adj\ntrt1-ctrl -0.371 -1.0622161 0.3202161 0.3908711\ntrt2-ctrl  0.494 -0.1972161 1.1852161 0.1979960\ntrt2-trt1  0.865  0.1737839 1.5562161 0.0120064\n\n\nIn the result, we see, as hinted to before, that there is a significant difference between trt1 and trt2. Note that the p-values of the post-hoc tests are larger than those of the lm summary(). This is because post-hoc tests nearly always correct p-values for multiple testing, while regression estimates are usually not corrected for multiple testing (you could of course if you wanted).\n\n\n\n\n\n\nNote\n\n\n\nRemember: a significance test with a significance threshold of alpha = 0.05 has a type I error rate of 5%. Thus, if you make 20 tests, you would get at least one significant result, even if there are no effects whatsoever. Corrections for multiple testing are used to compensate for this, usually aiming at controlling the family-wise error rate (FWER), the probability of making one or more type I errors when performing multiple hypotheses tests.\n\n\n\n\n2.6.4 Compact Letter Display\nIt is common to visualize the results of the post-hoc tests with the so-called Compact Letter Display (cld). This doesn’t work with the base TukeyHSD function, so we will use the multcomp pacakge:\n\nlibrary(multcomp)\n\nfit = lm(weight ~ group, data = PlantGrowth)\ntuk = glht(fit, linfct = mcp(group = \"Tukey\"))\nsummary(tuk)          # Standard display.\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = weight ~ group, data = PlantGrowth)\n\nLinear Hypotheses:\n                 Estimate Std. Error t value Pr(&gt;|t|)  \ntrt1 - ctrl == 0  -0.3710     0.2788  -1.331   0.3909  \ntrt2 - ctrl == 0   0.4940     0.2788   1.772   0.1979  \ntrt2 - trt1 == 0   0.8650     0.2788   3.103   0.0119 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\ntuk.cld = cld(tuk)    # Letter-based display.\nplot(tuk.cld)\n\nThe cld gives a new letter for each group of factor levels that are statistically undistinguishable. You can see the output via tuk.cld, here I only show the plot:"
  },
  {
    "objectID": "2A-LinearRegression.html#specifying-a-multiple-regression",
    "href": "2A-LinearRegression.html#specifying-a-multiple-regression",
    "title": "2  Linear Regression",
    "section": "2.7 Specifying a multiple regression",
    "text": "2.7 Specifying a multiple regression\nMultiple (linear) regression means that we consider more than 1 predictor in the same model. The syntax is very easy: Just add your predictors (numerical or categorical) to your regression formula, as in the following example for the airquality dataset. To be able to also add a factor, I created a new variable fMonth to have month as a factor (categorical):\n\nairquality$fMonth = factor(airquality$Month)\nfit = lm(Ozone ~ Temp + Wind + Solar.R + fMonth, data = airquality)\n\nIn principle, everything is interpreted as before: First of all, residual plots:\n\npar(mfrow =c(2,2))\nplot(fit)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nMaybe you have noted that the residual plots used fitted values as the x axis. This is in general the most parsimonious choice, as there are many predictors in a multiple regression, and each influences the response. However, if running a multiple regression, you should additionally plot residuals against each predictor separately - if doing so, you often see a misfit that doesn’t occur in the res ~ predicted plot.\n\n\nThe resulting regression table already looks a bit intimidating,\n\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Temp + Wind + Solar.R + fMonth, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.344 -13.495  -3.165  10.399  92.689 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -74.23481   26.10184  -2.844  0.00537 ** \nTemp          1.87511    0.34073   5.503 2.74e-07 ***\nWind         -3.10872    0.66009  -4.710 7.78e-06 ***\nSolar.R       0.05222    0.02367   2.206  0.02957 *  \nfMonth6     -14.75895    9.12269  -1.618  0.10876    \nfMonth7      -8.74861    7.82906  -1.117  0.26640    \nfMonth8      -4.19654    8.14693  -0.515  0.60758    \nfMonth9     -15.96728    6.65561  -2.399  0.01823 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.72 on 103 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6369,    Adjusted R-squared:  0.6122 \nF-statistic: 25.81 on 7 and 103 DF,  p-value: &lt; 2.2e-16\n\n\nLuckily, we also have the effect plots to make sense of this:\n\nlibrary(effects)\nplot(allEffects(fit, partial.residuals = T) )\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that for the multiple regression, that scatter around the regression line displayed by the effects package is not the raw data, but the so-called partial residuals. Essentially, what this means is that we plot the fitted line for the respective predictor, and then we calculate the residual for the full model (including the other predictors) and add them on the regression line. The advantage is that this subtracts the effect of other predictors, and removes possible spurious correlatins that occur due to collinearity between predictors (see next section).\n\n\n\nFormula syntax\n\n\n\n\n\n\n\nFormula\nMeaning\nDetails\n\n\n\n\ny~x_1\n\\(y=a_0 +a_1*x_1\\)\nSlope+Intercept\n\n\ny~x_1 - 1\n\\(y=a_1*x_1\\)\nSlope, no intercept\n\n\ny~I(x_1^2)\n\\(y=a_0 + a_1*(x_1^2)\\)\nQuadratic effect\n\n\ny~x_1+x_2\n\\(y=a_0+a_1*x_1+a_2*x_2\\)\nMultiple linear regression (two variables)\n\n\ny~x_1:x_2\n\\(y=a_0+a_1*(x_1*x_2)\\)\nInteraction between x1 and x2\n\n\ny~x_1*x_2\n\\(y=a_0+a_1*(x_1*x_2)+a_2*x_1+a_3*x_2\\)\nInteraction and main effects"
  },
  {
    "objectID": "2A-LinearRegression.html#the-effect-of-collinearity",
    "href": "2A-LinearRegression.html#the-effect-of-collinearity",
    "title": "2  Linear Regression",
    "section": "2.8 The Effect of Collinearity",
    "text": "2.8 The Effect of Collinearity\nA common misunderstanding is that a multiple regression is just a convenient way to run several independent univariate regressions. This, however, is wrong. A multiple regression is doing a fundamentally different thing. Let’s compare the results of the multiple regression above to the simple regression.\n\nfit = lm(Ozone ~ Wind , data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Wind, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.572 -18.854  -4.868  15.234  90.000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  96.8729     7.2387   13.38  &lt; 2e-16 ***\nWind         -5.5509     0.6904   -8.04 9.27e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.47 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.3619,    Adjusted R-squared:  0.3563 \nF-statistic: 64.64 on 1 and 114 DF,  p-value: 9.272e-13\n\n\nThe simple regression estimates an effect of Wind that is- 5.55, while in the multiple regression, we had -3.1.\nThe reason is that a multiple regression estimates the effect of Wind, corrected for the effect of the other variables, while a univariate regression estimates the raw correlation of Ozone and Wind. And this can make a big difference if Wind has correlations with other variables(the technical term is collinear). We can see, e.g., that Wind and Temp are indeed correlated by running\n\nplot(Wind ~ Temp, data = airquality)\n\n\n\n\nIn such a case, if Temp is not included in the regression, the effect of Temp will be absorbed in the Wind effect to the extent to which both variables are collinear. In other words, if one variable is missing, there will be a spillover of the effect of this variable to all collinear variables.\n\n\n\n\n\n\nTip\n\n\n\nTo give a simple example of this spillover, let’s say\n\nWind has a positive effect of 1\nTemp has a negative effect of -1\nWind and Temp are correlated 50%\n\nA multiple regression would estimate 1,-1. In a single regression of Wind against Temp, the collinear part (50%) of the Temp effect would be absorbed by wind, so we would estimate a Wind effect of 0.5.\n\n\nNote that the same happens in univariate plots of the correlation, i.e. this is not a problem of the linear regression, but rather of looking at the univariate correlation of Ozone ~ Wind without correcting for the effect of Temp.\nAlso, note that collinearity is not fundamentally a problem for the regression - regression estimates stay unbiased and p-values and CIs calibrated under arbitrary high collinearity. However, the more collinearity we have, the more uncertain regression estimates will become (with associated large p-values). Here an example, where we simulate data under the assumption that x1 and x2 have each an effect of 1\n\nset.seed(123)\nx1 = runif(100)\nx2 = 0.95 *x1 + 0.05 * runif(100)\ny = x1 + x2 + rnorm(100)\n\nsummary(lm(y ~ x1 + x2))\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8994 -0.6821 -0.1086  0.5749  3.3663 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  -0.1408     0.2862  -0.492    0.624\nx1            1.6605     7.0742   0.235    0.815\nx2            0.4072     7.4695   0.055    0.957\n\nResidual standard error: 0.9765 on 97 degrees of freedom\nMultiple R-squared:  0.2668,    Adjusted R-squared:  0.2517 \nF-statistic: 17.65 on 2 and 97 DF,  p-value: 2.91e-07\n\n\nWe see that the multiple regression is not terribly wrong, but very uncertain about the true values. Some authors therefore advice to remove collinear predictions. This can sometimes be useful, in particular for predictions (see section on model selection); however, note that the effect of removed predictor will then be absorbed by the remaining predictor(s). In our case, the univariate regression would be very sure that there is an effect, note that the CI does not cover the true effect of x1 (which was 1).\n\nsummary(lm(y ~ x1))\n\n\nCall:\nlm(formula = y ~ x1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9000 -0.6757 -0.1067  0.5799  3.3630 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.1295     0.1965  -0.659    0.511    \nx1            2.0456     0.3426   5.971 3.78e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9715 on 98 degrees of freedom\nMultiple R-squared:  0.2668,    Adjusted R-squared:  0.2593 \nF-statistic: 35.65 on 1 and 98 DF,  p-value: 3.785e-08\n\n\n\n\n\n\n\n\nUnderstanding collinearity spillover in more detail\n\n\n\n\n\nWe can understand the problem of one variable influencing the effect of the other in more detail if we simulate some data. Let’s create 2 positively collinear predictors:\n\nx1 = runif(100, -5, 5)\nx2 = x1 + 0.2*runif(100, -5, 5)\n\nWe can check whether this has worked:\n\ncor(x1, x2)\n\n[1] 0.9810774\n\n\nNow, the first case I want to look at, is when the effects for x1 and x2 have equal signs. Let’s create such a situation, by simulating a normal response \\(y\\), where the intercept is 0, and both predictors have effect = 1:\n\ny = 0 + 1*x1 + 1*x2 + rnorm(100)\n\nIf you look at the formula, you can nearly visually see what the problem of the univariate regression is: as x2 is nearly identical to x1, the apparent slope between y and either x1 or x2 will be around 2 (although the true effect is 1).\nConsequently, the univariate models will estimate too high effect sizes. We also say by taking out one predictor, the remaining one is absorbing the effect of the other predictor.\n\ncoef(lm(y ~ x1))\n\n(Intercept)          x1 \n0.002362246 2.003283129 \n\ncoef(lm(y ~ x2))\n\n(Intercept)          x2 \n-0.08366353  1.91355593 \n\n\nThe multivariate model, on the other hand, gets the right estimates (with a bit of error):\n\ncoef(lm(y~x1 + x2))\n\n(Intercept)          x1          x2 \n-0.03627352  1.05770683  0.92146207 \n\n\nYou can also see this visually:\n\npar(mfrow = c(1, 2))\nplot(x1, y, main = \"x1 effect\", ylim = c(-12, 12))\nabline(lm(y ~ x1))\n\n# Draw a line with intercept 0 and slope 1,\n# just like we simulated the true dependency of y on x1:\nabline(0, 1, col = \"red\")\n\nlegend(\"topleft\", c(\"fitted\", \"true\"), lwd = 1, col = c(\"black\", \"red\"))\nplot(x2, y, main = \"x2 effect\", ylim = c(-12, 12))\nabline(lm(y ~ x2))\nabline(0, 1, col = \"red\")\nlegend(\"topleft\", c(\"fitted\", \"true\"), lwd = 1, col = c(\"black\", \"red\"))\n\n\n\n\nCheck what happens if the 2 effects have opposite sign.\n\nx1 = runif(100, -5, 5)\nx2 = -x1 + 0.2*runif(100, -5, 5)\ny = 0 + 1*x1 + 1*x2 + rnorm(100)\n\ncor(x1, x2)\n\n[1] -0.9804515\n\ncoef(lm(y ~ x1))\n\n(Intercept)          x1 \n-0.03467159 -0.01170653 \n\ncoef(lm(y ~ x2))\n\n(Intercept)          x2 \n-0.04036562  0.04795055 \n\npar(mfrow = c(1, 2))\nplot(x1, y, main = \"x1 effect\", ylim = c(-12, 12))\nabline(lm(y ~ x1))\nabline(0, 1, col = \"red\")\nlegend(\"topleft\", c(\"fitted\", \"true\"), lwd = 1, col = c(\"black\", \"red\"))\nplot(x2, y, main = \"x2 effect\", ylim = c(-12, 12))\nabline(lm(y ~ x2))\nabline(0, 1, col = \"red\")\nlegend(\"topleft\", c(\"fitted\", \"true\"), lwd = 1, col = c(\"black\", \"red\"))\n\n\n\ncoef(lm(y~x1 + x2))\n\n(Intercept)          x1          x2 \n-0.04622019  0.91103840  0.94186191 \n\n\nBoth effects cancel out."
  },
  {
    "objectID": "2A-LinearRegression.html#centering-and-scaling-of-predictor-variables",
    "href": "2A-LinearRegression.html#centering-and-scaling-of-predictor-variables",
    "title": "2  Linear Regression",
    "section": "2.9 Centering and Scaling of Predictor Variables",
    "text": "2.9 Centering and Scaling of Predictor Variables\n\n2.9.1 Centering\nIn our multiple regression, we saw an intercept of -74. Per definition, the intercept is the predicted value for \\(y\\) (Ozone) at a value of 0 for all other variables. It’s fine to report this, as long as we are interested in this value, but often the value at predictor = 0 is not particularly interesting. In this specific case, a value of -74 clearly doesn’t make sense, as Ozone concentrations can only be positive.\nTo explain what’s going on, let’s look at the univariate regression against Temp:\n\nfit = lm(Ozone ~ Temp, data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n\n\nNow, the intercept is -146. We can see the reason more clearly when we plot the results:\n\nplot(Ozone ~ Temp, data = airquality, xlim = c(-10, 110), ylim = c(-200, 170))\nabline(fit)\nabline(h = 0, lty = 2)\nabline(v = 0, lty = 2)\n\n\n\n\nThat shows us that the value 0 is far outside of the set of our observed values for Temp, which is measured in Fahrenheit. Thus, we are extrapolating the Ozone far beyond the observed data. To avoid this, we can simply re-define the x-Axis, by subtracting the mean, which is called centering:\n\nairquality$cTemp = airquality$Temp - mean(airquality$Temp)\n\nAlternatively, you can center with the build-in R command scale\n\nairquality$cTemp = scale(airquality$Temp, center = T, scale = F)\n\nFitting the model with the centered variable moves the intercept line in the middle of the predictor range\n\nfit = lm(Ozone ~ cTemp, data = airquality)\nplot(Ozone ~ cTemp, data = airquality)\nabline(fit)\nabline(v = 0, lty = 2)\n\n\n\n\nwhich produces a more interpretable value for the intercept: when we center, the intercept of the centered variable can be interpreted as the Ozone concentrate at the mean temperature. For a univariate regression, this value will also typically be very similar to the grand mean mean(airquality$Ozone).\n\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ cTemp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  42.1576     2.2018   19.15   &lt;2e-16 ***\ncTemp         2.4287     0.2331   10.42   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThere is no simple way to center categorical variables for a fixed effect model. Thus, note that if you center, the intercept is predicted for the reference group of all categorical variables, evaluated at value of the mean for all numeric variables. However, as we will see later, random effects, among other things, are a means to center categorical variables, as they estimate a grand mean over all groups.\n\n\n\n\n2.9.2 Scaling\nAnother very common transformation is to divide the x axis by a certain value to bring all variables on a similar scale. This is called scaling.\nThe main motivation for scaling is to make effect sizes in a multiple regression more comparable. As an example, look again at the multiple regression with 4 predictors:\n\nairquality$fMonth = factor(airquality$Month)\nfit = lm(Ozone ~ Temp + Wind + Solar.R + fMonth, data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Temp + Wind + Solar.R + fMonth, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.344 -13.495  -3.165  10.399  92.689 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -74.23481   26.10184  -2.844  0.00537 ** \nTemp          1.87511    0.34073   5.503 2.74e-07 ***\nWind         -3.10872    0.66009  -4.710 7.78e-06 ***\nSolar.R       0.05222    0.02367   2.206  0.02957 *  \nfMonth6     -14.75895    9.12269  -1.618  0.10876    \nfMonth7      -8.74861    7.82906  -1.117  0.26640    \nfMonth8      -4.19654    8.14693  -0.515  0.60758    \nfMonth9     -15.96728    6.65561  -2.399  0.01823 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.72 on 103 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6369,    Adjusted R-squared:  0.6122 \nF-statistic: 25.81 on 7 and 103 DF,  p-value: &lt; 2.2e-16\n\n\nSo, which of the predictors is the strongest (= largest effect on the response)? Superficially, it looks as if Month has the highest values. But we have to remember that the effect on the response is \\(y = \\text{regression estimate} * \\text{predictor}\\), i.e if we have a predictor with a large range (difference between min/max values), it may have a strong effect even though the estimate is small. So, if the predictors have a different range, we cannot compare effect sizes directly regarding their “effective influence” on y.\nTo make the effect sizes comparable, we can scale all numeric predictors by dividing them by their standard deviation, which will bring them all roughly on a range between -2, 2. You can do this by hand, or use the scale() function in R:\n\n\n\n\n\n\nTip\n\n\n\nBy default, the scale(...) function will scale and center. As discussed before, centering is nearly always useful as it improves the interpretability of the intercept, so I would suggest to use this as a default when scaling.\n\n\n\nairquality$sTemp = scale(airquality$Temp)\nairquality$sWind = scale(airquality$Wind)\nairquality$sSolar.R = scale(airquality$Solar.R)\n\n\n\n\n\n\n\nTip\n\n\n\nHere, I create a new variable for each scaled predictor. It is possible to use the scale() command inside the regression formula as well, but that sometimes creates problems with downstream plotting functions, so I recommend to create a new variable in your data for scaling.\n\n\nRunning the regression:\n\nfit = lm(Ozone ~ sTemp + sWind + sSolar.R + fMonth, data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ sTemp + sWind + sSolar.R + fMonth, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.344 -13.495  -3.165  10.399  92.689 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   50.558      5.526   9.150 5.74e-15 ***\nsTemp         17.748      3.225   5.503 2.74e-07 ***\nsWind        -10.952      2.325  -4.710 7.78e-06 ***\nsSolar.R       4.703      2.131   2.206   0.0296 *  \nfMonth6      -14.759      9.123  -1.618   0.1088    \nfMonth7       -8.749      7.829  -1.117   0.2664    \nfMonth8       -4.197      8.147  -0.515   0.6076    \nfMonth9      -15.967      6.656  -2.399   0.0182 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.72 on 103 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6369,    Adjusted R-squared:  0.6122 \nF-statistic: 25.81 on 7 and 103 DF,  p-value: &lt; 2.2e-16\n\n\nAs you see, effect sizes have now changed considerably. The difference in interpretation is the following: for the unscaled variables, we estimate the effect of 1 unit change (e.g. 1 degree for temp). For the scaled variable, we estimate the effect of a change of 1 standard deviation. If we look at the sd of temperature\n\nsd(airquality$Temp)\n\n[1] 9.46527\n\n\nwe can conclude that the scaled regression is estimating an effect of a 9.46527 degree change in temperature.\nWhile the latter is numerically a bit confusing, the advantage is that so we can interpret the Ozone effect scaled to typical temperature differences in the data, and the same for all other variables. Consequently, it makes sense to compare the scaled effect sizes between variables, which suggests that Temp is the most important predictor.\nWhat about categorical variables, do we need to scale them as well? The answer is no, because they are effectively already scaled to a range of 1, because in the standard contrasts (treatment effects), we estimate the effect of going from option 1 to option 2. However, note that because the sd scaling creates a numeric range with a average of +/-1 sd = 2, some authors also scale by 2 sd, so that numeric and categorical variables are more comparable in their effect sizes.\n\n\n\n\n\n\nDiscuss\n\n\n\nUnder which circumstances should you center / scale, and how should you discuss the estimated coefficients in a paper?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nScaling: scaled, you get estimate of relative importance, while unscaled, effects are interpretable in their original units. Depending on what you are more interested in, you may report one of the two, or both. Centering: changes the place of the origin, and thus the interpretation of the intercept (and possibly also main effects if interactions are present, see next chapter). Per default, I would scale.\n\n\n\n\n\n\n\n\n\nExcercise\n\n\n\nHow does adding or multiplying a factor on the predictor change the intercept and slope (effect, CI, p-values) of a regression?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOriginal model\n\nfit = lm(Ozone ~ Temp, data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n\n\n\nfit = lm(Ozone ~ I(Temp + 100), data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ I(Temp + 100), data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -389.8658    41.5257  -9.389 7.39e-16 ***\nI(Temp + 100)    2.4287     0.2331  10.418  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n\n\nAdditive transformation change the intercept value. All p-values, CIs stay the same (except for the intercept, as the test contrast changes)\n\nfit = lm(Ozone ~ I(Temp * 10), data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ I(Temp * 10), data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -146.99549   18.28717  -8.038 9.37e-13 ***\nI(Temp * 10)    0.24287    0.02331  10.418  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: &lt; 2.2e-16\n\n\nMultiplicative transformations change the slope value. P-values and relative CIs for intercept and slope stay the same. Combinations of both have both effects together"
  },
  {
    "objectID": "2A-LinearRegression.html#interactions",
    "href": "2A-LinearRegression.html#interactions",
    "title": "2  Linear Regression",
    "section": "2.10 Interactions",
    "text": "2.10 Interactions\n\n2.10.1 Syntax\nWhen we have multiple variables, we can have the situation that the value of one variable influences the effect of the other(s). Technically, this is called in interaction. In situations where the causal direction is known, this is also called a moderator. An example: Imagine we observe that the effect of aspirin differs depending on the weight of the subject. Technically, we have an interaction between aspirin and weight. Physiologically, we know the causal direction is “weight -&gt; effect of aspirin”, so we can say weight is a moderator for the effect of aspirin.\n\nfit = lm(Ozone ~ Temp * Wind, data = airquality)\nplot(allEffects(fit))\n\n\n\n\nWe will have a look at the summary later, but for the moment, let’s just look at the output visually. In the effect plots, we see the effect of Temperature on Ozone for different values of Wind. We also see that the slope changes. For low Wind, we have a strong effect of Temperature. For high Wind, the effect is basically gone.\nLet’s look at the interaction syntax in more detail. The “*” operator in an lm() is a shorthand for main effects + interactions. You can write equivalently:\n\nfit = lm(Ozone ~ Wind + Temp + Wind:Temp, data = airquality)\n\nWhat is fit here is literally a third predictor that is specified as Wind * Temp (normal multiplication). The above syntax would allow you to also have interactions without main effects, e.g.:\n\nfit = lm(Ozone ~ Wind + Wind:Temp, data = airquality)\n\nAlthough this is generally never advisable, as the main effect influences the interaction, unless you are sure that the main effect must be zero.\nThere is another important syntax in R:\n\nfit = lm(Ozone ~ (Wind + Temp + Solar.R)^2 , data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ (Wind + Temp + Solar.R)^2, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.685 -11.727  -2.169   7.360  91.244 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -1.408e+02  6.419e+01  -2.193  0.03056 * \nWind          1.055e+01  4.290e+00   2.460  0.01555 * \nTemp          2.322e+00  8.330e-01   2.788  0.00631 **\nSolar.R      -2.260e-01  2.107e-01  -1.073  0.28591   \nWind:Temp    -1.613e-01  5.896e-02  -2.735  0.00733 **\nWind:Solar.R -7.231e-03  6.688e-03  -1.081  0.28212   \nTemp:Solar.R  5.061e-03  2.445e-03   2.070  0.04089 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.17 on 104 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6863,    Adjusted R-squared:  0.6682 \nF-statistic: 37.93 on 6 and 104 DF,  p-value: &lt; 2.2e-16\n\nplot(allEffects(fit), selection = 1)\n\n\n\nplot(allEffects(fit), selection = 2)\n\n\n\nplot(allEffects(fit), selection = 3)\n\n\n\n\nThis creates all main effect and second order (aka two-way) interactions between variables. You can also use ^3 to create all possible 2-way and 3-way interactions between the variables in the parentheses. By the way: The ()^2 syntax for interactions is the reason why we have to write I(x^2) if we want to write a quadratic effect in an lm.\n\n\n2.10.2 Interactions with categorical variables\nWhen you include an interaction with a categorical variable, that means a separate effect will be fit for each level of the categorical variable, as in\n\nfit = lm(Ozone ~ Wind * fMonth, data = airquality)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Wind * fMonth, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-54.528 -12.562  -2.246  10.691  77.750 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    50.748     15.748   3.223  0.00169 ** \nWind           -2.368      1.316  -1.799  0.07484 .  \nfMonth6       -41.793     31.148  -1.342  0.18253    \nfMonth7        68.296     20.995   3.253  0.00153 ** \nfMonth8        82.211     20.314   4.047 9.88e-05 ***\nfMonth9        23.439     20.663   1.134  0.25919    \nWind:fMonth6    4.051      2.490   1.627  0.10680    \nWind:fMonth7   -4.663      2.026  -2.302  0.02329 *  \nWind:fMonth8   -6.154      1.923  -3.201  0.00181 ** \nWind:fMonth9   -1.874      1.820  -1.029  0.30569    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.12 on 106 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.5473,    Adjusted R-squared:  0.5089 \nF-statistic: 14.24 on 9 and 106 DF,  p-value: 7.879e-15\n\n\nThe interpretation is like for a single categorical predictor, i.e. we see the effect of Wind as the effect for the first Month 5, and the Wind:fMonth6 effect, for example, tests for a difference in the Wind effect between month 5 (reference) and month 6. As before, you could change this behavior by changing contrasts.\n\n\n2.10.3 Interactions and centering\nA super important topic when working with numeric interactions is centering. To get this started, let’s make a small experiment:\n\n\n\n\n\n\nTask\n\n\n\nCompare the estimates for Wind / Temp for the following models\n\nOzone ~ Wind\nOzone ~ Temp\nOzone ~ Wind + Temp\nOzone ~ Wind * Temp\n\nHow do you explain the differences in the estimates for the main effects of Wind and Temp? What do you think corresponds most closely to the “true” effect of Wind and Temp?\n\n\nWhat you should have seen in the models above is that the main effects for Wind / Temp change significantly when the interaction is introduced.\nMaybe you know the answer already. If not, consider the following simulation, where we create data with known effect sizes:\n\n# Create predictor variables.\nx1 = runif(100, -1, 1)\nx2 = runif(100, -1, 1)\n\n# Create response for lm, all effects are 1.\ny = x1 + x2 + x1*x2 + rnorm(100, sd = 0.3)\n\n# Fit model, but shift the mean of the predictor.\nfit = lm(y ~ x1 * I(x2 + 5))\nsummary(fit)\n\n\nCall:\nlm(formula = y ~ x1 * I(x2 + 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.78590 -0.24028 -0.01261  0.23085  0.72614 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -5.52248    0.32056 -17.227  &lt; 2e-16 ***\nx1           -3.56776    0.60688  -5.879 5.98e-08 ***\nI(x2 + 5)     1.09592    0.06277  17.460  &lt; 2e-16 ***\nx1:I(x2 + 5)  0.89799    0.11935   7.524 2.85e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3248 on 96 degrees of freedom\nMultiple R-squared:  0.8632,    Adjusted R-squared:  0.8589 \nF-statistic: 201.9 on 3 and 96 DF,  p-value: &lt; 2.2e-16\n\nplot(allEffects(fit))\n\n\n\n\nPlay around with the shift in x2, and observe how the effects change. Try how the estimates change when centering the variables via the scale() command. If you understand what’s going on, you will realize that you should always center your variables, whenever you use any interactions.\nExcellent explanations of the issues also in the attached paper\nhttps://besjournals.onlinelibrary.wiley.com/doi/epdf/10.1111/j.2041-210X.2010.00012.x.\n\n\n2.10.4 Interactions in GAMs\nThe equivalent of an interaction in a GAM is a tensor spline. It describes how the response varies as a smooth function of 2 predictors. You can specify a tensor spline with the te() command, as in\n\nlibrary(mgcv)\nfit = gam(Ozone ~ te(Wind, Temp) , data = airquality)\nplot(fit, pages = 0, residuals = T, pch = 20, lwd = 1.9, cex = 0.4)"
  },
  {
    "objectID": "2A-LinearRegression.html#predictions",
    "href": "2A-LinearRegression.html#predictions",
    "title": "2  Linear Regression",
    "section": "2.11 Predictions",
    "text": "2.11 Predictions\nTo predict with a fitted model\n\nfit = lm(Ozone ~ Wind, data = airquality)\n\nwe can use the predict() function.\n\npredict(fit) # predicts on current data\npredict(fit, newdata = X) # predicts on new data\npredict(fit, se.fit = T)\n\nTo generate confidence intervals on predictions, we can use se.fit = T, which returns standard errors on predictions. As always, 95% confidence interval (CI) = 1.96 * se.\nSo, let’s assume we want to predict Ozone with a 95% CI for for wind between 0 and 10:\n\nWind = seq(0,10,0.1)\nnewData = data.frame(Wind = Wind)\npred = predict(fit, newdata = newData, se.fit = T)\nplot(Wind, pred$fit, type = \"l\")\nlines(Wind, pred$fit - 1.96 * pred$se.fit, lty = 2)\nlines(Wind, pred$fit + 1.96 * pred$se.fit, lty = 2)"
  },
  {
    "objectID": "2A-LinearRegression.html#nonparametrics-and-the-bootstrap",
    "href": "2A-LinearRegression.html#nonparametrics-and-the-bootstrap",
    "title": "2  Linear Regression",
    "section": "2.12 Nonparametrics and the bootstrap",
    "text": "2.12 Nonparametrics and the bootstrap\nThe linear model (LM) has the beautiful property that it delivers unbiased estimates, calibrated p-values and all that based on analytical expressions that remain valid, regardless of the sample size. Unfortunately, there are very few extensions of the LM that completely maintain all of these desirable properties for limited sample sizes. The underlying problem is that for more complicated models, there are usually no closed-form solutions for the expected null or sampling distributions, or those distributions depend on the observed data. For that reason, non-parametric methods to generate confidence intervals and p-values are increasingly used when going to more complicated models, which motivates this first small excursion into non-parametric methods, and in particular cross-validation and the bootstrap.\nThere are two non-parametric methods that you should absolutely know and that form the backbone of basically all non-parametric for regression analyses with statistical and machine learning models. Those two are:\n\nthe bootstrap: generate variance of estimates, allows us to calculate non-parametric CIs and p-values\ncross-validation: generate non-parametric estimates of the predictive error of the model (connects to ANOVA, AIC, see later sections on ANOVA and model selection)\n\nHere, we will look at the bootstrap. Cross-validation will be introduced in the chapter on model selection.\n\n\n\n\n\n\nExcursion: randomization null models\n\n\n\n\n\nAnother non-parametric method that you may encounter are randomization null models. Let’s look at this with an example: here, we create data from a normal and a lognormal distribution. We are interested in the difference of the mean of the two (test statistic).\n\nset.seed(1337)\n\ngroupA = rnorm(50)\ngroupB = rlnorm(50)\n\ndat = data.frame(value = c(groupA, groupB), group = factor(rep(c(\"A\", \"B\"), each = 50)))\nplot(value ~ group, data = dat)\n\n\n\n# test statistic: difference of the means\nreference = mean(groupA) - mean(groupB)\n\nWe can show mathematically that means of repeated draws from a normal distribution are Student-t distributed, which is why we can use this distribution in the t-test to calculate the probability of a certain deviation from a null expectation. If we work with different distributions, however, we may not be so sure what the null distribution should be. Either we have to do the (possibly very complicated) math to get a closed-form expression for this problem, or we try to generate a non-parametric expectation for the null distribution via a randomization null model.\nThe idea is that we generate a null expectation for the test statistic by re-shuffling the data in a way that conforms to the null assumption. In this case, we can just randomize the group labels, which means that we assume that both distributions (A,B) are the same (note that this is a bit more than asking if they have the same mean, but for the sake of the example, we are happy with this simplification).\n\nnSim = 5000\nnullDistribution = rep(NA, nSim)\n\nfor(i in 1:nSim){\n  sel = dat$value[sample.int(100, size = 100)]\n  nullDistribution[i] = mean(sel[1:50]) - mean(sel[51:100])\n}\n\nhist(nullDistribution, xlim = c(-2,2))\nabline(v = reference, col = \"red\")\n\n\n\necdf(nullDistribution)(reference) # 1-sided p-value\n\n[1] 0\n\n\nVoila, we have a p-value, and it is significant. Randomization null models are used in many R packages where analytical p-values are not available, e.g., in the packages vegan (ordinations, community analysis) and bipartide (networks).\n\n\n\n\n2.12.1 Non-parametric bootstrap\nThe key problem to calculate parametric p-values or CIs is that we need to know the distribution of the statistics of interest that we would obtain if we (hypothetically) repeated the experiment many times. The bootstrap generates new data so that we can approximate this distribution. There are two variants of the bootstrap:\n\nNon-parametric bootstrap: we sample new data from the old data with replacement\nParametric bootstrap: we sample new data from the fitted model\n\nLet’s first look at the non-parametric bootstrap, using a simple regression of Ozone ~ Wind\n\nfit = lm(Ozone ~ Wind, data = airquality)\n\nFirst, for convenience, we generate a function that extracts the estimate that we want to bootstrap from a fitted model. I decided to extract the slope estimate for Wind, but you could make difference choices\n\ngetEstimate &lt;- function(model){\n  coef(model)[2]\n}\ngetEstimate(fit)\n\n     Wind \n-5.550923 \n\n\nNext, we perform the non-parametric bootstrap\n\nperformBootstrap = function(){\n  sel = sample.int(n = nrow(airquality), replace = T)\n\n  fitNew = lm(Ozone ~ Wind, data = airquality[sel,])\n  return(getEstimate(fitNew))\n}\n\nbootstrappedEstimate = replicate(1000, performBootstrap())\n\nThe bootstrapped estimates of the slope can basically be interpreted as the uncertainty of the slope estimate.\n\nhist(bootstrappedEstimate, breaks = 50)\nabline(v = mean(bootstrappedEstimate), col = \"red\", lwd = 2)\nabline(v = getEstimate(fit), col = \"red\", lwd = 2, lty = 2)\n\n\n\nsd(bootstrappedEstimate)\n\n[1] 0.8314342\n\n\nWe estimate a standard error of 0.84, slightly higher than the parametric error of 0.69 returned by summary.lm()\n\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Wind, data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.572 -18.854  -4.868  15.234  90.000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  96.8729     7.2387   13.38  &lt; 2e-16 ***\nWind         -5.5509     0.6904   -8.04 9.27e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.47 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.3619,    Adjusted R-squared:  0.3563 \nF-statistic: 64.64 on 1 and 114 DF,  p-value: 9.272e-13\n\n\nIn R, you don’t have to code the bootstrap by hand, you can also use the boot package. The code will not neccessarily be shorter, because you still need to code the statistics, but it offers a few convenient downstream functions for CIs etc.\n\n\n\n\n\n\nCode for same example using the boot package\n\n\n\n\n\n\n# see help, needs arguments d = data, k = index\ngetBoot = function(d, k){\n  fitNew = lm(Ozone ~ Wind, data = d[k,])\n  return(getEstimate(fitNew))\n}\n\nlibrary(boot)\n\n\nAttaching package: 'boot'\n\n\nThe following object is masked from 'package:survival':\n\n    aml\n\nb1 = boot(airquality, getBoot, R = 1000)\nb1\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = airquality, statistic = getBoot, R = 1000)\n\n\nBootstrap Statistics :\n     original      bias    std. error\nt1* -5.550923 -0.04177248   0.8113522\n\n\n\n\n\n\n\n2.12.2 Parametric bootstrap\nThe non-parametric bootstrap does the same thing, but the data is not re-sampled, but generated from the fitted model. This can be done conveniently in R as long as the regression package implements a simulate() function that works on the fitted model.\n\nperformBootstrapParametric = function(){\n\n  newData = model.frame(fit)\n  newData$Ozone = unlist(simulate(fit))\n\n  fitNew = lm(Ozone ~ Wind, data = newData)\n  return(getEstimate(fitNew))\n}\n\nbootstrappedEstimateParametric = replicate(1000,\n                                           performBootstrapParametric())\n\nIn the results, we see that the parametric boostrap is much closer to the analytical results\n\nhist(bootstrappedEstimateParametric, breaks = 50)\nabline(v = mean(bootstrappedEstimateParametric), col = \"red\", lwd = 2)\nabline(v = getEstimate(fit), col = \"red\", lwd = 2, lty = 2)\n\n\n\nsd(bootstrappedEstimateParametric)\n\n[1] 0.6835152\n\n\n\n\n\n\n\n\nCode for same example using the boot package\n\n\n\n\n\nFor the parametric boostrap, we just need a function of the data to return the estimate\n\ngetBoot = function(d){\n  fitNew = lm(Ozone ~ Wind, data = d)\n  return(getEstimate(fitNew))\n}\n\nPlus, we need a function that generates the simulations.\n\nrgen = function(dat, mle){\n  newData = model.frame(mle)\n  newData$Ozone = unlist(simulate(mle))\n  return(newData)\n}\n\nThe call to boot is then\n\nb2 = boot(airquality, getBoot, R = 1000, sim = \"parametric\", ran.gen = rgen, mle = fit)\nb2\n\n\nPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = airquality, statistic = getBoot, R = 1000, sim = \"parametric\", \n    ran.gen = rgen, mle = fit)\n\n\nBootstrap Statistics :\n     original     bias    std. error\nt1* -5.550923 0.02205086   0.6735757"
  },
  {
    "objectID": "2A-LinearRegression.html#case-studies",
    "href": "2A-LinearRegression.html#case-studies",
    "title": "2  Linear Regression",
    "section": "2.13 Case studies",
    "text": "2.13 Case studies\n\n2.13.1 Univariate Regression\n\n\n\n\n\n\nPlantheight\n\n\n\nLook at the plantHeight dataset in Ecodata. Let’s assume we want to analyze whether height of plant species from around the world depends on temperature at the location of occurrence. Note that “loght” = log(height).\n\nlibrary(EcoData)\n\n\nAttaching package: 'EcoData'\n\n\nThe following objects are masked from 'package:boot':\n\n    melanoma, nitrofen\n\n\nThe following object is masked from 'package:survival':\n\n    rats\n\n\nThe following object is masked from 'package:MASS':\n\n    snails\n\nplot(loght ~ temp, data = plantHeight)\n\n\n\n\n\nFit a linear model for this relationship. Perform residual checks and modify the model if you think it is necessary. Interpret the fitted model - how would you present the results in a scientific publication?\nThe data set also includes a categorical variable “growthform”. Test if growthform has an effect on the plant height. Also, consider changing the reference level of the treatment contrasts in this case.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n1. Residual checks\n\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\nLooks OK!\n2. Including “growthform”\n\nmodel2 = lm(loght ~ growthform, data = plantHeight)\nsummary(model2)\n\n\nCall:\nlm(formula = loght ~ growthform, data = plantHeight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.32294 -0.26091  0.03608  0.24666  1.50761 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)           0.25527    0.41664   0.613   0.5409  \ngrowthformHerb       -0.66946    0.42135  -1.589   0.1140  \ngrowthformHerb/Shrub -0.07918    0.58921  -0.134   0.8933  \ngrowthformShrub      -0.08724    0.42087  -0.207   0.8361  \ngrowthformShrub/Tree  0.56999    0.43365   1.314   0.1906  \ngrowthformTree        1.00564    0.42004   2.394   0.0178 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4166 on 162 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.7366,    Adjusted R-squared:  0.7285 \nF-statistic: 90.62 on 5 and 162 DF,  p-value: &lt; 2.2e-16\n\n\nThere is an effect of growth form, however, note that the comparisons are against the growth form fern (intercept), which has only one observation (table(plantHeight$growthform)), so it may make sense to re-order the factor in the regression so that you compare, e.g., against Herb (will yield more significant comparisons).\n\nplantHeight$growthform2 = relevel(as.factor(plantHeight$growthform), \"Herb\")\nmodel2 = lm(loght ~ growthform2, data = plantHeight)\nsummary(model2)\n\n\nCall:\nlm(formula = loght ~ growthform2, data = plantHeight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.32294 -0.26091  0.03608  0.24666  1.50761 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)           -0.41419    0.06281  -6.594 5.76e-10 ***\ngrowthform2Fern        0.66946    0.42135   1.589    0.114    \ngrowthform2Herb/Shrub  0.59028    0.42135   1.401    0.163    \ngrowthform2Shrub       0.58222    0.08653   6.728 2.81e-10 ***\ngrowthform2Shrub/Tree  1.23945    0.13569   9.135 2.57e-16 ***\ngrowthform2Tree        1.67510    0.08241  20.327  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4166 on 162 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.7366,    Adjusted R-squared:  0.7285 \nF-statistic: 90.62 on 5 and 162 DF,  p-value: &lt; 2.2e-16\n\n\nIt would also make sense in this case to follow up with an ANOVA and possible post-hoc tests.\n\n\n\n\n\n\n\n\n\nsolomonislands\n\n\n\nLook at the solomonislands dataset in Ecodata.\n\nlibrary(EcoData)\n?solomonislands\nsummary(solomonislands)\n\n\nFit a lm, trying to fit the classical relationship of Species ~ log(Area)\nPerform residual checks, and change the model if neccessary\nInterpret the results\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n1. Fitting the species-area curve \n\nmodel = lm(Species ~ log(Area), data = solomonislands)\nsummary(model)\n\n\nCall:\nlm(formula = Species ~ log(Area), data = solomonislands)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-31.422  -3.705   3.460   6.309  13.014 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   30.105      1.613   18.67   &lt;2e-16 ***\nlog(Area)      4.935      0.359   13.75   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.6 on 50 degrees of freedom\nMultiple R-squared:  0.7907,    Adjusted R-squared:  0.7866 \nF-statistic: 188.9 on 1 and 50 DF,  p-value: &lt; 2.2e-16\n\nplot(allEffects(model))\n\n\n\n\nArea has a small positive significant effect on the number of species.\n2. Residual checks\n\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\nResiduals don’t look particularly normal … would get better fitting ~ log(Area+1), but that doesn’t correspond to the classical model. In this case, it’s a trade-off between fitting the classical model and get better residuals.\nWith advanced methods, we could also think about GLMs or quantile regressions (see next chapter).\n3. Interpretation\nSignificant increase of species richness with area that follows a classical log(Area) relationship.\n\n\n\n\n\n2.13.2 Mulitple Regression\n\n\n\n\n\n\nPlant Height revisited\n\n\n\nRevisit exercise our previous analysis of EcoData::plantHeight\n\nlibrary(EcoData)\nmodel = lm(loght ~ temp, data = plantHeight)\n\nUse (separate) multiple regressions to test if:\n\nIf temp or NPP (net primary productivity) is a more important predictor.\nIf growth forms (variable growthform) differ in their temperature effects.\nIf the effect of temp remains significant if we include latitude and an interaction of latitude with temp. If not, why? Tip: plot temp ~ lat.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplantHeight$sTemp = scale(plantHeight$temp)\nplantHeight$sLat = scale(plantHeight$lat)\nplantHeight$sNPP = scale(plantHeight$NPP)\n\n# relevel \nplantHeight$growthform2 = relevel(as.factor(plantHeight$growthform), \"Herb\")\n\n\nNPP or Temp?\n\n\nfit = lm(loght ~ sTemp + sNPP, data = plantHeight)\nsummary(fit)\n\n\nCall:\nlm(formula = loght ~ sTemp + sNPP, data = plantHeight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.69726 -0.47935  0.04285  0.39812  1.77919 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.44692    0.05119   8.731 2.36e-15 ***\nsTemp        0.20846    0.07170   2.907 0.004134 ** \nsNPP         0.24734    0.07164   3.452 0.000702 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6711 on 169 degrees of freedom\n  (6 observations deleted due to missingness)\nMultiple R-squared:  0.2839,    Adjusted R-squared:  0.2754 \nF-statistic:  33.5 on 2 and 169 DF,  p-value: 5.553e-13\n\n\nNPP is slightly more important\n\nInteraction with growth form\n\n\nfit = lm(loght ~ growthform2 *  sTemp , data = plantHeight)\nsummary(fit)\n\n\nCall:\nlm(formula = loght ~ growthform2 * sTemp, data = plantHeight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.19634 -0.21217 -0.00997  0.22750  1.62398 \n\nCoefficients: (2 not defined because of singularities)\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 -0.310748   0.062150  -5.000 1.51e-06 ***\ngrowthform2Fern              0.624160   0.375650   1.662 0.098586 .  \ngrowthform2Herb/Shrub        0.456394   0.377088   1.210 0.227967    \ngrowthform2Shrub             0.562799   0.083100   6.773 2.36e-10 ***\ngrowthform2Shrub/Tree        0.957088   0.486858   1.966 0.051069 .  \ngrowthform2Tree              1.586005   0.080756  19.640  &lt; 2e-16 ***\nsTemp                        0.203808   0.053231   3.829 0.000185 ***\ngrowthform2Fern:sTemp              NA         NA      NA       NA    \ngrowthform2Herb/Shrub:sTemp        NA         NA      NA       NA    \ngrowthform2Shrub:sTemp       0.103357   0.076860   1.345 0.180636    \ngrowthform2Shrub/Tree:sTemp -0.004614   0.526866  -0.009 0.993024    \ngrowthform2Tree:sTemp       -0.244410   0.077661  -3.147 0.001971 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3713 on 158 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.796, Adjusted R-squared:  0.7844 \nF-statistic: 68.51 on 9 and 158 DF,  p-value: &lt; 2.2e-16\n\n\nYes, because (some) interactions are significant.\nNote that the n.s. effect of sTemp is the first growth form (Ferns), for which we had only one observation. In a standard multiple regression, you don’t have p-values for the significance of the temperature effect against 0 for the other growth forms, because you test against the reference. What one usually does is to run an ANOVA (see chapter on ANOVA) to see if temp is overall significant.\n\nanova(lm(loght ~ growthform *  sTemp , data = plantHeight))\n\nAnalysis of Variance Table\n\nResponse: loght\n                  Df Sum Sq Mean Sq  F value    Pr(&gt;F)    \ngrowthform         5 78.654 15.7309 114.1241 &lt; 2.2e-16 ***\nsTemp              1  3.543  3.5426  25.7006 1.104e-06 ***\ngrowthform:sTemp   3  2.800  0.9333   6.7707 0.0002524 ***\nResiduals        158 21.779  0.1378                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAlternatively, if you want to test if a specific growth form has a significant temperature effect, you could either extract the p-value from a multiple regression (a bit complicated) or just run a univariate regression for this growth form\n\nfit = lm(loght ~ sTemp + 0, data = plantHeight[plantHeight$growthform == \"Tree\",])\nsummary(fit)\n\n\nCall:\nlm(formula = loght ~ sTemp + 0, data = plantHeight[plantHeight$growthform == \n    \"Tree\", ])\n\nResiduals:\n   Min     1Q Median     3Q    Max \n0.2636 0.7198 0.9672 1.3503 2.3914 \n\nCoefficients:\n      Estimate Std. Error t value Pr(&gt;|t|)   \nsTemp   0.5013     0.1699    2.95  0.00452 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.21 on 60 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.1267,    Adjusted R-squared:  0.1121 \nF-statistic: 8.704 on 1 and 60 DF,  p-value: 0.004522\n\n\nOr you could fit the interaction but turn-off the intercept (by saying +0 or -1) and remove the plantHeight intercepts:\n\nfit = lm(loght ~ sTemp:growthform + 0, data = plantHeight[,])\nsummary(fit)\n\n\nCall:\nlm(formula = loght ~ sTemp:growthform + 0, data = plantHeight[, \n    ])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5156 -0.1396  0.3488  0.8103  2.3914 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \nsTemp:growthformFern        -0.8949     2.9233  -0.306 0.759911    \nsTemp:growthformHerb         0.3195     0.1077   2.967 0.003460 ** \nsTemp:growthformHerb/Shrub   1.1788     5.5825   0.211 0.833026    \nsTemp:growthformShrub        0.2375     0.1197   1.984 0.048974 *  \nsTemp:growthformShrub/Tree   0.8833     0.2613   3.380 0.000908 ***\nsTemp:growthformTree         0.5013     0.1171   4.281 3.17e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8339 on 162 degrees of freedom\n  (10 observations deleted due to missingness)\nMultiple R-squared:  0.2083,    Adjusted R-squared:  0.179 \nF-statistic: 7.106 on 6 and 162 DF,  p-value: 9.796e-07\n\n\n\nInteraction with lat\n\n\nfit = lm(loght ~ sTemp * sLat, data = plantHeight)\nsummary(fit)\n\n\nCall:\nlm(formula = loght ~ sTemp * sLat, data = plantHeight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.97905 -0.45112  0.01062  0.42852  1.74054 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.46939    0.06771   6.932 7.78e-11 ***\nsTemp        0.26120    0.14200   1.839   0.0676 .  \nsLat        -0.13072    0.13616  -0.960   0.3383    \nsTemp:sLat   0.01209    0.04782   0.253   0.8007    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6869 on 174 degrees of freedom\nMultiple R-squared:  0.2504,    Adjusted R-squared:  0.2375 \nF-statistic: 19.37 on 3 and 174 DF,  p-value: 6.95e-11\n\n\nAll is n.s. … how did this happen? If we check the correlation between temp and lat, we see that the two predictors are highly collinear.\n\ncor(plantHeight$temp, plantHeight$lat)\n\n[1] -0.9249304\n\n\nIn principle, the regression model should be able to still separate them, but the higher the collinearity, the more difficult it becomes for the regression to infer if the effect is caused by one or the other predictor."
  },
  {
    "objectID": "2B-ANOVA.html",
    "href": "2B-ANOVA.html",
    "title": "3  ANOVA",
    "section": "",
    "text": "ANOVA stands for ANalysis Of VAriance. The basic idea is to find out how much of the signal (variance) is explained by different factors. We had already short introduced ANOVA in the section on categorical predictors.\nThe problem with explaining ANOVA is that the term is overfraught with historical meanings and explanations that are no further relevant. It used to be that ANOVA is a stand-alone method that you use for experimental designs with different treatments, that ANOVA assumes normal distribution and partitions sum of squares, that there are repeated-measure ANOVAS and all that, and those varieties of ANOVA partly still exist, but in general, there is a much simpler and general explanation of ANOVA:\nModern explanation: ANOVA is not a statistical model, but a hypothesis test that can be performed on top of any regression model. What this test is doing is to measure how much model fit improves when a predictor is added, and if this improvement is significant.\n\n\nAs an example, here is a ANOVA (function aov()) performed on the fit of a linear model\n\nfit = lm(Ozone ~ Wind + Temp, data = airquality)\nsummary(aov(fit))\n\n             Df Sum Sq Mean Sq F value   Pr(>F)    \nWind          1  45284   45284   94.81  < 2e-16 ***\nTemp          1  25886   25886   54.20 3.15e-11 ***\nResiduals   113  53973     478                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n37 observations deleted due to missingness\n\n\nIn the standard ANOVA for the LM, model fit is measured by the reduction in residual sum of squares. Let’s look at the example above. In the ANOVA table above, we (virtually) start with an intercept only model. Now, what the table tells us is that adding Wind to the model reduces the Sum Sq. by 45284, and adding also Temp reduces the Sum Sq by another 25886, which leaves us with 53973 residual sum sq. From this, we can also conclude that the total variance of the response is 53973 + 25886 + 45284 = 125143. Let’s check this:\n\nsum((fit$model$Ozone - mean(fit$model$Ozone))^2)\n\n[1] 125143.1\n\n\nThus, we can conclude that the R2 explained by each model component is\n\n53973/125143 # Wind\n\n[1] 0.4312906\n\n25886/125143 # Temp\n\n[1] 0.2068514\n\n45284/125143 # Residual\n\n[1] 0.361858\n\n\nMoreover, the ANOVA table performs tests to see if the improvement of model fit is significant against a null model. This is important because, as mentioned before (particular in the chapter on model selection), adding a predictor always improves model fit.\nTo interpret the p-values, consider that H0 = the simpler model is true, thus we test if the improvement of model fit is higher than what we would expect if the predictor has no effect.\n\n\n\nThere are a number of cases where it can make sense to perform an ANOVA for larger parts of the model. Consider, for example, the following regression:\n\nfit = lm(Ozone ~ Wind + I(Wind^2) + Temp + I(Temp^2), data = airquality)\n\nMaybe, we would like to ask how much variance is explained by Wind + Wind^2, and how much by Temp + Temp^2. In this case, we can perform custom ANOVA, using the anova() function that we already introduced in the section on model selection via likelihood ratio tests (LRTs).\n\nm0 = lm(Ozone ~ 1, data = airquality)\nm1 = lm(Ozone ~ Wind + I(Wind^2) , data = airquality)\nm2 = lm(Ozone ~ Wind + I(Wind^2) + Temp + I(Temp^2), data = airquality)\nanova(m0, m1, m2)\n\nAnalysis of Variance Table\n\nModel 1: Ozone ~ 1\nModel 2: Ozone ~ Wind + I(Wind^2)\nModel 3: Ozone ~ Wind + I(Wind^2) + Temp + I(Temp^2)\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1    115 125143                                  \n2    113  64360  2     60783 81.397 < 2.2e-16 ***\n3    111  41445  2     22915 30.686 2.464e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2B-ANOVA.html#fundamental-issues-in-anova",
    "href": "2B-ANOVA.html#fundamental-issues-in-anova",
    "title": "3  ANOVA and other tests on the LM",
    "section": "3.2 Fundamental issues in ANOVA",
    "text": "3.2 Fundamental issues in ANOVA\nThere are three basic problems that we will come back again when generalizing this principle across a range of models:\n\nHow should we measure “improvement of model fit”. Traditionally, improvement is measured by the reduction of the residual sum of squares, but for GLMs, we will have to expand this definition\nHow should we test if the improvement in fit is significant? For simple models, this is not so much a problem\nHow should we partition variance if predictors are collinear, and thus the order in which predictors are included matters\nShould we correct for complexity?\n\nLet’s look at the problem one by one:\n\n3.2.1 Measuring model fit\nThe definition of model fit via sum of squares makes sense as long as we work with linear models.\nFor GLMs, this definition doesn’t make sense any more. A number of so-called pseudo-R2 metrics have been proposed. Most of them are based on the likelihood (as a measure of model fit) and try to recover as far as possible the properties of an R2 for the linear model.\nA common metrics is McFadden pseudo-R2, which is defined as 1-[LogL(M)/LogL(M0))], where M is our model, and M0 is an intercept only model.\n\n\n3.2.2 Testing if the improvement is significant\nThe test used in our example before is an F-test. The F-test is used for models that assume normal distribution. The F-test can be interpreted as a special case of a likelihood ratio test (LRT), which we already used in the chapter on model selection. An LRT can be used on two nested models, with M0 being the simpler model, and makes the following assumptions:\n\nH0 = M0 is true\nTest statistic = likelihood ratio -2 [MLE(M0)/MLE(H1)]\nThen, under relatively broad conditions, the test static will be chi-squared distributed, with df = difference residual df (parameters) of the models M1, M0\n\nThis setup works for LMs and GLMs, but runs into problems when the definition how many df a model has is unclear. This is in particular the case for mixed models. In this case, one can resort to simulated LRTs. Simulated LRTs are a special case of the boostrap, which is a very general and popular method to generate nonparametric confidence intervals and null distributions. We will talk in detail about these methods in the chapter on nonparametric methods, but because this method is crucial for mixed models, I want to shortly explain it already here:\nThe difference between a parametric and a nonparametric test is that the latter does not make assumptions about the test statistic (point 3 above), but somehow generates the latter from the data. The parametric bootstrap does this in the following way:\n\nSimulate data from H0 (= the fitted model M0)\nRe-fit M0 and M1, and calculate likelihood ratios\nRepeat n times to get an idea about the expected increase in likelihood when moving to M1 under the assumption that M0 is correct\n\n\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nm0 = lm(Ozone ~ Wind , data = airquality)\nm1 = lm(Ozone ~ Wind + Temp, data = airquality)\nsimulateLRT(m0, m1)\n\n\n\n\n\n    DHARMa simulated LRT\n\ndata:  m0: m0 m1: m1\nLogL(M1/M0) = 22.723, p-value &lt; 2.2e-16\nalternative hypothesis: M1 describes the data better than M0\n\n# for comparison\nanova(m0, m1)\n\nAnalysis of Variance Table\n\nModel 1: Ozone ~ Wind\nModel 2: Ozone ~ Wind + Temp\n  Res.Df   RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    114 79859                                  \n2    113 53973  1     25886 54.196 3.149e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3.2.3 Partitioning variance if order matters\nAnother key problem in ANOVA, and a source of much confusion, is how to deal with the fact that often, the order in which model components are added matters. Compare the results of our previous ANOVA with this one, where I only flipped the order of Temp and Wind:\n\nfit = lm(Ozone ~ Temp + Wind, data = airquality)\nsummary(aov(fit))\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nTemp          1  61033   61033  127.78  &lt; 2e-16 ***\nWind          1  10137   10137   21.22 1.08e-05 ***\nResiduals   113  53973     478                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n37 observations deleted due to missingness\n\n\nThe result is markedly different, and reason is that the aov function performs a so-called type I ANOVA. The type I ANOVA adds variables in the order in which they are in the model formula, and because Temp and Wind are collinear, the variable that is added first to the model will absorb variation from the other, and thus seems to explain more of the response.\nThere are other types of ANOVA that avoid this problem. The so-called type II ANOVA shows for each variable only the part that is uniquely attributable to the respective variable\n\ncar::Anova(fit, type = \"II\")\n\nAnova Table (Type II tests)\n\nResponse: Ozone\n          Sum Sq  Df F value    Pr(&gt;F)    \nTemp       25886   1  54.196 3.149e-11 ***\nWind       10137   1  21.223 1.080e-05 ***\nResiduals  53973 113                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThere is also type III, which is as type II, but avoids a similar problem for interactions, i.e. it discards the variance that is shared between main effects and interactions. Note that numeric variables should be centered and categorical variables should have orthogonal contrast when running a type III ANOVA (see below).\n\ncar::Anova(fit, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: Ozone\n            Sum Sq  Df F value    Pr(&gt;F)    \n(Intercept)   4335   1  9.0763  0.003196 ** \nTemp         25886   1 54.1960 3.149e-11 ***\nWind         10137   1 21.2230 1.080e-05 ***\nResiduals    53973 113                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere is an overview of the situation for 2 predictors A and B and their interaction. The upper left figure corresponds to the case where we have no collinearity between either of those variables. The figure on the top right (and similarly types I - III) are the three possible types of ANOVA for variables with collinearity. The “overlap” between the circles depicts the shared part, i.e. the variability that can be expressed by either variable (due to collinearity). Note that the shares in Type II, III do not add up to 1, as there is a kind of “dark variation” that we cannot securely add to either variable.\n\n\n\n\n\nWhich type of ANOVA is most appropriate? Unfortunately, the answer is both “it depends on what you want to know” and “there is a lot of discussion about this topic”. I would say the following:\n\nIf you have an orthogonal, balanced design, it doesn’t matter.\nIf not, type I is really only appropriate if you explicitly want to have the order-dependence. There may be some rare cases for this, e.g. if you want to say my regression should first explain everything it can by variable A, and then I only want to see if variable B adds something.\nIf you want to test for significance and partition R2 in a fair way, you should either choose type II or type III ANOVA. Discussions about which is preferable are ongoing, see e.g. (Hector, Von Felten, and Schmid 2010). As you will see when reading this, most of these discussion evolve around your exact definition of the “R2 of an interaction” and what exactly you want to test.\n\n\n\n\n\n\n\nCaution\n\n\n\nNote that, apart from collinearity and balance, also centering of the variables can affect the shared component between main effects and interactions, and thus the variance partitioned in a type III ANOVA. To understand this, consider the following: in a regression x1*x2, where both numeric predictors x1, x2 are centered, the interaction cannot create an average main effect on either x1 or x2, because either predictor has a equal amount of positive and negative values. If x1,x2 are uncentered, however, it is possible to make the regression predict increasing values with increasing x1 on average by setting an intercept value alone.\nFor that reason, working with uncentered predictors will lead to an increased shared variance between x1, x2 and the interaction x1:x2, and thus to lower SumSq values for the main effects in the type III ANOVA. You can see this by running the following code:\n\nfit = lm(Ozone ~ scale(Temp) * scale(Wind), data = airquality)\nfit2 = lm(Ozone ~ Temp * Wind, data = airquality)\n\ncar::Anova(fit, type = \"II\")\ncar::Anova(fit2, type = \"II\")\n\n\ncar::Anova(fit, type = \"III\")\ncar::Anova(fit2, type = \"III\")\n\nFor categorical variables, the same thing can happen - the choice of contrasts (see section on contrasts) will influence the amount of shared variation between main effects and the interactions. When using orthogonal contrasts, this shared component is minimized, because the interaction cannot explain . Let’s do a small example, where we compare a regression with treatment contrasts (which are not orthogonal) to Helmert contrasts (which are orthogonal):\n\nairquality$fMonth = as.factor(airquality$Month)\nfit1 = lm(Ozone ~ Temp * fMonth , data = airquality)\nfit2 = lm(Ozone ~ Temp * fMonth , data = airquality,\n          contrasts = list(fMonth = \"contr.helmert\"))\n\ncar::Anova(fit1, type = \"III\")\ncar::Anova(fit2, type = \"III\")\n\nLet’s compare the results to an ANOVA without an interaction (note that in this case, type II / III) are identical for all sensible contrasts.\n\nfit1 = lm(Ozone ~ Temp + fMonth , data = airquality)\ncar::Anova(fit1, type = \"III\")\n\nIf you run this, you will see that the Helmert contrasts (any other orthogonal contrasts would do as well) match much better with the results of the model without interaction, because they de-correlate the variance explained by the main effects and the interactions. Because of this, it is usually advised to use orthogonal contrasts (note: the default treatment contrasts are not orthogonal) when running a type III ANOVA with 2-way interactions between two categorical variables.\n\n\n\n\n\n\n\n\nExcercise\n\n\n\nTry out the difference between type I, II, III ANOVA for the airquality data set, either for the simple Wind + Temp model, or for more complicated models. If you want to see the effects of Type III Anova, you need to add an interaction (see next section).\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet’s use the model with Wind, Temp, Solar.R, and their interactions. Wind and Temp, and Solar.R and Temp are collinear.\n\n\n\nCall:\nlm(formula = Ozone ~ (scale(Wind) + scale(Temp) + scale(Solar.R))^2, \n    data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.685 -11.727  -2.169   7.360  91.244 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  38.025      2.052  18.535  &lt; 2e-16 ***\nscale(Wind)                 -11.801      2.087  -5.654 1.38e-07 ***\nscale(Temp)                  15.689      2.172   7.222 8.78e-11 ***\nscale(Solar.R)                8.664      2.157   4.016 0.000112 ***\nscale(Wind):scale(Temp)      -5.377      1.966  -2.735 0.007332 ** \nscale(Wind):scale(Solar.R)   -2.294      2.122  -1.081 0.282122    \nscale(Temp):scale(Solar.R)    4.315      2.084   2.070 0.040889 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.17 on 104 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.6863,    Adjusted R-squared:  0.6682 \nF-statistic: 37.93 on 6 and 104 DF,  p-value: &lt; 2.2e-16\n\n\nDifference type I and II ANOVA:\n\n\n                            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nscale(Wind)                  1  45694   45694 124.386  &lt; 2e-16 ***\nscale(Temp)                  1  25119   25119  68.376 4.74e-13 ***\nscale(Solar.R)               1   2986    2986   8.129  0.00526 ** \nscale(Wind):scale(Temp)      1   7227    7227  19.673 2.29e-05 ***\nscale(Wind):scale(Solar.R)   1    996     996   2.710  0.10271    \nscale(Temp):scale(Solar.R)   1   1575    1575   4.287  0.04089 *  \nResiduals                  104  38205     367                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n42 observations deleted due to missingness\n\n\nAnova Table (Type II tests)\n\nResponse: Ozone\n                           Sum Sq  Df F value    Pr(&gt;F)    \nscale(Wind)                 11680   1 31.7934 1.483e-07 ***\nscale(Temp)                 18996   1 51.7098 1.021e-10 ***\nscale(Solar.R)               3618   1  9.8497  0.002211 ** \nscale(Wind):scale(Temp)      2748   1  7.4810  0.007332 ** \nscale(Wind):scale(Solar.R)    429   1  1.1689  0.282122    \nscale(Temp):scale(Solar.R)   1575   1  4.2867  0.040889 *  \nResiduals                   38205 104                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOnly the Temp:Solar.R interaction is not affected!\nDifference type II and III ANOVA:\n\n\nAnova Table (Type III tests)\n\nResponse: Ozone\n                           Sum Sq  Df  F value    Pr(&gt;F)    \n(Intercept)                126210   1 343.5613 &lt; 2.2e-16 ***\nscale(Wind)                 11745   1  31.9706 1.383e-07 ***\nscale(Temp)                 19159   1  52.1545 8.782e-11 ***\nscale(Solar.R)               5926   1  16.1323 0.0001118 ***\nscale(Wind):scale(Temp)      2748   1   7.4810 0.0073323 ** \nscale(Wind):scale(Solar.R)    429   1   1.1689 0.2821218    \nscale(Temp):scale(Solar.R)   1575   1   4.2867 0.0408890 *  \nResiduals                   38205 104                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIdentical when variables are scaled.\n\n\n\n\n\n3.2.4 Correcting partitions for complexity\nA last problem is that model components with more complexity (df) will always tend to explain more variance. We can see this when a model with a separate mean per day:\n\nm = lm(Ozone ~ as.factor(Day) , data = airquality)\nsummary(aov(m))\n\n               Df Sum Sq Mean Sq F value Pr(&gt;F)\nas.factor(Day) 30  41931    1398   1.428  0.104\nResiduals      85  83212     979               \n37 observations deleted due to missingness\n\n\nThe ANOVA tells us that this variable Day explains around 1/3 of the variation in the data, although it is not even significant. Because of this problem, the summary.lm() function reports a raw and an adjusted R2 for each model. The adjusted R2 tries to correct the R2 for the complexity of the model.\n\nsummary(m)\n\n\nCall:\nlm(formula = Ozone ~ as.factor(Day), data = airquality)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-82.667 -16.250  -2.175  16.450  71.333 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         77.75      15.64   4.970 3.43e-06 ***\nas.factor(Day)2    -34.75      22.12  -1.571  0.11997    \nas.factor(Day)3    -44.50      22.12  -2.011  0.04746 *  \nas.factor(Day)4    -15.42      23.90  -0.645  0.52058    \nas.factor(Day)5    -29.08      23.90  -1.217  0.22696    \nas.factor(Day)6    -36.25      22.12  -1.638  0.10502    \nas.factor(Day)7    -23.55      20.99  -1.122  0.26501    \nas.factor(Day)8    -20.75      22.12  -0.938  0.35096    \nas.factor(Day)9    -16.35      20.99  -0.779  0.43815    \nas.factor(Day)10   -28.42      23.90  -1.189  0.23770    \nas.factor(Day)11   -52.25      27.10  -1.928  0.05716 .  \nas.factor(Day)12   -55.00      22.12  -2.486  0.01488 *  \nas.factor(Day)13   -54.35      20.99  -2.589  0.01131 *  \nas.factor(Day)14   -48.42      23.90  -2.026  0.04589 *  \nas.factor(Day)15   -65.08      23.90  -2.723  0.00784 ** \nas.factor(Day)16   -47.55      20.99  -2.265  0.02603 *  \nas.factor(Day)17   -41.15      20.99  -1.961  0.05321 .  \nas.factor(Day)18   -53.15      20.99  -2.532  0.01317 *  \nas.factor(Day)19   -42.55      20.99  -2.027  0.04577 *  \nas.factor(Day)20   -48.35      20.99  -2.304  0.02369 *  \nas.factor(Day)21   -65.00      22.12  -2.938  0.00425 ** \nas.factor(Day)22   -63.42      23.90  -2.654  0.00950 ** \nas.factor(Day)23   -57.75      27.10  -2.131  0.03596 *  \nas.factor(Day)24   -36.75      22.12  -1.661  0.10038    \nas.factor(Day)25    18.92      23.90   0.792  0.43080    \nas.factor(Day)26   -36.75      23.90  -1.538  0.12780    \nas.factor(Day)27   -25.75      34.98  -0.736  0.46370    \nas.factor(Day)28   -29.00      22.12  -1.311  0.19346    \nas.factor(Day)29   -20.00      22.12  -0.904  0.36856    \nas.factor(Day)30    -7.00      22.12  -0.316  0.75248    \nas.factor(Day)31   -17.42      23.90  -0.729  0.46811    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 31.29 on 85 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.3351,    Adjusted R-squared:  0.1004 \nF-statistic: 1.428 on 30 and 85 DF,  p-value: 0.1039\n\n\nHere, we see that the adjusted R2 is considerably lower than the raw R2, even though not zero.\nIn general, reliably adjusting R2 components for complexity in an ANOVA is very complicated. I would recommend that\n\nIf you report raw components (which is the default in most papers), consider that this is a description of your model, not an inference about the true variance created by the respective factor, and include in your interpretation that variables or model components with more df will always tend to explain more variance\nAlternatively, if the true variance is really crucial for your study, you can adjust R2 by a null expectation, similar to the parametric bootstrap for the LRT. However, because you subtract what is expected under H0, this will be a conservative estimate."
  },
  {
    "objectID": "2B-ANOVA.html#case-studies",
    "href": "2B-ANOVA.html#case-studies",
    "title": "3  ANOVA and other tests on the LM",
    "section": "3.4 Case studies",
    "text": "3.4 Case studies\n\n3.4.1 Plant Height\nLet’s look at our plant height case study from the previous chapter. Which type of ANOVA should you run to analyze the model? Compare results to “less appropriate” alternatives.\n\nlibrary(EcoData)\nfit = lm(loght ~ temp * lat, data = plantHeight)\nsummary(fit)\n\n\nCall:\nlm(formula = loght ~ temp * lat, data = plantHeight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.97905 -0.45112  0.01062  0.42852  1.74054 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  2.917e-01  5.617e-01   0.519    0.604\ntemp         2.596e-02  2.040e-02   1.273    0.205\nlat         -8.948e-03  9.711e-03  -0.921    0.358\ntemp:lat     7.738e-05  3.061e-04   0.253    0.801\n\nResidual standard error: 0.6869 on 174 degrees of freedom\nMultiple R-squared:  0.2504,    Adjusted R-squared:  0.2375 \nF-statistic: 19.37 on 3 and 174 DF,  p-value: 6.95e-11\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have correlations between variables and an interaction, so should run a type II or III Anova, for III you should center predictors:\n\nplantHeight$stemp = scale(plantHeight$temp, scale = FALSE) # only center variable\nplantHeight$slat = scale(plantHeight$lat, scale = FALSE)\nfit = lm(loght ~ stemp * slat, data = plantHeight)\nsummary(fit)\n\n\nCall:\nlm(formula = loght ~ stemp * slat, data = plantHeight)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.97905 -0.45112  0.01062  0.42852  1.74054 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.694e-01  6.771e-02   6.932 7.78e-11 ***\nstemp        2.838e-02  1.543e-02   1.839   0.0676 .  \nslat        -7.700e-03  8.020e-03  -0.960   0.3383    \nstemp:slat   7.738e-05  3.061e-04   0.253   0.8007    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6869 on 174 degrees of freedom\nMultiple R-squared:  0.2504,    Adjusted R-squared:  0.2375 \nF-statistic: 19.37 on 3 and 174 DF,  p-value: 6.95e-11\n\nprint(car::Anova(fit, type = \"III\"))\n\nAnova Table (Type III tests)\n\nResponse: loght\n            Sum Sq  Df F value    Pr(&gt;F)    \n(Intercept) 22.670   1 48.0522 7.782e-11 ***\nstemp        1.596   1  3.3834   0.06756 .  \nslat         0.435   1  0.9218   0.33834    \nstemp:slat   0.030   1  0.0639   0.80074    \nResiduals   82.089 174                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nArguably not appropriate (because order dependent, and here we have strong collinearity) is a type I ANOVA:\n\nsummary(aov(fit))\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nstemp         1  26.97  26.967  57.160 2.21e-12 ***\nslat          1   0.42   0.421   0.892    0.346    \nstemp:slat    1   0.03   0.030   0.064    0.801    \nResiduals   174  82.09   0.472                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that we have centered our variables, let’s see what happens when we use the non centered variables:\n\nfit2 = lm(loght ~ temp * lat, data = plantHeight)\nprint(car::Anova(fit2, type = \"III\"))\n\nAnova Table (Type III tests)\n\nResponse: loght\n            Sum Sq  Df F value Pr(&gt;F)\n(Intercept)  0.127   1  0.2696 0.6043\ntemp         0.764   1  1.6198 0.2048\nlat          0.401   1  0.8490 0.3581\ntemp:lat     0.030   1  0.0639 0.8007\nResiduals   82.089 174               \n\n\nThe shared proportion has proportionally gained a lot sum of squares!\n\n\n\n\n\n\n\nHector, Andy, Stefanie Von Felten, and Bernhard Schmid. 2010. “Analysis of Variance with Unbalanced Data: An Update for Ecology & Evolution.” Journal of Animal Ecology 79 (2): 308–16."
  },
  {
    "objectID": "2C-RandomEffects.html",
    "href": "2C-RandomEffects.html",
    "title": "4  Linear mixed models",
    "section": "",
    "text": "Random effects are a very common addition to regression models that are used to account for grouping (categorical) variables such as subject, year, location. To explain the motivation for these models, as well as the basic syntax, we will use an example data set containing exam scores of 4,059 students from 65 schools in Inner London. This data set is located in the R package mlmRev.\n\nResponse: “normexam” (Normalized exam score).\nPredictor 1: “standLRT” (Standardised LR test score; Reading test taken when they were 11 years old).\nPredictor 2: “sex” of the student (F / M).\nGrouping factor: school\n\nIf we analyze this with a simple LM, we see that reading ability and sex have the expected effects on the exam score.\n\nlibrary(mlmRev)\nlibrary(effects)\n\nmod0 = lm(normexam ~ standLRT + sex , data = Exam)\nplot(allEffects(mod0))\n\n\n\n\nHowever, it is reasonable to assume that not all schools are equally good, which has two possible consequences:\n\nIf school identity correlates with sex or standLRT, school could be a confounder (more on this later)\nResiduals will be correlated in school, thus they are not iid (pseudo-replication)\n\nWe could solve this problem by fitting a different mean per school\n\nmod0b = lm(normexam ~ standLRT + sex + school , data = Exam)\n\nhowever, with 65 schools this would cost 64 degrees of freedom, which is a high cost just for correcting the possibility of confounding and a minor residual problem.\nThe solution: Mixed / random effect models. In a mixed model, we assume (differently to a fixed effect model) that the variance between schools originates from a normal distribution. There are different interpretations of a random effect (more on this later), but one interpretation is to view the random effect as an additional error, so that such a mixed model has two levels of error:\n\nFirst, the random effect, which is a normal “error” acting on an entire group of data points (in this case school).\nAnd second, the residual error, which is a normal error per observation and acts on top of the random effect\n\nBecause of this hierarchical structure, these models are also called “multi-level models” or “hierarchical models”.\n\n\n\n\n\n\nNote\n\n\n\nBecause grouping naturally occurs in any type of experimental data (batches, blocks, observer, subject etc.), random and mixed effect models are the de-facto default for most experimental data! Naming conventions:\n\nNo random effect = fixed effect model\nOnly random effects = random effect model\nRandom effects + fixed effects = mixed model\n\nMost models that are used in practice are mixed models"
  },
  {
    "objectID": "2C-RandomEffects.html#the-random-intercept-model",
    "href": "2C-RandomEffects.html#the-random-intercept-model",
    "title": "4  Linear mixed models",
    "section": "4.2 The random intercept model",
    "text": "4.2 The random intercept model\nThe simplest random effect structure is the random intercept model.\n\n\n\n\n\n\nNote\n\n\n\nEach random effect model has a fixed counterpart. For the random intercept model, this counterpart is the model we already saw, with school as a fixed effect\n\nmod0b = lm(normexam ~ standLRT + sex + school , data = Exam)\n\n\n\nThe random intercept model is practically identical to the model above, except that it assumes that the school effects come from a common normal distribution. The model can be estimated with lme4::lmer\n\nlibrary(lme4)\nmod1 = lmer(normexam ~ standLRT + sex +  (1 | school/student), data = Exam)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: normexam ~ standLRT + sex + (1 | school/student)\n   Data: Exam\n\nREML criterion at convergence: 9346.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7120 -0.6314  0.0166  0.6855  3.2735 \n\nRandom effects:\n Groups         Name        Variance Std.Dev.\n student:school (Intercept) 0.00000  0.0000  \n school         (Intercept) 0.08986  0.2998  \n Residual                   0.56252  0.7500  \nNumber of obs: 4059, groups:  student:school, 4055; school, 65\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.07639    0.04202   1.818\nstandLRT     0.55947    0.01245  44.930\nsexM        -0.17136    0.03279  -5.226\n\nCorrelation of Fixed Effects:\n         (Intr) stnLRT\nstandLRT -0.013       \nsexM     -0.337  0.061\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nIf we look at the outputs, you will probably note that the summary() function doesn’t return p-values. More on this in the section on problems and solutions for mixed models. Also, we see that the effects of the individual school are not explicitly mentioned. The summary only returns the mean effects over all schools, and the variance between schools. However, you can obtain the individual school effects via\n\nranef(mod1)\n\nMoreover, unlike the outputs we have seen before, the model also returns a correlation between fixed effect estimates. This, however, is simply a choice of the programmers, it has nothing to do with the random effect models - you could calculate the same output for the lm() via the function vcov().\nHere is a visualization of the fitted effects for standLRT\n\n\n\n\n\nWe can see that there is the same slope, but a different intercept per school.\n\n\n\n\n\n\nNote\n\n\n\nThe RE effectively “centers” the categorical predictor - unlike for the fixed effect model, where the intercept would be interpreted as the value for the first school, the intercept in the random effect model is the mean across all schools, and the REs measure the deviation of the individual school from the mean. It should be stressed, however, that this is a welcome consequence, but not the main motivation, for the RE structure.\n\n\n\n\n\n\n\n\nGenerate equation with the equatiomatic package\n\n\n\n\n\nWe can use the equatiomatic package to print LaTeX code for the underlying equation of our random intercept model:\n\nlibrary(equatiomatic)\nextract_eq(mod1)\n\n\\[\n\\begin{aligned}\n  \\operatorname{normexam}_{i}  &\\sim N \\left(\\alpha_{j[i]} + \\beta_{1}(\\operatorname{standLRT}) + \\beta_{2}(\\operatorname{sex}_{\\operatorname{M}}), \\sigma^2 \\right) \\\\\n    \\alpha_{j}  &\\sim N \\left(\\mu_{\\alpha_{j}}, \\sigma^2_{\\alpha_{j}} \\right)\n    \\text{, for school j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "2C-RandomEffects.html#the-random-slope-model",
    "href": "2C-RandomEffects.html#the-random-slope-model",
    "title": "4  Linear mixed models",
    "section": "4.3 The random slope model",
    "text": "4.3 The random slope model\nA second effect that we could imagine is that the effect of standLRT is different for each school. As a fixed effect model, we would express this, for example, as lm(normexam ~ standLRT*school + sex)\nThe random effect equivalent is called the random slope model. A random slope model assumes that each school also gets their own slope for a given parameter (per default we will always estimate slope and intercept, but you could overwrite this, not recommended!). Let’s do this for standLRT (you could of course random slopes on both predictors as well).\n\nmod2 = lmer(normexam ~ standLRT + sex +  (standLRT | school), data = Exam)\nsummary(mod2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: normexam ~ standLRT + sex + (standLRT | school)\n   Data: Exam\n\nREML criterion at convergence: 9303.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.8339 -0.6373  0.0245  0.6819  3.4500 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n school   (Intercept) 0.08795  0.2966       \n          standLRT    0.01514  0.1230   0.53\n Residual             0.55019  0.7417       \nNumber of obs: 4059, groups:  school, 65\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.06389    0.04167   1.533\nstandLRT     0.55275    0.02016  27.420\nsexM        -0.17576    0.03228  -5.445\n\nCorrelation of Fixed Effects:\n         (Intr) stnLRT\nstandLRT  0.356       \nsexM     -0.334  0.036\n\n\nAgain, the difference to the fixed effect model is that in the random effect model, we assume that the variance between slopes come from a normal distribution. The results is similar to the random intercept model, except that we have an additional variance term for the slopes, and a term for the interaction between slopes and intercepts.\n\n\n\n\n\n\nGenerate equation with the equatiomatic package\n\n\n\n\n\nWe can use the equatiomatic package to print LaTeX code for the underlying equation of our random slope model:\n\nlibrary(equatiomatic)\nextract_eq(mod2)\n\n\\[\n\\begin{aligned}\n  \\operatorname{normexam}_{i}  &\\sim N \\left(\\alpha_{j[i]} + \\beta_{1j[i]}(\\operatorname{standLRT}) + \\beta_{2}(\\operatorname{sex}_{\\operatorname{M}}), \\sigma^2 \\right) \\\\    \n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\alpha_{j} \\\\\n      &\\beta_{1j}\n    \\end{aligned}\n  \\end{array}\n\\right)\n  &\\sim N \\left(\n\\left(\n  \\begin{array}{c}\n    \\begin{aligned}\n      &\\mu_{\\alpha_{j}} \\\\\n      &\\mu_{\\beta_{1j}}\n    \\end{aligned}\n  \\end{array}\n\\right)\n,\n\\left(\n  \\begin{array}{cc}\n     \\sigma^2_{\\alpha_{j}} & \\rho_{\\alpha_{j}\\beta_{1j}} \\\\\n     \\rho_{\\beta_{1j}\\alpha_{j}} & \\sigma^2_{\\beta_{1j}}\n  \\end{array}\n\\right)\n\\right)\n    \\text{, for school j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\\]\n\n\n\nHere a visualization of the results\n\nwith(Exam, {\n  randcoefI = ranef(mod2)$school[,1]\n  randcoefS = ranef(mod2)$school[,2]\n  fixedcoef = fixef(mod2)\n  plot(standLRT, normexam)\n    for(i in 1:65){\n      abline(a = fixedcoef[1] + randcoefI[i] , b = fixedcoef[2] + randcoefS[i], col = i)\n    }\n})\n\n\n\n\n\n\n\n\n\n\nExcercise: a mixed model for plantHeight\n\n\n\nTake plantHeight model that we worked with already:\n\nlibrary(EcoData)\n\noldModel &lt;- lm(loght ~ temp, data = plantHeight)\nsummary(oldModel)\n\nWe have one observation per species. Consider that the relationship height ~ temp may be different for each family. Some families may have larger plants in general (random intercept), but it may also be that the temperature relationship changes per family (random slope). Run these two models for our problem.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# Random intercept model\n\nlibrary(lme4)\nlibrary(lmerTest)\nrandomInterceptModel &lt;- lmer(loght ~ temp + (1|Family), data = plantHeight)\nsummary(randomInterceptModel)\n\n# Random slope model\n\nrandomSlopeModel &lt;- lmer(loght ~ temp + (temp | Family), data = plantHeight)\nsummary(randomSlopeModel)\n\n# scaling helps the optimizer\nplantHeight$sTemp = scale(plantHeight$temp)\n\nrandomSlopeModel &lt;- lmer(loght ~ sTemp + (sTemp | Family), data = plantHeight)\nsummary(randomSlopeModel)"
  },
  {
    "objectID": "2C-RandomEffects.html#specifying-and-estimating-mixed-models",
    "href": "2C-RandomEffects.html#specifying-and-estimating-mixed-models",
    "title": "4  Linear mixed models",
    "section": "4.4 Specifying and estimating mixed models:",
    "text": "4.4 Specifying and estimating mixed models:\n\n4.4.1 lme4 / glmmTMB\nThere are a large number of packages to fit mixed models in R. The main packages that you use, because they are most user-friendly and stable, are lme4 and glmmTMB. They share more or less the same syntax for random effects.\n\nRandom intercept: (1 | group).\nONLY random slope for a given fixed effect: (0 + fixedEffect | group).\nRandom slope + intercept + correlation (default): (fixedEffect | group).\nRandom slope + intercept without correlation: (fixedEffect || group), identical to\n(1 | group) + (0 + fixedEffect | group).\nNested random effects: (1 | group / subgroup). If groups are labeled A, B, C, … and subgroups 1, 2, 3, …, this will create labels A1, A2 ,B1, B2, so that you effectively group in subgroups. Useful for the many experimental people that do not label subgroups uniquely, but otherwise no statistical difference to a normal (crossed) random effect.\nCrossed random effects: You can add random effects independently, as in\n(1 | group1) + (1 | group2).\n\n\n\n\n\n\n\nMore on crossed vs. nested REs\n\n\n\n\n\nThere is often confusion about the behavior and meaning of crossed vs. nested REs. From the statistical / mathematical side, there is no difference between them, there are just REs. Crossed vs. nested is a shorthand to tell you how the data was coded.\nLet’s look at the Exam data, for example. If you run\n\ntable(Exam$student, Exam$school)\n\nyou will see that the student ID = 1 appears in many schools. Clearly, this cannot be the same student, the reason is that each school uses their own ID. If you would fit a crossed RE, as in\n\nm1 = lmer(normexam ~ standLRT + sex +  (1 | school) + (1 |student), data = Exam)\nsummary(m1)\n\nyou would fit a common effect for each student with ID = 1. This, however, doesn’t make sense, there is no connection between these students. Thus, you should fit\n\nm2 = lmer(normexam ~ standLRT + sex +  (1 | school/student), data = Exam)\nsummary(m2)\n\nCompare the number of RE groups in the summary: in m1, you have 650 students. In m2, you have 4055 student:school levels for the RE. Thus, the nested syntax makes sure each individual student gets his/her own estimated value.\nYou could fit the same mod by changing the coding of student. To do this, we create a new variable, combining school and student into a unique ID\n\nExam$uniqueStudent = as.factor(paste(Exam$school, \"-\", Exam$student))\n\nm3 = lmer(normexam ~ standLRT + sex +  (1 | school) + (1|uniqueStudent), data = Exam)\nsummary(m3)\n\nIf you check the summary of m3, now we also have 4055 levels for uniqueStudent, and the results are identical to the nested model version.\n\n\n\n\n\n4.4.2 nlme\nnlme is a classic package for fitting (nonlinear) mixed effect models. I would argue that it is largely superseded by lme4 and glmmTMB, but it still offers a few special tricks that are not available in either lme4 or glmmTMB. A linear mixed model with random intercept in nlme looks like this\n\nnlme::lme(Resp ~ Predictor, random=~1|Group)\n\n\n\n4.4.3 mgcv\nIf you want to use GAMs, there is basically only mgcv (or brms, if you’re willing to go Bayesian).\n\nmgcv::gam(Resp ~ Predictor + s(Group, bs = 're'))\n\n\n\n4.4.4 brms\nbrms is a very interesting package that allows to estimate mixed models via Bayesian Inference with STAN.\n\nbrms::brm(Resp ~ Predictor + (1|Group),\n         data = Owls , family = poisson)\nsummary(mod1)"
  },
  {
    "objectID": "2C-RandomEffects.html#advantages-and-interpretation-of-random-effects",
    "href": "2C-RandomEffects.html#advantages-and-interpretation-of-random-effects",
    "title": "4  Linear mixed models",
    "section": "4.5 Advantages and interpretation of random effects",
    "text": "4.5 Advantages and interpretation of random effects\nWhat do the random effects mean, and why should we use them?\n\n4.5.1 Interpretation\nThe classical interpretation of the random intercept is that they model a group-structured residual error, i.e. they are like residuals, but for an entire group. However, this view breaks down a bit for a random slope model.\nAnother view this is that REs terms absorb group-structured variation in model parameters. This view works for random intercept and slope models. So, our main model fits the grand mean, and the RE models variation in the parameters, which is assumed to be normally distributed.\nA third view of RE models is that they are regularized fixed effect models. What does that mean? Note again that, conceptual, every random effect model corresponds to a fixed effect model. For a random intercept model, this would be\n\nlmer(y ~ x + (1|group)) =&gt; lm(y ~ x + group)\n\nand for the random slope, it would be\n\nlmer(y ~ x + (x|group)) =&gt; lm(y ~ x * group)\n\nSo, alternatively to a mixed model, we could always fit a fixed effect model, and that would also take care of the variation in groups. Of course, the output of the two models differs, but that’s just what people programmed. The real difference, however, is that by making the assumption that the random effects come from a normal distribution, we are imposing a constraint on the estimates, which creates a shrinkage on the fitted REs (see below).\n\n\n4.5.2 RE shrinkage\nTo understand shrinkage, note that whatever the fitted sd of the normal distribution underlying the RE, as long as it is finite, it will always be more likely that the RE estimate is at zero (i.e. the grand mean) than at 1 or 2 sd. This means that RE estimates are biased towards the grand mean.\nWe can visualize the effect by comparing estimates of a fixed to a random effect model\n\nmf = lm(normexam ~ 0 + school, data = Exam)\nmr = lmer(normexam ~ (1 | school), data = Exam)\n\nef = coef(mf)\ner = ranef(mr)$school$`(Intercept)`\nplot(ef, er, xlab = \"fixed\", ylab = \"random\")\nabline(0,1)\n\n\n\n\nThe plot shows that random effect estimates are biased towards the grand mean, i.e. positive values are lower than in the LM, and negative values are higher than in the LM. This is called shrinkage.\n\n\n\n\n\n\nNote\n\n\n\nThe shrinkage imposed by the RE is equivalent to an L2 (ridge) regression (see section on model selection), or a normally distributed Bayesian prior. You can see this by considering that the log likelihood of a normal distribution is equivalent to a quadratic penalty for deviations from the mean. The difference is that the strength of the penalty, controlled by the RE sd, is estimated. For that reason, the RE shrinkage is also referred to as an example of “adaptive shrinkage”.\n\n\nThis RE shrinkage is useful in many situations, beyond describing a group-structured error in the data. A common use is that by using a random slope model, parameter estimates for groups with few data points are drawn towards the grand mean and thus informed by the estimates of the other groups. For example, if we have data for common and rare species, and we are interested in the density dependence of the species, we could fit. Consider the plantHeight case study we did in the section on linear regression. We saw that, even after re-leveling the predictor, not all interactions could be estimated, because some factor levels had only one observation.\n\nlibrary(EcoData)\nplantHeight$growthform2 = relevel(as.factor(plantHeight$growthform), \"Herb\")\nplantHeight$sTemp = scale(plantHeight$temp)\nfit &lt;- lm(loght ~ sTemp * growthform2 , data = plantHeight)\nsummary(fit)\n\nIf you run the same thing as a random slope model, you will see that we can estimate a random intercept and slope for each species, even those that have only one observation\n\nfit &lt;- lmer(loght ~ sTemp + (sTemp | growthform2) , data = plantHeight)\nranef(fit)\n\n$growthform2\n           (Intercept)       sTemp\nHerb       -0.70060787  0.07032554\nFern       -0.07771756  0.01484118\nHerb/Shrub -0.18519919  0.03351554\nShrub      -0.17681675  0.11176808\nShrub/Tree  0.29044226 -0.04361958\nTree        0.84989911 -0.18683076\n\nwith conditional variances for \"growthform2\" \n\n\nThe reason is that rare growth forms will be constrained by the RE, while common growth forms with a lot of data can overrule the normal distribution imposed by the random slope and get their own estimate. In this picture, the random effect imposes an adaptive shrinkage, similar to a LASSO or ridge shrinkage estimator, with the shrinkage strength controlled by the standard deviation of the random effect. An alternative view is that the RE variance acts as a prior distribution on the slope. Of course, estimates are biased towards zero, so if you want to interpret these estimatest that is a cost that you have to take into account.\n\n\n4.5.3 When to use what?\nSo, to finalize this discussion: there are essentially two main differences between a random vs. fixed effect model\n\nIn the RE model, we are fitting a grand mean and differences from the grand mean, while in the fixed effect model, we have to set up contrasts (various options), with treatment contrasts as defaults\nThe LM (OLS) is mathematically proven to be the best linear unbiased estimator (BLUE) of the regression problem. In a mixed model, REs (but not fixed effects) are biased towards the grand mean, but the advantage is that the random effect looses less degrees of freedom, i.e. the power on the fixed effects will be higher.\n\nWhile point 1 is more a choice about how to report results (in principle, we could also calculate the grand mean for fixed effect group estimates), the second point (higher power) is a clear advantage in many cases, in particular if you don’t care about the bias on the REs. Therefore, the rule of thumb for REs is:\n\nIf you have a categorical variable, and you’re not interested in it’s estimates, model it as a RE\nIf you want to get unbiased estimates of the categorical variable (or its interactions), use a fixed effect\n\nThis is just a rule of thumb, as said, in data-poor situations, it is common to use random slope models and interpret the estimates as well. In this case, we are using the RE to optimize the bias-variance trade-off (see chapter on model selection)."
  },
  {
    "objectID": "2C-RandomEffects.html#model-checks",
    "href": "2C-RandomEffects.html#model-checks",
    "title": "4  Linear mixed models",
    "section": "4.6 Model checks",
    "text": "4.6 Model checks\nAs residual checks for an LMM, you should first of all check for everything that you would check in an LM, i.e. you would expect that the residual distribution is iid normal. On top of that, you should check the REs for:\n\nConvergence (mixed models often have convergence problems)\nNormality\nCorrelation with predictors -&gt; RE can mask misfit\nResidual patterns -&gt; evidence for random slope\n\nI will show all of those using the example\n\nmod1 = lmer(normexam ~ standLRT + sex +  (1 | school), data = Exam)\nsummary(mod1)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: normexam ~ standLRT + sex + (1 | school)\n   Data: Exam\n\nREML criterion at convergence: 9346.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7120 -0.6314  0.0166  0.6855  3.2735 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n school   (Intercept) 0.08986  0.2998  \n Residual             0.56252  0.7500  \nNumber of obs: 4059, groups:  school, 65\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  0.07639    0.04202   1.818\nstandLRT     0.55947    0.01245  44.930\nsexM        -0.17136    0.03279  -5.226\n\nCorrelation of Fixed Effects:\n         (Intr) stnLRT\nstandLRT -0.013       \nsexM     -0.337  0.061\n\n\n\n4.6.1 Convergence\nIf there is a convergence problem, you should usually get a warning. Other signs of convergence problems may be RE estimates that are zero, CIs that are extremely large, or strong correlations in the correlation matrix of the predictors that is reported by summary(). Here, we have no problems, but we’ll see examples of this as we go on.\nThere are different types of convergence problems that you can encounter:\n\nWarnings about scaling and centering -&gt; just do it, it doesn’t cost you!\nThe warning “boundary (singular) fit: see help(‘isSingular’)” (meaning that some dimensions of the variance-covariance matrix have been estimated to zero. -&gt; often, this just means that a RE estimate is zero. In many cases, this is not a problem and can actually be ignored, but it is hard to give general advice to when this is the case. In random slope models, it often helps to move from | to || (supressing the correlation). Removing the RE will solve the problem, but could lead to type I error inflation. A more conservative solution would be to change the RE to a fixed effect\nOptimizer / convergence warnings: use glmerControl() to change the optimizer to bobyqa and possibly increase the number of iterations.\n\n\n\n4.6.2 Observation-level residual plots\nUnfortunately, lme4 plots only provide res ~ fitted as default\n\nplot(mod1)\n\n\n\n\nFor a qqplot, you have to calculate the residuals by hand\n\nqqnorm(residuals(mod1))\nqqline(residuals(mod1))\n\n\n\n\nOf course, any time you do a qqnorm plot, you could also run a shapiro.test to get a p-value for the deviation from normality. You can also use the DHARMa plots\n\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nres &lt;- simulateResiduals(mod1, plot = T)\n\n\n\n\nBoth plots look fine. Of course, we should also check residuals against predictors. In DHARMa, we can do this via\n\npar(mfrow = c(1,2))\nplotResiduals(res, Exam$standLRT)\nplotResiduals(res, Exam$sex)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen you predict with a random effect model, you can do conditional or marginal predictions. The difference is that conditional predictions include the REs, while marginal predictions only include the fixed effects.\n\nresiduals(mod1, re.form = NULL) # conditional, default\npredict(mod1, re.form = ~0) # marginal\n\nThis also makes a difference for the residual calculations: in principle, you could calculate residuals with respect to both predictions. In practice, the default in lme4 is to calculate residual predictions\n\nresiduals(mod1)\nExam$normexam - predict(mod1, re.form = NULL) \nExam$normexam - predict(mod1, re.form = ~0) \n\nwhile the default in DHARMa is to calculate marginal predictions. Both has advantages and disadvantages:\n\nConditional predictions are more sensitive to violations of the iid normal assumptions of the outer (observation-level) normal error\nHowever, REs can sometimes absorb misfit, such that plotting conditional residuals ~ pred looks fine, but uncoditional ~ pred does not. For that reason, if you use unconditional predictions, you should also check REs ~ pred (see below)\n\n\n\n\n\n4.6.3 Normality of REs\nI prefer to look at this visually. You can also run a shapiro.test() to check if you should consider the pattern at all.\n\nx = ranef(mod1)\nqqnorm(x$school$`(Intercept)`)\n\n\n\n\n\n\n4.6.4 Correlation of REs with predictors\nAs mentioned above, REs can sometimes absorb misfit of the fixed effect structure. It can therefore be useful to check if there is a correlation with the predictors, especially if you only check conditional residuals. Example:\n\ny = aggregate(cbind(standLRT, sex) ~ school, FUN = mean, data = Exam)\nplot(x$school$`(Intercept)` ~ y$standLRT)\n\n\n\n\n\n\n4.6.5 Residual pattern per group\nlme4 has an excellent plot syntax to plot residuals per grouping factor. Check out the help of plot.merMod. The following plot shows residuals ~ fitted for each school\n\nplot(mod1, resid(., scaled=TRUE) ~ standLRT | school, abline = 0)\n\n\n\n\nLooks still good to me, but if we think we see a pattern here, we should introduce a random slope."
  },
  {
    "objectID": "2C-RandomEffects.html#problems-with-mixed-models",
    "href": "2C-RandomEffects.html#problems-with-mixed-models",
    "title": "4  Linear mixed models",
    "section": "4.7 Problems With Mixed Models",
    "text": "4.7 Problems With Mixed Models\nSpecifying mixed models is quite simple, however, there is a large list of (partly quite complicate) issues in their practical application. Here, a (incomplete) list:\n\n4.7.1 Degrees of freedom for a random effect\nPossibly the most central problem is: How many parameters does a random effect model have?\n\n\n\n\n\n\nNote\n\n\n\nWhy is it so important to know how many parameters a model has? The answer is that what seems like a very minor point is needed in all the mathematics for calculating p-values, AIC, LRTs and all that, so without knowing the complexity of the model, the mathematics that is used in lm() breaks down.\n\n\nWe can make some guesses about the complexity of an LMM by looking at its fixed effect version counterparts:\n\nmod1 = lm(normexam ~ standLRT + sex , data = Exam)\nmod1$rank # 3 parameters.\n\nmod2 = lmer(normexam ~ standLRT + sex +  (1 | school), data = Exam)\n# No idea how many parameters.\n\nmod3 = lm(normexam ~ standLRT + sex + school, data = Exam)\nmod3$rank # 67 parameters.\n\nSo, we see that the fixed effect model without school has 3 parameters, and with school 67. It is reasonable to assume that the complexity of the mixed model is somewhere in-between, because it fits the same effects as mod3, but they are constrained by the normal distribution.\nThe width of this normal distribution is controlled by the estimated variance of the random effect. For a high variance, the normal is very wide, and the mixed model is nearly as complex as mod3; but for a low variance, the normal is very narrow, and the mixed model it is only as complex as mod1.\n\n\n\n\n\n\nNote\n\n\n\nTechnically, the mixed model mod2 actually has one parameter more than the fixed effect model mod3 - it also estimates the standard deviation of the random effects. Could it thus ever be more flexible than mod3? The answer is no - the RE standard deviation is what is technically called a hyperparameter. It is not an effect that is estimated, but rather a parameter that controls the freedom of the estimated effects. The number of parameters that are used to estimate the response are identical in mod2 and mod3.\n\n\nBecause of these issues, lmer by default does not return p-values. The help advises you to use bootstrapping to generate valid confidence intervals and p-values on parameters. This is possible indeed, but very cumbersome.\nHowever, you can calculate p-values based on approximate degrees of freedom via the lmerTest package, which also corrects ANOVA for random effects, but not AIC.\n\nlibrary(lmerTest)\n\nm2 = lmer(normexam ~ standLRT + sex +  (1 | school), data = Exam, REML = F)\nsummary(m2)\n\nLinear mixed model fit by maximum likelihood . t-tests use Satterthwaite's\n  method [lmerModLmerTest]\nFormula: normexam ~ standLRT + sex + (1 | school)\n   Data: Exam\n\n     AIC      BIC   logLik deviance df.resid \n  9340.0   9371.6  -4665.0   9330.0     4054 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.7117 -0.6326  0.0168  0.6865  3.2744 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n school   (Intercept) 0.08807  0.2968  \n Residual             0.56226  0.7498  \nNumber of obs: 4059, groups:  school, 65\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)    0.07646    0.04168   76.66670   1.834   0.0705 .  \nstandLRT       0.55954    0.01245 4052.83930  44.950  &lt; 2e-16 ***\nsexM          -0.17138    0.03276 3032.37966  -5.231  1.8e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n         (Intr) stnLRT\nstandLRT -0.013       \nsexM     -0.339  0.061\n\n\n\n\n\n\n\n\nExcercise: mixed vs. fixed effect models\n\n\n\nFor the plantHeigt mixed models in the previous exercise, compare the p-values of the slope for temp to the fixed-effect equivalent (adding family as main effect / interaction). What do you find?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfixedInterceptModel &lt;- lm(loght ~ sTemp + Family, data = plantHeight)\nrandomInterceptModel &lt;- lmer(loght ~ sTemp + (1 | Family), data = plantHeight)\nsummary(fixedInterceptModel)\nsummary(randomInterceptModel)\n\nfixedSlopeInterceptModel &lt;- lm(loght ~ sTemp * Family, data = plantHeight)\nrandomSlopeModel &lt;- lmer(loght ~ sTemp + (sTemp | Family), data = plantHeight)\n\nsummary(fixedSlopeInterceptModel)\nsummary(randomSlopeModel)\n\nSo, for the model with a different intercept per Family, the p-value decreases, and for intercept + interaction, it even becomes n.s. (although it should be noted that we get a different p-values for each species here).\nThe reason for the generally lower p-values (= higher power) of the mixed model is the lower flexibility of the mixed model:\nFor the fixed effect models, we loose 69 respectively 2x69 df if we add family as a predictor. For the mixed model, it depends on the variance estimate: wide sd of the RE, loose nearly 69df, narrow sd = loose nothing except the sd estimate -&gt; flexibility of an random effect model is adaptive (not fixed).\nFollow-up: have a look at the variance estimates for the REs - how wide are they? Are the RE estimates effectively “free” to move?\n\n\n\n\n\n4.7.2 Predictions\nRelated to the interpretation of an RE is the question whether the REs should be included when you make predictions or calculate other outputs of the model, e.g. residuals.\n\nMarginal predictions = predictions without the RE, i.e. predict the grand mean\nConditional predictions = predictions with RE\n\nIn some, but unfortunately not all packages, the predict function can be adjusted. In lme4, this is via the option re.form, which is available in the predict and simulate function. Let look again at a simple random intercept model\n\nm = lmer(normexam ~ standLRT + sex +  (1 | school), data = Exam)\n\nTo generate predictions from this model, use the following syntax.\n\npredict(m, re.form = NULL) # include all random effects\npredict(m, re.form = ~0) # include no random effects, identical re.form = NA\npredict(m, re.form = ~ (1|school)) # condition ONLY on 1|school\n\nThe third option is in this case identical to the first, but if we would add more REs to the model, it would differ.\n\n\n\n\n\n\nTip\n\n\n\nDo you know how to get the correct help function for an S3 object? Concretely, how do we get the help for predicting with a fitted mixed model? ?predict alone will not work, because this will call the general help. What you need is predict.CLASSNAME, where CLASSNAME is the class of the object on which you want to work on. In R, you get the class via\n\nclass(m)\n\n[1] \"lmerModLmerTest\"\nattr(,\"package\")\n[1] \"lmerTest\"\n\n\nSo, a model fitted with lme4 is of class lmerMod. If we want to get the help for predicting from such a class, we have to type\n\n?predict.merMod\n\n\n\nA second issue with predictions is that, unlike for lm and glm objects, lme4 again does not calculate a standard error on the predictions due to the degree of freedom problem. Unfortunately, the lmerTest package does not help us in this case either, so now we have to resort to the parametric bootstrap, which is implemented in lme4\nFor this, we generate first a function that generates predictions\n\npred &lt;- function(m) predict(m,re.form = NA)\n\nNote regarding this function that\n\nWe could add the argument newdata if we wanted to predict to newdata\nHere, we have re.form = NA, which is identical to reform = ~0, meaning that we predict the grand mean\n\nSecond, we now bootstrap this prediction to generate an uncertainty of the prediction\n\nboot &lt;- bootMer(m, pred, nsim = 100, re.form = NA)\n\n# visualize results\nplot(boot)\n\n# get 95% confidence interval \nconfint(boot)\n\n\n\n4.7.3 REML vs. MLE\nFrequentist linear mixed models are usually estimated with REML = restricted maximum likelihood, rather than with classical MLE. Most packages allow you to switch between REML and MLE. The motivation for REML is that for limited data (non-asymptotics), the MLE for variance components of the model are typically negatively biased, because the uncertainty of the fixed effects is not properly accounted for. If you go through the mathematics, you can make an analogy to the bias-corrected sampling variance (Bates 2011).\nREML uses a mathematical transformation to first obtain the residuals conditional on the fixed effect components of the model (thus accounting for the df lost in this part), and then estimating variance components. One can view REML as a special case of an expectation maximization (EM) algorithm.\nMost packages offer an option to switch between REML and ML estimation of the model. In lme4, this is done via\nRegarding when to use what, use the following rule of thumb:\n\nPer default, use REML\nSwitch to ML if you use downstream any model functions that use the likelihood, unless you are really sure that the calculation you do is also valid for REML.\n\n\n\n\n\n\n\nTip\n\n\n\nIn the next part of the book, we will talk about model selection via LRTs and AIC. Because those depend on the likelihood, it is generally recommended to switch from REML to ML in this case. The reason is that, because REML estimates the REs conditional on the fixed effects, REML likelihoods are not directly comparable if fixed effects are changed. I would recommend to stick to this, although I have to say that I have always wondered how big this problem is, compared to the other df problems associated to mixed models (see below).\n\n\n\n\n4.7.4 Model selection and hypothesis tests with mixed models\nFor model selection, the degrees of freedom problem means that normal model selection techniques such as AIC or naive LRTs don’t work on the random effect structure, because they don’t count the correct degrees of freedom.\n\n4.7.4.1 Selecting on fixed effects\nThe easy part is the selection on the fixed effect structure - if we can assume that by changing the fixed effect structure, the flexibility of the REs doesn’t change a lot (you would see this by looking at the RE sd), we can use all standard model selection methods (with all their caveats) as before on the fixed effect structure. All I will say about model selection on standard models applies also here: good for predictions, rarely a good idea if your goal is causal inference (see later section).\n\n\n\n\n\n\nCareful when selecting on models fit with REML\n\n\n\nAs discussed, LMMs are per default fit with REML. For model selection, you should use the likelihood of these models. See also (Verbyla 2019)\n\n\n\n\n4.7.4.2 A prior selection on random effects\nWhen changing the RE structure, you should not use standard AIC or LRTs, because of the df problem. For that reason, I would generally recommend to fix the RE structure a priori and keep selection on REs to a minimum.\nWhat structure you should choose may depend on the experimental setting and scientific / biological expectations, but a base recipe for a limited number of data is:\n\nadd random intercept on all obvious grouping variables\ncheck residuals per group (e.g. with the plot function below), or simulated LRT (see below) and add random slope if needed\n\n\nm1 &lt;- lmer(y ~ x + (1|group))\n\nplot(m1, \n     resid(., scaled=TRUE) ~ fitted(.) | group, \n     abline = 0)\n\nI would deviate from this recipe and also add random slopes a priori when I have either sufficient power to support the random slopes without problems (see also section on model selection), or if there are good a priori reasons to expect random slopes.\n\n\n4.7.4.3 Simulated LRTs\nIf you want to formally select on the variance structure, the best way is to use a simulated likelihood-ratio test (LRT), based on the parametric bootstrap, which we introduced in the section on ANOVA. In such a test, we make the same assumptions as in a standard LRT, which is:\n\nWe want to compare 2 nested models M0, M1\nH0: simpler model M0 is correct\nTest statistic: M1/M0 or log(M1/M0) = log(M1) - log(M0)\n\nHowever, unlike in the standard LRT, where we assumed that the test statistic is Chi2 distributed, we use the parametric bootstrap (see section on non-parametric methods) to generate new data under H0, fit M0 and M1 to this simulated data, and thus generate a null distribution of the LR for these two models.\nSimulated LRTs for mixed models are implemented in a number of packages. To my knowledge, one of the most general functions is DHARMa::simulateLRT. Here an example:\n\nset.seed(123)\ndat &lt;- DHARMa::createData(sampleSize = 200, randomEffectVariance = 1)\n\nlibrary(lme4)\nm1 = glmer(observedResponse ~ Environment1 + (1|group), data = dat, family = \"poisson\")\nm0 = glm(observedResponse ~ Environment1 , data = dat, family = \"poisson\")\n\nDHARMa::simulateLRT(m0, m1)\n\n\n\n\n\n    DHARMa simulated LRT\n\ndata:  m0: m0 m1: m1\nLogL(M1/M0) = 225.8, p-value &lt; 2.2e-16\nalternative hypothesis: M1 describes the data better than M0\n\n\nIf you want to understand the method in more detail, you can expand the hand-coded example of a simulated LRT below:\n\n\n\n\n\n\nHand-coded simulated LRT\n\n\n\n\n\nWe use the models from the previous example - first, let’s look at the observed LR\n\nobservedLR = logLik(m1) - logLik(m0)\n\nThe log LR of m1 (the RE model) over m0 is 225, meaning that seeing the observed data under m1 is exp(225) times more likely than under m0.\nAs noted, the complex model will always fit better, so that the data is more likely under M1 is expected, but is the increase significant? In this case, the LR is actually so large that we actually wouldn’t need to test. A rough rule of thumb is that you need a log LR of 2 for each df that you add, and here we have an RE with 10 groups, so even if we could 1 df for each RE group, this should be easily significant. Nevertheless, let’s do the formal test, and this time by hand.\nFirst, we write a function that creates new LRs under H0, by simulating new data from the fitted M0, and re-fitting M0/M1 to this data, and return the LR.\n\nresampledParameters = function(){\n  newData = dat\n  newData$observedResponse = unlist(simulate(m0))\n  mNew0 = glm(observedResponse ~ Environment1, data = newData, family = \"poisson\")\n  mNew1 = glmer(observedResponse ~ Environment1 + (1|group), data = newData, family = \"poisson\")\n  return(logLik(mNew1) - logLik(mNew0))\n}\n\nNow, let’s generate the null distribution. There will be a lot of warnings which you can usually ignore. The reason is that we simulate the data under H0, i.e. with no RE, and thus the mixed model estimates the RE variance to zero, which creates a warning.\n\nnullDistribution = replicate(500, resampledParameters())\n\nIf we plot the null distribution, we see that if the data would really not have an RE, we would expect an increase of likelihood for the more complicated model of not more than 4 or so.\n\nhist(nullDistribution, breaks = 50, main = \"Null distribution log(L(M1) / L(M0))\")\n\n\n\n\nHowever, what we actually observe is an increase of 225. I rescaled the x axis to make this visible.\n\nhist(nullDistribution, breaks = 50, main = \"Null distribution log(L(M1) / L(M0))\", xlim = c(-5,250))\nabline(v = observedLR, col = \"red\")\n\n\n\n\nThe p-value is 0 obviously\n\nmean(nullDistribution &gt; observedLR)\n\n[1] 0\n\n\n\n\n\n\n\n\n4.7.5 Variance partitioning / ANOVA\nAlso variance partitioning in mixed models is a bit tricky, as (see type I/II/III ANOVA discussion) fixed and random components of the model are in some way “correlated”. Moreover, a key question is (see also interpretation above): Do you want to count the random effect variance as “explained”, or “residual”. The most common approach is the hierarchical partitioning proposed by by Nakagawa & Schielzeth 2013, Nakagawa et al. (2017), which is implemented in the MuMIn package. With this, we can run\n\nlibrary(MuMIn)\n\nr.squaredGLMM(m2) \n\n           R2m       R2c\n[1,] 0.3303846 0.4210712\n\n\nInterpretation\n\nR2m: Marginal R2 value associated with fixed effects.\nR2c: Conditional R2 value associated with fixed effects plus the random effects.\n\nIt is important to note that this is a description of the fitted model, not necessarily valid inference - the reason is that if you add a lot of REs, you will tend to get a lot of variance explained by the RE structure. If you want to correct for this, one option is to calculate R2 values under a cross-validation."
  },
  {
    "objectID": "2C-RandomEffects.html#case-studies",
    "href": "2C-RandomEffects.html#case-studies",
    "title": "4  Linear mixed models",
    "section": "4.8 Case studies",
    "text": "4.8 Case studies\n\n4.8.1 Honeybee Data\nWe use a dataset on bee colonies infected by the American Foulbrood (AFB) disease. The study measured spores of the bacterium “Paenibacillus larvae” (variable Spobee) that infect honeybee larvae, as well as the degree of the infection of the colony (integer: 0 (none), 1 (minor), 2 (moderate), 3 (major)).\nThe scientific question is if colonies with a higher degree of infection also exhibit more spores, i.e. if infection predicts spores. Additional variables that you can consider are BeesN (Bees per hive) and Hive (ID of the respective hive. (integer, 1 - 24).)\nTask: fit an appropriate linear mixed model on this data.\n\nlibrary(EcoData)\nstr(bees)\n\n'data.frame':   72 obs. of  6 variables:\n $ Spobee   : num  6.67 13.33 6.67 6.67 20 ...\n $ Hive     : int  1 1 1 2 2 2 3 3 3 4 ...\n $ X        : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Y        : num  0 0 0 91 91 91 262 262 262 353 ...\n $ Infection: int  0 0 0 0 0 0 0 0 0 0 ...\n $ BeesN    : int  95000 95000 95000 95000 95000 95000 85000 85000 85000 90000 ...\n\n\nPerform the data analysis, according to the hypothesis discussed in the course.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nBecause of the skewed distribution of Spobee, it is better to use log(Spobee + 1) or something similar as response (else you will not get nice residuals)\nI decided to add BeesN in the regression because it could be a confounder (more on confounding in section on causal inference)\nIt is natural to model hive as a random intercept (1|Hive). I don’t expect that the infection ~ spores relationship should be different between hives, but can test for this after fitting the model\n\n\nlibrary(lme4)\nfit &lt;- lmer(log(Spobee + 1) ~ Infection + BeesN + (1|Hive), data = bees)\nsummary(fit)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: log(Spobee + 1) ~ Infection + BeesN + (1 | Hive)\n   Data: bees\n\nREML criterion at convergence: 260.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.27939 -0.45413  0.09209  0.52159  1.64023 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Hive     (Intercept) 4.8463   2.2014  \n Residual             0.6033   0.7767  \nNumber of obs: 72, groups:  Hive, 24\n\nFixed effects:\n              Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)  5.583e+00  1.907e+00  2.100e+01   2.927  0.00805 ** \nInfection    2.663e+00  5.528e-01  2.100e+01   4.818 9.22e-05 ***\nBeesN       -2.068e-05  2.558e-05  2.100e+01  -0.808  0.42804    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n          (Intr) Infctn\nInfection -0.476       \nBeesN     -0.966  0.398\nfit warnings:\nSome predictor variables are on very different scales: consider rescaling\n\n\nStandard residual plots look fine, possible minor evidence for heteroskedasticity\n\nplot(fit)\n\n\n\n\nLet’s have a look at the random effects\n\nx = ranef(fit)\nqqnorm(x$Hive$`(Intercept)`)\nqqline(x$Hive$`(Intercept)`)\n\n\n\nshapiro.test(x$Hive$`(Intercept)`)\n\n\n    Shapiro-Wilk normality test\n\ndata:  x$Hive$`(Intercept)`\nW = 0.95543, p-value = 0.3537\n\n\nLooks good. Could also check with DHARMa. The default plot calculates residuals, i.e. residuals based on random effect and residual variation together. This leads to more noise, which, however, I would disregard in this case.\n\nsimulateResiduals(fit, plot = T)\n\n\n\n\nObject of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. \n \nScaled residual values: 0.272 0.32 0.26 0.248 0.424 0.06 0.356 0.22 0.292 0.476 0.436 0.64 0.208 0.036 0.032 0.988 0.988 0.976 0.016 0.228 ...\n\n\nWith lme4, you can switch to conditinal residuals by using the re.form syntax of lme4\n\nsimulateResiduals(fit, plot = T, re.form = NULL)\n\n\n\n\nObject of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. \n \nScaled residual values: 0.388 0.672 0.372 0.624 0.96 0.008 0.688 0.192 0.568 0.364 0.32 0.84 0.948 0.208 0.12 0.864 0.732 0.192 0.02 0.944 ...\n\n\nResidual plot to check for evidence of a random slope shows that hives are either infected or not, thus doesn’t make sense to add a random slope\n\nplot(fit, \n     resid(., scaled=TRUE) ~ fitted(.) | Hive, \n     abline = 0)\n\n\n\n\n\n\n\n\n\n4.8.2 College Student Performance\nThe GPA (college grade point average) data is a longitudinal data set (also named panel data, German: “Längsschnittstudie”. A study repeated at several different moments in time, compared to a cross-sectional study (German: “Querschnittstudie”) which has several participants at one time). In this data set, 200 college students and their GPA have been followed 6 consecutive semesters. Look at the GPA data set, which can be found in the EcoData package:\n\nlibrary(EcoData)\nstr(gpa)\n\nIn this data set, there are GPA measures on 6 consecutive occasions, with a job status variable (how many hours worked) for the same 6 occasions. There are two student-level explanatory variables: The sex (1 = male, 2 = female) and the high school gpa. There is also a dichotomous student-level outcome variable, which indicates whether a student has been admitted to the university of their choice. Since not every student applies to a university, this variable has many missing values. Each student and each year of observation have an id.\nTask\nAnalyze if the student’s GPA improves over time (occasion)! Here a few hints to look at:\n\nConsider which fixed effect structure you want to fit. For example, you might be interested if males and females differ in their temporal trend\nStudent is the grouping variable -&gt; RE. Which RE structure do you want to fit? A residual plot may help\nFor your benefit, have a look at the difference in the regression table (confidence intervals, coefficients and p-values) of mixed and corresponding fixed effects model. You can also look at the estimates of the mixed effects model (hint: ?ranef).\nAfter having specified the mixed model, have a look at residuals.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(lme4)\nlibrary(glmmTMB)\nlibrary(EcoData)\n# initial model with a random intercept and fixed effect structure based on\n# causal assumptions\n\ngpa$sOccasion = scale(gpa$occasion)\ngpa$nJob = as.numeric(gpa$job)\n\nfit &lt;- lmer(gpa ~ sOccasion*sex + nJob + (1|student), data = gpa)\nsummary(fit)\n\n# plot seems to show a lot of differences still, so add random slope\nplot(fit, \n     resid(., scaled=TRUE) ~ fitted(.) | student, \n     abline = 0)\n\n# slope + intercept model\nfit &lt;- lmer(gpa ~ sOccasion*sex + nJob + (sOccasion|student), data = gpa)\n\n# checking residuals - heteroskedasticity \nplot(fit)\n\n# We only learn later how to specify a variable, dispersion, but as a teaser - I'm modelling this using glmmTMB, alternatively could add weights nlme::lme, which also allows specifying mixed models with all variance modelling options that we discussed for gls, but random effect specification is different than here\nfit &lt;- glmmTMB(gpa ~ sOccasion*sex + nJob + (sOccasion|student), data = gpa, dispformula = ~ sOccasion)\nsummary(fit)\n\n# unfortunately, the dispersion in this model cannot be reliably checked, because the functions for this are not (yet) implemented in glmmTMB\nplot(residuals(fit, type = \"pearson\") ~ predict(fit)) # not implemented\nlibrary(DHARMa)\nsimulateResiduals(fit, plot = T) \n# still, the variable dispersion model is highly supported by the data and clearly preferable over a fixed dispersion model\n\n\n\n\n\n\n\n\nBates, Douglas. 2011. “Computational Methods for Mixed Models.” Vignette for Lme4.\n\n\nVerbyla, Arunas Petras. 2019. “A Note on Model Selection Using Information Criteria for General Linear Models Estimated Using REML.” Australian & New Zealand Journal of Statistics 61 (1): 39–50. https://doi.org/10.1111/anzs.12254."
  },
  {
    "objectID": "3A-MissingData.html",
    "href": "3A-MissingData.html",
    "title": "5  Missing data",
    "section": "",
    "text": "5.0.2 Summarizing NAs\nAs mentioned, a quick way to get a summary about the NAs in your data is to run the summary() function, which will return the number of NAs per predictor. You can check this out via running:\n\nsummary(airquality)\n\nThe problem with the summary is that in regression models, an observation will be removed as soon as one of the predictors has an NA. It is therefore crucial in which combinations NAs occur - we need complete rows. Let’s visualize the position of NAs in the data\n\nimage(is.na(t(airquality)), axes = F)\naxis(3, at = seq(0,1, len = 6), labels = colnames(airquality))\n\n\n\n\nWe see that NAs are in Ozone and Solar.R, and sometimes they’re together, sometimes they are seperate. To check if you have a complete row, we can use the function complete.cases(), which returns a vector with T/F if a row is complete or not. Here, I check how many complete observations we have\n\nsum(complete.cases(airquality))\n\nTo create a dataset with all observations that contain NAs removed, use\n\nairquality[complete.cases(airquality), ]\n\nNote that in general, you should only do this for the variables that you actually use in your regression, else you might remove more observations than needed. Either you select the columns by hand, or a small hack is to use the model.matrix, which allows you to copy / past the regression formula syntax\n\nrows = rownames(model.matrix(Ozone ~ Wind + Temp, data = airquality))\nairquality = airquality[rows, ]\n\n\n\n5.0.3 Missing at random\nAfter having an overview about the NAs in your data, as second step in the analysis (or ideally already during the design of the experiment) is to ask yourself under which process the missingness was created. We distinguish three crucial classes of processes:\n\nMCAR = missing completely at random\nMAR = missing at random\nMNAR = missing not at random\n\nThe easiest cases is arguably MCAR = missing completely at random - in this case, the occurrences of NAs in variable X are completely random, and do not correlate with either X or other variables. This occurs when the causes of NAs are completely unrelated to the variables in the data. The opposite case is MNAR = missing not at random - in this case, the occurrences of NAs in variable X depend on the (true) value of X and possibly from other values in the dataset. A mix is MAR = missing at random - in this case, the occurrences of NAs in variable X do not depend on X, but possibly on other variables. See also (Bhaskaran and Smeeth 2014).\n\n\n5.0.4 Imputation\nThe main alternative to throwing out observations with NAs is filling them up. This process is known as imputation. Multiple refers to the insight that imputing several times often improves the result. There are a number of packages that do imputations, for example missRanger\n\nlibrary(missRanger)\nairqualityImp<- missRanger(airquality)\n\nLet’s fit the model to this imputed data:\n\nfit = lm(Ozone ~ Wind + Temp, data = airqualityImp)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Wind + Temp, data = airqualityImp)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.251 -13.695  -2.856  11.390 100.367 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -71.0332    23.5780  -3.013   0.0032 ** \nWind         -3.0555     0.6633  -4.607 1.08e-05 ***\nTemp          1.8402     0.2500   7.362 3.15e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.85 on 113 degrees of freedom\nMultiple R-squared:  0.5687,    Adjusted R-squared:  0.5611 \nF-statistic:  74.5 on 2 and 113 DF,  p-value: < 2.2e-16\n\n\nThe p-values are smaller now, as one would expect, having more data. The obvious problem with this is that there is uncertainty with the imputed values, which is currently not accounted for. One option to do this is running a multiple imputation.ze\n\n# run 20 imputations\nairqualityMImp <- replicate(20, missRanger(airquality), simplify = FALSE)\n\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\niter 4: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\niter 4: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\niter 4: .\niter 5: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\niter 4: .\niter 5: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\niter 4: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\niter 4: .\niter 5: .\niter 6: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\niter 4: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\niter 4: .\niter 5: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\niter 4: .\n\nMissing value imputation by random forests\n\n  Variables to impute:      Solar.R\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\niter 1: .\niter 2: .\niter 3: .\n\n# fit 20 models\nmodels <- lapply(airqualityMImp, function(x) lm(Ozone ~ Wind + Temp, x))\n\n# use mice package to compute corrected p-values\nrequire(mice)\nsummary(pooled_fit <- pool(models)) \n\n         term   estimate  std.error statistic       df      p.value\n1 (Intercept) -71.033218 23.5779922 -3.012692 111.0406 3.207552e-03\n2        Wind  -3.055491  0.6632503 -4.606844 111.0406 1.096691e-05\n3        Temp   1.840179  0.2499634  7.361793 111.0406 3.377387e-11\n\n\nMore on missing data and imputation methods in Stef van Buuren’s book “Flexible Imputation of Missing Data”\n\nlibrary(mitools)\n\ndata(smi)\nmodels<-with(smi, glm(drinkreg~wave*sex,family=binomial()))\nsummary(MIcombine(models))\n\nMultiple imputation results:\n      with(smi, glm(drinkreg ~ wave * sex, family = binomial()))\n      MIcombine.default(models)\n                results         se      (lower     upper) missInfo\n(Intercept) -2.25974358 0.26830731 -2.78584855 -1.7336386      4 %\nwave         0.24055250 0.06587423  0.11092461  0.3701804     12 %\nsex          0.64905222 0.34919264 -0.03537187  1.3334763      1 %\nwave:sex    -0.03725422 0.08609199 -0.20623121  0.1317228      7 %\n\n\n\n\n\n\nBhaskaran, Krishnan, and Liam Smeeth. 2014. “What Is the Difference Between Missing Completely at Random and Missing at Random?” International Journal of Epidemiology 43 (4): 1336–39."
  },
  {
    "objectID": "3B-CausalInference.html",
    "href": "3B-CausalInference.html",
    "title": "6  Causal inference",
    "section": "",
    "text": "The most fundamental distinction in strategies for model choice is if we want to estimate effects, or if we want to predict. If we want to estimate effects, we nearly always want to estimate causal effects.\nLet me first define what we mean by “a causal effect”: if we have a system with a number of variables, the causal effect of A on B is the change in B that would happen if we changed A, but kept all other aspects of this system constant.\nAssume we look at the effect of coffee consumption on Lung Cancer. Assume further that there is no such effect. However, there is an effect of smoking on lung cancer, and for some reason, smoking also affects coffee consumption.\nIt is common to visualize and analyze such relationships in a causal graph. Here, I use the ggdag package\n\nlibrary(ggdag)\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(ggplot2)\ntheme_set(theme_dag())\ndag = confounder_triangle(x = \"Coffee\", y = \"Lung Cancer\", z = \"Smoking\") \nggdag(dag, text = FALSE, use_labels = \"label\")\n\n\n\n\nWe can use the ggdag package (or to be more exact: the underlying daggitty package) to explore the implications of this graph. The way I created the graph already includes the assumption that we are interested in the effect of Coffee on Lung Cancer. I can use the function ggdag_dconnected() to explore if those two are d-connected, which is just a fancy word for correlated.\n\nggdag_dconnected(dag, text = FALSE, use_labels = \"label\")\n\n\n\n\nIn this case, the plot highlights me to the fact that Coffee and Lung cancer d-connected. What that means is: if I plot coffee against lung cancer, I will see a correlation between them, even though coffee consumption does not influence lung cancer at all. We can easily confirm via a simulation that this is true:\n\nsmoking <- runif(50)\nCoffee <- smoking + rnorm(50, sd = 0.2)\nLungCancer <- smoking + rnorm(50, sd =0.2)\nfit <- lm(LungCancer ~ Coffee)\nplot(LungCancer ~ Coffee)\nabline(fit)\n\n\n\n\nSo, if we would fit a regression of LungCancer ~ Coffee, we would at least conclude that there is a correlation between the two variables. Many people would even go a step further, and conclude that Coffee consumption affects lung cancer. This is a classical misinterpretation created by the confounder smoking. Realizing this, I could ask the ggdag_dconnected function what would happen if I control for the effect of smoking.\n\nggdag_dconnected(dag, text = FALSE, use_labels = \"label\", controlling_for = \"z\")\n\n\n\n\nThe result conforms with our intuition - once we control for smoking, there will be no correlation between Coffee and Lung Cancer (because there is no causal effect between them). In causal lingo, we say the two are now d-separated."
  },
  {
    "objectID": "3B-CausalInference.html#how-to-control",
    "href": "3B-CausalInference.html#how-to-control",
    "title": "6  Causal inference",
    "section": "6.2 How to control?",
    "text": "6.2 How to control?\nBut how do we control for the confounder smoking?\n\n6.2.1 Experimental control\nThe conceptually easiest approach is: we perform a controlled experiment. The entire idea experiments is that we vary only one (or a few) factors, keeping the others constant, and thus we are able to “see” causal effects directly. So, if we did an experiment with people that all smoke the same, varying coffee, we would get the right result:\n\nsmoking <- rep(0.5, 50)\nCoffee <- smoking + rnorm(50, sd = 0.2)\nLungCancer <- smoking + rnorm(50, sd =0.2)\nfit <- lm(LungCancer ~ Coffee)\nplot(LungCancer ~ Coffee)\nabline(fit)\n\n\n\n\nWhich us to an important point: people that run controlled experiments always estimate causal effects, unless something went wrong with the control. Actually, all advice regarding experimental design (control, randomization) is aimed are removing any possible confounding with other factors, so that we can isolate causal effects.\n\n\n6.2.2 Synthetic control\nIn observational data, we have to use statistical methods to achieve synthetic control. Luckily, we have a great tool for that: the multiple regression.\nThe multiple regression can estimate the effect of coffee, corrected for the effect of smoking. Thus, by including smoking in the multiple regression, we are “virtually” holding smoking constant, thus allowing us to estimate the causal effect of coffee. Let’s try this out:\n\nsmoking <- runif(100)\nCoffee <- smoking + rnorm(100, sd = 0.2)\nLungCancer <- smoking + rnorm(100, sd =0.2)\nfit1 <- lm(LungCancer ~ Coffee)\nfit2 <- lm(LungCancer ~ Coffee + smoking)\nplot(LungCancer ~ Coffee)\nabline(fit1)\nabline(fit2, col = \"red\")\nlegend(\"topleft\", c(\"simple regression\", \"multiple regression\"), col = c(1,2), lwd = 1)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nGAMs are particularly useful for confounders. If you have confounders, you usually don’t care that the fitted relationship is a bit hard to interpret, you just want the confounder effect to be removed. So, if you want to fit the causal relationship between Ozone ~ Wind, account for the other variables, a good strategy might be:\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.8-40. For overview type 'help(\"mgcv-package\")'.\n\nfit = gam(Ozone ~ Wind + s(Temp) + s(Solar.R) , data = airquality)\nsummary(fit)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nOzone ~ Wind + s(Temp) + s(Solar.R)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  72.2181     6.3166  11.433  < 2e-16 ***\nWind         -3.0302     0.6082  -4.982 2.55e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n             edf Ref.df      F p-value    \ns(Temp)    3.358  4.184 14.972  <2e-16 ***\ns(Solar.R) 2.843  3.551  3.721  0.0115 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.664   Deviance explained = 68.6%\nGCV = 402.11  Scale est. = 372.4     n = 111\n\n\nIn this way, you still get a nicely interpretable linear effect for Wind, but you don’t have to worry about the functional form of the other predictors."
  },
  {
    "objectID": "3B-CausalInference.html#a-framework-for-causal-analysis",
    "href": "3B-CausalInference.html#a-framework-for-causal-analysis",
    "title": "6  Causal inference",
    "section": "6.3 A framework for causal analysis",
    "text": "6.3 A framework for causal analysis\nWhat we have just seen is an example of a causal analysis. The goal of a causal analysis is to control for other variables, in such a way that we estimate the same effect size we would obtain if only the target predictor was manipulated (as in a randomized controlled trial). If we are after causal effects, the correct selection of variables is crucial, while it isn’t if we just want to predict.\nYou probably have learned in your intro stats class that, to do so, we have to control for confounders. I am less sure, however, if everyone is clear about what a confounder is. In particular, confounding is more specific than having a variable that correlates with predictor and response. The direction is crucial to identify true confounders. Imagine that there is a third variable that is included\n\ndag = collider_triangle(x = \"Coffee\", y = \"Lung Cancer\", m = \"Nervousness\") \nggdag(dag, text = FALSE, use_labels = \"label\")\n\n\n\n\nLet’s simulate some data according to this structure\n\nset.seed(123)\nCoffee <- runif(100)\nLungCancer <- runif(100)\nnervousness = Coffee + LungCancer + rnorm(100, sd = 0.1)\n\nIf we fit a multiple regression on this structure, we erroneously conclude that there is an effect of coffee on lung cancer\n\nfit1 <- lm(LungCancer ~ Coffee + nervousness)\nsummary(fit1)\n\n\nCall:\nlm(formula = LungCancer ~ Coffee + nervousness)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.316061 -0.053494  0.002907  0.071460  0.164066 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.07886    0.02575   3.063  0.00284 ** \nCoffee      -0.90108    0.04466 -20.179  < 2e-16 ***\nnervousness  0.88282    0.03307  26.696  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.09188 on 97 degrees of freedom\nMultiple R-squared:  0.8811,    Adjusted R-squared:  0.8787 \nF-statistic: 359.5 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nthis time, the univariate regression gets it right:\n\nfit1 <- lm(LungCancer ~ Coffee)\nsummary(fit1)\n\n\nCall:\nlm(formula = LungCancer ~ Coffee)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.51061 -0.21159 -0.00986  0.17769  0.48695 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.55449    0.05342  10.380   <2e-16 ***\nCoffee      -0.08077    0.09314  -0.867    0.388    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2641 on 98 degrees of freedom\nMultiple R-squared:  0.007616,  Adjusted R-squared:  -0.00251 \nF-statistic: 0.7521 on 1 and 98 DF,  p-value: 0.3879\n\n\nWhat we’ve seen here is a collider bias - a collider is a variable that is influenced by predictor and response. Although it correlates with predictor and response, correcting for it (or including it) in a multiple regression will create a bias on the causal link we are interested in (Corollary: Including all variables is not always a good thing).\nThere is an entire framework to analyze which variables you need to include in the regression to achieve proper control (Pearl 2000, 2009). In the following picture, I have summarized the three basic structures. The one that we haven’t discussed yet is a mediation structure. A mediator is an intermediate variable that “mediates” an effect between exposure and response.\n\n\n\n\n\nHow do we deal with the basic causal structures colliders, mediators and confounders in a regression analysis? Start by writing down the hypothesis / structure that you want to estimate causally (for example, in A, B “Plant diversity” -> Ecosystem productivity). Then\n\nControl for all confounding structures\nDo not control for colliders and other similar relationships, e.g. “M-Bias” (red paths).\nIt depends on the question whether we should control for mediators (yellow paths).\n\nNote: If other variables are just there to correct our estimates, they are nuisance parameters (= we are not interested in them), and we should later not discuss them, as they were not themselves checked for confounding (Table 2 fallacy).\n\n\n\n\n\n\nTip\n\n\n\nThe best practical guidance paper I know on estimating causal effects is Lederer et al., 2018, “Control of Confounding and Reporting of Results in Causal Inference Studies. Guidance for Authors from Editors of Respiratory, Sleep, and Critical Care Journals” which is available here.\n\n\n\n\n\n\n\n\nCase study 1\n\n\n\nTake the example of the past exercise (airquality) and assume, the goal is to understand the causal effect of Temperature on Ozone (primary hypothesis). Draw a causal diagram to decide which variables to take into the regression (i.e. noting which are confounders, mediators or colliders), and fit the model.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nSolar.R could affect both Temp, Ozone -> Coufounder, include\nWind could affect Temp, Ozone -> Coufounder, include. Alternatively, one could assume that Temp is also affecting Wind, then it’s a mediator\nI would not include Month, as the Month itself should not affect Ozone, it’s the Temp, Solar.R of the month that must have the effect. It’s more like a placeholder, but if you include it it will nearly act as a collider, because it can snitch away some of the effects of the other variables."
  },
  {
    "objectID": "3B-CausalInference.html#graphs-and-models-on-graphs",
    "href": "3B-CausalInference.html#graphs-and-models-on-graphs",
    "title": "6  Causal inference",
    "section": "6.4 Graphs and models on graphs",
    "text": "6.4 Graphs and models on graphs\nhttps://www.andrewheiss.com/blog/2020/02/25/closing-backdoors-dags/\nLet’s take a more complicated example. We will use data from Grace & Keeley (2006), who try to understand plant diversity following wildfires in fire-prone shrublands of California. The authors have measured various variables, and they have a specific hypothesis how those are related (see below).\n\nlibrary(piecewiseSEM)\ntheme_set(theme_dag())\n\ndag <- dagify(rich ~ distance + elev + abiotic + age + hetero + firesev + cover,\n  firesev ~ elev + age + cover,\n  cover ~ age + elev + abiotic ,\n  exposure = \"cover\",\n  outcome = \"rich\"\n  )\n\nggdag(dag)\n\n\n\n\nFor this exercise, I want to assume that we are particularly interested in the causal effect of cover on species richness. I have specified this in the dag above already. With the ggdag_paths() command, I can isolate all paths that would create a correlation between cover and richness\n\nggdag_paths(dag)\n\n\n\n\nLooking at the paths, I can see confounding structures, for example for age, abiotic and elevation. So I should definitely control for them. The function\n\nggdag_adjustment_set(dag)\n\n\n\n\nhelps me by suggesting which confounders I should control for. Note that the function doesn’t suggest to adjust for mediation structures per default, but keeps them in the graph, so you have to decide what to do with them. You can directly ask to include adjustment for mediation if you run\n\nggdag_adjustment_set(dag, effect=\"direct\")\n\n\n\n\nin this case, plot suggest to adjust for fireseverity, to isolate the direct effect of cover on richness.\nSo, the model we should fit is\n\nfit = lm(rich ~ cover + abiotic + elev + firesev + age, data = keeley)\nsummary(fit)\n\n\nCall:\nlm(formula = rich ~ cover + abiotic + elev + firesev + age, data = keeley)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.146  -8.030   0.033   7.909  37.819 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.033165  11.432393   1.140    0.258    \ncover        6.692065   4.754448   1.408    0.163    \nabiotic      0.790552   0.182962   4.321 4.24e-05 ***\nelev         0.007117   0.005565   1.279    0.204    \nfiresev     -1.285966   0.946084  -1.359    0.178    \nage         -0.176153   0.121290  -1.452    0.150    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.23 on 84 degrees of freedom\nMultiple R-squared:  0.3815,    Adjusted R-squared:  0.3446 \nF-statistic: 10.36 on 5 and 84 DF,  p-value: 9.213e-08\n\n\n\n6.4.1 Table II fallacy\nWhen reporting this, note that we have only corrected the relationship rich ~ cover for confounding. Consequently, we should not report the entire regression table above, or at least make a clear distinction about which variables have been corrected, and which haven’t.\nThe blind reporting of the entire regression table is known as Table II fallacy (because regression tables are often Table II in a paper).\n\n\n6.4.2 Structural equation models (SEMs)\nBut what if we are interested in all the causal relationships? If we have a graph already, the best option is to fit a structural equation model.\nStructural equation models (SEMs) are models that are designed to estimate entire causal diagrams. For GLMs responses, you will currently have to estimate the DAG (directed acyclic graph) piece-wise, e.g. with https://cran.r-project.org/web/packages/piecewiseSEM/vignettes/piecewiseSEM.html.\nFor linear (= normally ditributed) SEMs, we can estimate the entire DAG in one go. This also allows to have unobserved variables in the DAG. One of the most popular packages for this is lavaan. The advantage of lavaan is that you can also have unobserved covariates which can me modelled latent.\nIf you have REs or GLM-type dependencies, the only way to fit everything in one go is to go to use Bayesian Inference, e.g. STAN. However, what you can do is to fit your models piece-wise. The most popular package for this purpose is piecewiseSEM. As of May 2023, supported models include lm, glm, gls, Sarlm, lme, glmmPQL, lmerMod, merModLmerTest,glmerMod, glmmTMB, gam.\n\nlibrary(piecewiseSEM)\n\nmod = psem(\n  lm(rich ~ distance + elev + abiotic + age + hetero + firesev + cover, data = keeley),\n  lm(firesev ~ elev + age + cover, data = keeley), \n  lm(cover ~ age + elev + hetero + abiotic, data = keeley)\n)\n\nsummary(mod)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\nStructural Equation Model of mod \n\nCall:\n  rich ~ distance + elev + abiotic + age + hetero + firesev + cover\n  firesev ~ elev + age + cover\n  cover ~ age + elev + hetero + abiotic\n\n    AIC\n 1041.203\n\n---\nTests of directed separation:\n\n            Independ.Claim Test.Type DF Crit.Value P.Value \n  firesev ~ distance + ...      coef 85    -0.8264  0.4109 \n    cover ~ distance + ...      coef 84     0.4201  0.6755 \n   firesev ~ abiotic + ...      coef 85    -1.1799  0.2413 \n    firesev ~ hetero + ...      coef 85    -0.5755  0.5665 \n\n--\nGlobal goodness-of-fit:\n\nChi-Squared = 1.968 with P-value = 0.742 and on 4 degrees of freedom\nFisher's C = 6.543 with P-value = 0.587 and on 8 degrees of freedom\n\n---\nCoefficients:\n\n  Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate    \n      rich  distance   0.6157    0.1855 82     3.3195  0.0013       0.3599  **\n      rich      elev  -0.0092    0.0059 82    -1.5663  0.1211      -0.1569    \n      rich   abiotic   0.4881    0.1641 82     2.9741  0.0039       0.2482  **\n      rich       age   0.0241    0.1097 82     0.2199  0.8265       0.0201    \n      rich    hetero  44.4135   10.8093 82     4.1088  0.0001       0.3376 ***\n      rich   firesev  -1.0181    0.8031 82    -1.2677  0.2085      -0.1114    \n      rich     cover  12.3998    4.2206 82     2.9379  0.0043       0.2604  **\n   firesev      elev  -0.0006    0.0006 86    -0.9298  0.3551      -0.0874    \n   firesev       age   0.0473    0.0129 86     3.6722  0.0004       0.3597 ***\n   firesev     cover  -1.5214    0.5204 86    -2.9236  0.0044      -0.2921  **\n     cover       age  -0.0101    0.0024 85    -4.1757  0.0001      -0.3991 ***\n     cover      elev   0.0004    0.0001 85     2.9688  0.0039       0.2999  **\n     cover    hetero  -0.7875    0.2719 85    -2.8960  0.0048      -0.2850  **\n     cover   abiotic   0.0021    0.0042 85     0.4855  0.6286       0.0498    \n\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n\n---\nIndividual R-squared:\n\n  Response method R.squared\n      rich   none      0.57\n   firesev   none      0.30\n     cover   none      0.26\n\nplot(mod)"
  },
  {
    "objectID": "3B-CausalInference.html#case-studies",
    "href": "3B-CausalInference.html#case-studies",
    "title": "6  Causal inference",
    "section": "6.6 Case studies",
    "text": "6.6 Case studies\n\n6.6.1 Case study: Swiss fertility\nPerform a causal, a predictive and an exploratory analysis of the Swiss fertility data set called “swiss”, available in the standard R data sets. Target for the causal analysis is to estimate the causal (separate direct and indirect effects) of education on fertility, i.e. lm(Fertility ~ Education, data = swiss).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nAgriculture, Catholic could be seen as confounders or mediators, depending on whether you think Education affects the number of people being in Agriculture or Catholic, or vice versa\nInfant mortality could be a mediator or a collider, depeding on whether you think fertility -> infant mortality or infant mortality -> fertility. I would tend to see it as a mediator.\n\nFor all mediators: remember that if you want to get the total (indirect + direct) effect of education on fertility, you should not include mediators. If you want to get the direct effect only, they should be included.\n\n\n\n\n\n6.6.2 Case study: Life satisfaction\nThe following data set contains information about life satisfaction (lebensz_org) in Germany, based on the socio-economic panel.\n\nlibrary(EcoData)\n?soep\n\nPerform a causal analysis of the effect of income on life satisfaction, considering possible confounding / mediation / colliders.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNearly all other variables are confounders, gesund_org could als be a collider\nMight consider splitting data into single households, families, as effects could be very different. Alternatively, could add interactions with single, families and / or time to see if effects of income are different\n\nA possible model is\n\nfit <- lm(lebensz_org ~ sqrt(einkommenj1) + syear + sex + alter + anz_pers + \n            bildung + erwerb + gesund_org, data = soep)\nsummary(fit)\n\n\nCall:\nlm(formula = lebensz_org ~ sqrt(einkommenj1) + syear + sex + \n    alter + anz_pers + bildung + erwerb + gesund_org, data = soep)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.7735 -0.7843  0.0966  0.9387  4.9146 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -3.257e+01  1.443e+01  -2.256 0.024072 *  \nsqrt(einkommenj1)  4.741e-04  1.661e-04   2.855 0.004307 ** \nsyear              2.032e-02  7.158e-03   2.838 0.004541 ** \nsex                6.830e-02  2.073e-02   3.296 0.000984 ***\nalter              1.272e-02  7.116e-04  17.882  < 2e-16 ***\nanz_pers           7.040e-02  7.753e-03   9.081  < 2e-16 ***\nbildung            2.027e-02  3.762e-03   5.387 7.23e-08 ***\nerwerb            -1.267e-02  8.762e-03  -1.446 0.148277    \ngesund_org        -8.283e-01  1.139e-02 -72.728  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.479 on 21611 degrees of freedom\n  (1902 observations deleted due to missingness)\nMultiple R-squared:  0.2172,    Adjusted R-squared:  0.2169 \nF-statistic: 749.6 on 8 and 21611 DF,  p-value: < 2.2e-16\n\n\nNote that you shouldn’t interpret the other variables (Table II fallacy) in a causal analysis, unless the other variables are analyzed / corrected for confounders / colliders."
  },
  {
    "objectID": "3C-ModelSelection.html",
    "href": "3C-ModelSelection.html",
    "title": "7  Model selection",
    "section": "",
    "text": "Apart from causality, the arguably most fundamental idea about modelling choice is the bias-variance trade-off, which applies regardless of whether we are interested in causal effects or predictive models. The idea is the following:\n\nThe more variables / complexity we include in the model, the better it can (in principle) adjust to the true relationship, thus reducing model error from bias.\nThe more variables / complexity we include in the model, the larger our error (variance) on the fitted coefficients, thus increasing model error from variance. This means, the model adopts to the given data but no longer to the underlying relationship.\n\nIf we sum both terms up, we see that at the total error of a model that is too simple will be dominated by bias (underfitting), and the total error of a model that is too complex will be dominated by variance (overfitting):\n\n\n\n\n\nLet’s confirm this with a small simulation - let’s assume I have a simple relationship between x and y:\n\nset.seed(123)\n\nx = runif(100)\ny = 0.25 * x + rnorm(100, sd = 0.3)\n\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67139 -0.18397 -0.00592  0.17890  0.66517 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -0.002688   0.058816  -0.046    0.964  \nx            0.223051   0.102546   2.175    0.032 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2908 on 98 degrees of freedom\nMultiple R-squared:  0.04605,   Adjusted R-squared:  0.03632 \nF-statistic: 4.731 on 1 and 98 DF,  p-value: 0.03203\n\n\nAs you see, the effect is significant. Now, I add 80 new variables to the model which are just noise\n\nxNoise = matrix(runif(8000), ncol = 80)\ndat = data.frame(y=y,x=x, xNoise)\n\nfullModel = lm(y~., data = dat)\nsummary(fullModel)\n\n\nCall:\nlm(formula = y ~ ., data = dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.263130 -0.086990 -0.007075  0.095086  0.293818 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.818772   1.309581   0.625   0.5397  \nx            0.187245   0.275626   0.679   0.5056  \nX1           0.065985   0.288601   0.229   0.8217  \nX2          -0.205767   0.226155  -0.910   0.3749  \nX3          -0.241677   0.264493  -0.914   0.3729  \nX4          -0.112305   0.259382  -0.433   0.6702  \nX5          -0.007687   0.278844  -0.028   0.9783  \nX6           0.016095   0.194323   0.083   0.9349  \nX7           0.210212   0.288974   0.727   0.4763  \nX8           0.248545   0.361409   0.688   0.5004  \nX9          -0.197172   0.441156  -0.447   0.6602  \nX10          0.206384   0.246107   0.839   0.4127  \nX11         -0.253228   0.249239  -1.016   0.3231  \nX12         -0.146196   0.218184  -0.670   0.5113  \nX13         -0.191434   0.629232  -0.304   0.7644  \nX14          0.079965   0.261570   0.306   0.7633  \nX15         -0.340678   0.338473  -1.007   0.3275  \nX16         -0.565212   0.291535  -1.939   0.0684 .\nX17          0.179863   0.510105   0.353   0.7285  \nX18          0.102642   0.242366   0.424   0.6769  \nX19         -0.097458   0.286541  -0.340   0.7377  \nX20          0.030439   0.278389   0.109   0.9141  \nX21         -0.144404   0.378193  -0.382   0.7071  \nX22          0.099958   0.207603   0.481   0.6360  \nX23          0.261590   0.299691   0.873   0.3942  \nX24          0.307017   0.290396   1.057   0.3044  \nX25         -0.261906   0.273628  -0.957   0.3512  \nX26          0.348920   0.243841   1.431   0.1696  \nX27         -0.036098   0.228266  -0.158   0.8761  \nX28          0.287817   0.270679   1.063   0.3017  \nX29          0.033879   0.307206   0.110   0.9134  \nX30          0.062557   0.362398   0.173   0.8649  \nX31          0.172122   0.265290   0.649   0.5247  \nX32          0.105700   0.201368   0.525   0.6061  \nX33         -0.147496   0.252870  -0.583   0.5669  \nX34         -0.167456   0.263864  -0.635   0.5337  \nX35          0.100169   0.178501   0.561   0.5816  \nX36          0.088712   0.295791   0.300   0.7677  \nX37          0.216354   0.323429   0.669   0.5120  \nX38          0.291084   0.317911   0.916   0.3720  \nX39         -0.012332   0.240035  -0.051   0.9596  \nX40          0.243437   0.367832   0.662   0.5165  \nX41          0.380828   0.319131   1.193   0.2482  \nX42         -0.074005   0.253109  -0.292   0.7733  \nX43          0.098013   0.227937   0.430   0.6723  \nX44          0.198824   0.302047   0.658   0.5187  \nX45          0.199577   0.219516   0.909   0.3753  \nX46         -0.421600   0.369403  -1.141   0.2687  \nX47          0.154510   0.230492   0.670   0.5111  \nX48         -0.413987   0.262710  -1.576   0.1325  \nX49         -0.077130   0.228585  -0.337   0.7397  \nX50         -0.096674   0.347191  -0.278   0.7838  \nX51         -0.200855   0.236771  -0.848   0.4074  \nX52         -0.043446   0.242367  -0.179   0.8597  \nX53         -0.131982   0.369716  -0.357   0.7253  \nX54         -0.195446   0.361531  -0.541   0.5954  \nX55         -0.236722   0.256146  -0.924   0.3676  \nX56         -0.368766   0.265788  -1.387   0.1822  \nX57         -0.334641   0.257425  -1.300   0.2100  \nX58          0.231276   0.237317   0.975   0.3427  \nX59         -0.096390   0.273072  -0.353   0.7282  \nX60          0.311881   0.271568   1.148   0.2658  \nX61          0.242950   0.217372   1.118   0.2784  \nX62         -0.084825   0.442712  -0.192   0.8502  \nX63          0.029619   0.298468   0.099   0.9220  \nX64         -0.028104   0.261794  -0.107   0.9157  \nX65         -0.525963   0.361913  -1.453   0.1634  \nX66          0.144431   0.228127   0.633   0.5346  \nX67         -0.128939   0.258665  -0.498   0.6242  \nX68          0.112637   0.267554   0.421   0.6787  \nX69          0.348617   0.248531   1.403   0.1777  \nX70          0.006882   0.257210   0.027   0.9789  \nX71         -0.246016   0.316239  -0.778   0.4467  \nX72          0.320674   0.376736   0.851   0.4058  \nX73         -0.088619   0.248323  -0.357   0.7253  \nX74         -0.258652   0.245951  -1.052   0.3069  \nX75          0.304042   0.284942   1.067   0.3001  \nX76         -0.414025   0.290564  -1.425   0.1713  \nX77         -0.087306   0.272738  -0.320   0.7526  \nX78         -0.072697   0.250050  -0.291   0.7746  \nX79         -0.138113   0.280764  -0.492   0.6287  \nX80         -0.439736   0.259721  -1.693   0.1077  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3149 on 18 degrees of freedom\nMultiple R-squared:  0.7946,    Adjusted R-squared:  -0.1299 \nF-statistic: 0.8595 on 81 and 18 DF,  p-value: 0.6891\n\n\nThe effect estimates are relatively unchanged, but the CI has increased, and the p-values are n.s."
  },
  {
    "objectID": "3C-ModelSelection.html#model-selection-methods",
    "href": "3C-ModelSelection.html#model-selection-methods",
    "title": "7  Model selection",
    "section": "7.2 Model Selection Methods",
    "text": "7.2 Model Selection Methods\nBecause of the bias-variance trade-off, we cannot just fit the most complex model that we can imagine. Of course, such a model would have the lowest possible bias, but we would loose all power to see effects. Therefore, we must put bounds on model complexity. There are many methods to do so, but I would argue that three of them stand out\n\nTest whether there is evidence for going to a more complex model -> Likelihood Ratio Tests\nOptimize a penalized fit to the data -> AIC selection\nSpecify a preference for parameter estimates to be small -> shrinkage estimation\n\nLet’s go through them one by one\n\n7.2.1 Likelihood-ratio tests\nA likelihood-ratio test (LRT) is a hypothesis test that can be used to compare 2 nested models. Nested means that the simpler of the 2 models is included in the more complex model.\nThe more complex model will always fit the data better, i.e. have a higher likelihood. This is the reason why you shouldn’t use fit or residual patterns for model selection. The likelihood-ratio test tests whether this improvement in likelihood is significantly larger than one would expect if the simpler model is the correct model.\nLikelihood-ratio tests are used to get the p-values in an R ANOVA, and thus you can also use the anova function to perform an likelihood-ratio test between 2 models (Note: For simple models, this will run an F-test, which is technically not exactly a likelihood-ratio test, but the principle is the same):\n\n# Model 1\nm1 = lm(Ozone ~ Wind , data = airquality)\n\n# Model 2\nm2 = lm(Ozone ~ Wind + Temp, data = airquality)\n\n# LRT\nanova(m1, m2)\n\nAnalysis of Variance Table\n\nModel 1: Ozone ~ Wind\nModel 2: Ozone ~ Wind + Temp\n  Res.Df   RSS Df Sum of Sq      F    Pr(>F)    \n1    114 79859                                  \n2    113 53973  1     25886 54.196 3.149e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n7.2.2 AIC model selection\nAnother method for model selection, and probably the most widely used, also because it does not require that models are nested, is the AIC = Akaike Information Criterion.\nThe AIC is defined as \\(2 \\ln(\\text{likelihood}) + 2k\\), where \\(k\\) = number of parameters.\nEssentially, this means AIC = Fit - Penalty for complexity.\nLower AIC is better!\n\nm1 = lm(Ozone ~ Temp, data = airquality)\nm2 = lm(Ozone ~ Temp + Wind, data = airquality)\n\nAIC(m1)\n\n[1] 1067.706\n\nAIC(m2)\n\n[1] 1049.741\n\n\nNote 1: It can be shown that AIC is asymptotically identical to leave-one-out cross-validation, so what AIC is optimizing is essentially the predictive error of the model on new data.\nNote 2: There are other information criteria, such as BIC, DIC, WAIC etc., as well as sample-size corrected versions of either of them (e.g. AICc). The difference between the methods is beyond the scope of this course. For the most common one (BIC), just the note that this penalizes more strongly for large data sets, and thus corrects a tendency of AIC to overfit for large data sets.\n\n\n\n\n\n\nExercise\n\n\n\nCompare results of AIC with likelihood-ratio tests. Discuss: When to use one or the other?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\n\n7.2.3 Shrinkage estimation\nA third option option for model selection are shrinkage estimators. These include the LASSO and ridge, but also random-effects could be seen as shrinkage estimators.\nThe basic idea behind these estimators is not to reduce the number of parameters, but to reduce the flexibility of the model by introducing a penalty on the regression coefficients that code a preference for smaller or zero coefficient values. Effectively, this can either amount to model selection (because some coefficients are shrunk directly to zero), or it can mean that we can fit very large models while still being able to do good predictions, or avoid overfitting.\nTo put a ridge penalty on the standard lm, we can use\n\nlibrary(MASS)\nlm.ridge(Ozone ~ Wind + Temp + Solar.R, data = airquality, lambda = 2)\n\n                     Wind         Temp      Solar.R \n-62.73376169  -3.30622990   1.62842247   0.05961015 \n\n\nWe can see how the regression estimates vary for different penalties via\n\nplot( lm.ridge( Ozone ~ Wind + Temp + Solar.R, data = airquality,\n              lambda = seq(0, 200, 0.1) ) )\n\n\n\n\n\n\n\n\n\n\n(Adaptive) Shrinking with glmmTMB\n\n\n\nWe can use glmmTMB for shrinkage estimation by changing the slopes into random slopes with only one level in a dummy grouping variable:\n\nConstant shrinkage\n\n\nlibrary(glmmTMB)\n\nWarning in checkMatrixPackageVersion(): Package version inconsistency detected.\nTMB was built with Matrix version 1.4.1\nCurrent Matrix version is 1.5.4.1\nPlease re-install 'TMB' from source using install.packages('TMB', type = 'source') or ask CRAN for a binary version of 'TMB' matching CRAN's 'Matrix' package\n\n\nWarning in checkDepPackageVersion(dep_pkg = \"TMB\"): Package version inconsistency detected.\nglmmTMB was built with TMB version 1.9.3\nCurrent TMB version is 1.9.1\nPlease re-install glmmTMB from source or restore original 'TMB' package (see '?reinstalling' for more information)\n\nairquality$group = as.factor(rep(1, nrow(airquality)))\nridgeGlmmTMB1 = glmmTMB(Ozone~(0+ Wind + Temp + Solar.R || group ), \n                       data = airquality,\n                       start = list(theta = rep(1e-5, 3)),\n                       map = list(theta = factor(c(NA, NA, NA))))\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\nridgeGlmmTMB10 = glmmTMB(Ozone~(0+ Wind + Temp + Solar.R || group ), \n                       data = airquality,\n                       start = list(theta = rep(10, 3)),\n                       map = list(theta = factor(c(NA, NA, NA))))\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\nsummary(ridgeGlmmTMB1)\n\n Family: gaussian  ( identity )\nFormula:          Ozone ~ (0 + Wind + Temp + Solar.R || group)\nData: airquality\n\n     AIC      BIC   logLik deviance df.resid \n  1020.3   1025.8   -508.2   1016.3      109 \n\nRandom effects:\n\nConditional model:\n Groups   Name    Variance Std.Dev. Corr      \n group    Wind      1.0     1.00              \n          Temp      1.0     1.00    0.00      \n          Solar.R   1.0     1.00    0.00 0.00 \n Residual         452.3    21.27              \nNumber of obs: 111, groups:  group, 1\n\nDispersion estimate for gaussian family (sigma^2):  452 \n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -80.27      20.90  -3.841 0.000122 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(ridgeGlmmTMB10)\n\n Family: gaussian  ( identity )\nFormula:          Ozone ~ (0 + Wind + Temp + Solar.R || group)\nData: airquality\n\n     AIC      BIC   logLik deviance df.resid \n  1069.1   1074.6   -532.6   1065.1      109 \n\nRandom effects:\n\nConditional model:\n Groups   Name    Variance  Std.Dev. Corr      \n group    Wind    4.852e+08 22026.47           \n          Temp    4.852e+08 22026.47 0.00      \n          Solar.R 4.852e+08 22026.47 0.00 0.00 \n Residual         4.445e+02    21.08           \nNumber of obs: 111, groups:  group, 1\n\nDispersion estimate for gaussian family (sigma^2):  444 \n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)   \n(Intercept)   -64.34      22.95  -2.804  0.00505 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote: The (…||…) syntax will model the individual random slopes individually (the default is to fit a covariance matrix between the random effects).\nWe can use the map and the start arguments to a) mark the variances of the random slopes as constant and b) to set them so specific values (smaller values mean higher regularization).\nRandom effects are now our slopes:\n\nranef(ridgeGlmmTMB1)[[1]]$g\n\n       Wind     Temp    Solar.R\n1 -2.426746 1.736524 0.06169567\n\nranef(ridgeGlmmTMB10)[[1]]$g\n\n       Wind     Temp    Solar.R\n1 -3.333591 1.652093 0.05982059\n\n\nFor higher variances they are stronger biased to 0.\n\nAdaptive shrinkage\n\nThe next obvious step would be to let the model and the data decide on the variance estimate (the amount of shrinkage), which is called adaptive shrinkage.\nSmall simulation example:\n\nn = 25\np = 10\nX = matrix(runif(p*n), n, p)\nhead(X)\n\n           [,1]       [,2]      [,3]      [,4]      [,5]       [,6]      [,7]\n[1,] 0.02270001 0.07236709 0.4309256 0.4636517 0.1365052 0.69035166 0.9048984\n[2,] 0.51323953 0.46648525 0.3968551 0.5964720 0.1771364 0.03204482 0.1991984\n[3,] 0.63072615 0.33990565 0.6969568 0.9060510 0.5195605 0.92048915 0.6809630\n[4,] 0.41877162 0.68991861 0.6593197 0.1730012 0.8111208 0.47846889 0.1375178\n[5,] 0.87926595 0.51415737 0.4073507 0.7858811 0.1153620 0.26652058 0.1069947\n[6,] 0.10798707 0.51492302 0.3069202 0.2329344 0.8934218 0.85651072 0.0928594\n           [,8]      [,9]     [,10]\n[1,] 0.63402809 0.4981566 0.6975534\n[2,] 0.01729183 0.2827642 0.2641079\n[3,] 0.02673547 0.7764451 0.4264649\n[4,] 0.60784060 0.3038528 0.7213336\n[5,] 0.57054135 0.5155512 0.4744717\n[6,] 0.24585335 0.4779508 0.3385870\n\nY = 4*X[,1] + rnorm(n)\n\n10 observations, 20 predictors, and only X1 has an effect on Y, let’s start with a fixed-effect model:\n\nm1 = glmmTMB(Y~X)\n\nWarning in glmmTMB(Y ~ X): use of the 'data' argument is recommended\n\nsummary(m1)\n\n Family: gaussian  ( identity )\nFormula:          Y ~ X\n\n     AIC      BIC   logLik deviance df.resid \n    74.8     89.4    -25.4     50.8       13 \n\n\nDispersion estimate for gaussian family (sigma^2): 0.446 \n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.37729    1.06509   1.293  0.19597    \nX1           3.08829    0.54398   5.677 1.37e-08 ***\nX2          -0.02100    0.53881  -0.039  0.96891    \nX3          -0.55400    0.53375  -1.038  0.29930    \nX4           0.24001    0.61060   0.393  0.69426    \nX5           0.27346    0.53414   0.512  0.60868    \nX6           0.12971    0.56845   0.228  0.81951    \nX7           0.01086    0.45582   0.024  0.98099    \nX8          -1.61792    0.52323  -3.092  0.00199 ** \nX9          -0.60016    0.63089  -0.951  0.34146    \nX10          0.03258    0.73756   0.044  0.96476    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNot surprising, we get many warnings about convergence issues and NAs because p == n. Also the effects are not even close to zero.\nLet’s try adaptive shrinkage:\n\ndata = data.frame(Y = Y, X, group = as.factor(rep(1, n)))\nform = paste0(\"Y~ 0+ (0+\",paste0(paste0(  \"X\", 1:p ),  collapse = \"+\"),\"||group)\" )\nprint(form)\n\n[1] \"Y~ 0+ (0+X1+X2+X3+X4+X5+X6+X7+X8+X9+X10||group)\"\n\nm2 = glmmTMB(as.formula(form), data = data)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\nranef(m2)\n\n$group\n        X1           X2           X3           X4           X5        X6\n1 3.709808 2.407132e-11 9.307941e-13 1.408818e-09 4.790187e-09 0.4367558\n            X7         X8          X9         X10\n1 6.769959e-12 -0.4960516 8.61791e-12 2.25132e-10\n\n\nWe still get a convergence issue, but this is related to the very small variance estimates of the other predictors (glmmTMB tries to calculate standard errors based on the hessian, however, the hessian has very small elements on its diagonal, which makes it impossible to calculate its inverse).\n\n\n\n\n7.2.4 P-hacking\nThe most dubious model selection strategy, actually considered scientific misconduct, is p-hacking. The purpose of this exercises is to show you how not to do model selection, i.e, that by playing around with the variables, you can make any outcome significant. That is why your hypothesis needs to be fixed before looking at the data, ideally through pre-registration, based on an experimental plan or a causal analysis. Here is the example:\n\n\n\n\n\n\np-hacking exercise\n\n\n\nWe have (simulated) measurements of plant performance. The goal of the analysis was to find out if Gen1 has an effect on Performance. Various other variables are measured. As you can see, the way I simulated the data, none of the variables has an effect on the response, so this is pure noise\n\nset.seed(1)\ndat = data.frame(matrix(rnorm(300), ncol = 10))\ncolnames(dat) = c(\"Performance\", \"Gen1\", \"Gen2\", \"soilC\", \"soilP\", \"Temp\",\n                  \"Humidity\", \"xPos\", \"yPos\", \"Water\")\nfullModel <- lm(Performance ~ ., data = dat)\n\nWhen you run summary(fullModel), you will see that there is no significant effect of of Gen1. Task for you: P-hack the analysis, i.e. make an effect appear, by trying around (systematically, e.g. with selecting with data, model selection, or by hand to find a model combination that has an effect). Popular strategies for p-hacking include changing the mdoel, but also sub-selecting particualr data. The group who finds the model with the highest significance for Gen1 wins!\n\n\n\n\n\n\n\n\nPossible solution\n\n\n\n\n\n\nsummary(lm(Performance ~ Gen1 * Humidity, data = dat[20:30,]))\n\n\nCall:\nlm(formula = Performance ~ Gen1 * Humidity, data = dat[20:30, \n    ])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.71665 -0.39627 -0.05915  0.28044  0.91257 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)   \n(Intercept)    -0.5248     0.2277  -2.304  0.05465 . \nGen1            0.8657     0.2276   3.804  0.00668 **\nHumidity        0.6738     0.2544   2.649  0.03298 * \nGen1:Humidity  -0.5480     0.1756  -3.122  0.01680 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6102 on 7 degrees of freedom\nMultiple R-squared:  0.7004,    Adjusted R-squared:  0.572 \nF-statistic: 5.454 on 3 and 7 DF,  p-value: 0.03\n\n\n\n\n\nHere some more inspiration on p-hacking:\n\nHack Your Way To Scientific Glory: https://projects.fivethirtyeight.com/p-hacking/\nFalse-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant: https://journals.sagepub.com/doi/full/10.1177/0956797611417632\nSixty seconds on … P-hacking: https://sci-hub.tw/https://www.bmj.com/content/362/bmj.k4039\n\nJohn Oliver about p-hacking:"
  },
  {
    "objectID": "3C-ModelSelection.html#problems-with-model-selection-for-inference",
    "href": "3C-ModelSelection.html#problems-with-model-selection-for-inference",
    "title": "7  Model selection",
    "section": "7.3 Problems with model selection for inference",
    "text": "7.3 Problems with model selection for inference\nModel selection works great to generate predictive models. The big problem with model selection is when we want interpret effect estimates. Here, we have two problems\n\nModel selection doesn’t respect causal relationships\nModel selection methods often lead to wrong p-values, CIs etc.\n\nLet’s look at them 1 by one:\n\n7.3.1 Causality\nIn general, model selection does NOT solve the problem of estimating causal effects. Quite the contrary: most model selection methods act against estimating causal effects. Consider the following example, where we create a response that causally depends on two collinear predictors, both with an effect size of 1\n\nset.seed(123)\nx1 = runif(100)\nx2 = 0.8 * x1 + 0.2 *runif(100)\ny = x1 + x2 + rnorm(100)\n\nGiven the structure of the data, we should run a multiple regression, and the multiple regression will get it roughly right: both effects are n.s., but estimates are roughly right and true values are in the 95% CI.\n\nm1 = lm(y ~ x1 + x2)\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ x1 + x2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8994 -0.6821 -0.1086  0.5749  3.3663 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -0.1408     0.2862  -0.492    0.624\nx1            1.2158     1.5037   0.809    0.421\nx2            0.8518     1.8674   0.456    0.649\n\nResidual standard error: 0.9765 on 97 degrees of freedom\nMultiple R-squared:  0.237, Adjusted R-squared:  0.2212 \nF-statistic: 15.06 on 2 and 97 DF,  p-value: 2.009e-06\n\n\nA model selection (more on the method later) will remove one of the variables and consequently overestimates the effect size (effect size too high, true causal value outside the 95% CI).\n\nm2 = MASS::stepAIC(m1)\n\n\nsummary(m2)\n\n\nCall:\nlm(formula = y ~ x1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9047 -0.6292 -0.1019  0.6077  3.3394 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.04633    0.19670  -0.236    0.814    \nx1           1.88350    0.34295   5.492 3.13e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9725 on 98 degrees of freedom\nMultiple R-squared:  0.2353,    Adjusted R-squared:  0.2275 \nF-statistic: 30.16 on 1 and 98 DF,  p-value: 3.134e-07\n\n\nIf you understand what AIC model selection is doing, you wouldn’t be concerned - this is actually not a bug, but rather, the method is doing exactly what it is designed for. However, it was not designed to estimate causal effects. Let’s do another simulation: here, x1,x2 and x3 all have the same effect on the response, but x1 and x2 are highly collinear, while x3 is independent\n\nset.seed(123)\nx1 = runif(100)\nx2 = 0.95 * x1 + 0.05 *runif(100)\nx3 = runif(100)\ny = x1 + x2 + x3 + rnorm(100)\nm1 = lm(y ~ x1)\nAIC(m1)\n\n[1] 286.2746\n\n\nComparing a base model with only x1 to x1 + x2\n\nm2 = lm(y ~ x1 + x2)\nanova(m1, m2)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x1\nModel 2: y ~ x1 + x2\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     98 96.548                           \n2     97 94.066  1    2.4817 2.5591 0.1129\n\nAIC(m2)\n\n[1] 285.6705\n\n\nThe same for the comparison of x1 to x1 + x3\n\nm3 = lm(y ~ x1 + x3)\nanova(m1, m3)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x1\nModel 2: y ~ x1 + x3\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n1     98 96.548                                \n2     97 86.848  1    9.6998 10.834 0.001391 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAIC(m3)\n\n[1] 277.6868\n\n\nWhat we see is that the model selection methods are far more enthusiastic about including x3. And with good reason: x3 is an independent predictor, so adding x3 improves the model a lot. The effect of x2, however, is mostly already included in x1, so adding x2 will only slightly improve the model. Thus, from the point of view of how good the data is explained, it doesn’t pay off to add x2. From the causal viewpoint, however, adding x3 is irrelevant, because it is not a confounder, while adding x2 is crucial (assuming the causal direction is such that it is a confounder).\n\n\n7.3.2 P-values\nThe second problem with model selection is the calibration of p-values. Let’s revisit our simulation example from the bias-variance trade-off, where we added 80 noisy predictors with no effect to a situation where we had one variable with an effect.\n\nset.seed(123)\nx = runif(100)\ny = 0.25 * x + rnorm(100, sd = 0.3)\nxNoise = matrix(runif(8000), ncol = 80)\ndat = data.frame(y=y,x=x, xNoise)\nfullModel = lm(y~., data = dat)\n\nWe saw before that the y~x effect is n.s. after adding all 80 predictors. Let’s use AIC in a stepwise model selection procedure to reduce model complexity\n\n\n\n\n\n\nTip\n\n\n\nSystematic LRT or AIC model selections are often used stepwise or global. Stepwise means that start with the most simple (forward) or the most comples (backward) model, and then run a chain of model selection steps (AIC or LRT) adding (forward) or removing (backward) complexity until we arrive at an optimum. Global means that we run immediately all possible models and compare their AIC. The main function for stepwise selection is MASS::StepAIC, for global selection MuMIn::dredge. There was a lot of discussion about step-wise vs. global selection in the literature, that mostly revolved around the fact that a stepwise selection is faster, but will not always find the global optimum. However, compared to the other problems discussed here (causality, p-values), I do not consider this a serious problem. Thus, if you can, run a global selection by all means, but if this is computationally prohibitive, stepwise selections are also fine.\n\n\n\nlibrary(MASS)\nreduced = stepAIC(fullModel)\n\n\n\n\n\n\n\nNote\n\n\n\nWhen you inspect the output of stepAIC, you can see that the function calculates at each step the AIC improvement for each predictor that could be removed, and then chooses to remove the predictor that leads to the strongest AIC improvement first.\n\n\nHere is the selected model\n\nsummary(reduced)\n\n\nCall:\nlm(formula = y ~ x + X2 + X3 + X4 + X7 + X8 + X9 + X10 + X11 + \n    X12 + X13 + X15 + X16 + X17 + X19 + X23 + X24 + X25 + X26 + \n    X28 + X32 + X34 + X36 + X37 + X38 + X40 + X41 + X42 + X44 + \n    X45 + X46 + X47 + X48 + X49 + X51 + X54 + X55 + X56 + X57 + \n    X58 + X60 + X61 + X65 + X66 + X67 + X69 + X71 + X72 + X74 + \n    X75 + X76 + X79 + X80, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.29437 -0.11119 -0.00176  0.10598  0.33330 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.66297    0.37833   1.752 0.086372 .  \nx            0.23719    0.10004   2.371 0.021990 *  \nX2          -0.16068    0.10439  -1.539 0.130590    \nX3          -0.11884    0.10389  -1.144 0.258568    \nX4          -0.13278    0.10011  -1.326 0.191278    \nX7           0.17556    0.11021   1.593 0.118026    \nX8           0.17163    0.12746   1.347 0.184710    \nX9          -0.21560    0.10822  -1.992 0.052301 .  \nX10          0.24336    0.09903   2.457 0.017823 *  \nX11         -0.14428    0.11623  -1.241 0.220757    \nX12         -0.16108    0.10125  -1.591 0.118472    \nX13         -0.19011    0.14470  -1.314 0.195407    \nX15         -0.13111    0.10703  -1.225 0.226796    \nX16         -0.47202    0.10938  -4.315 8.37e-05 ***\nX17          0.25903    0.11279   2.297 0.026243 *  \nX19         -0.15402    0.10664  -1.444 0.155427    \nX23          0.17998    0.11368   1.583 0.120224    \nX24          0.23410    0.10586   2.212 0.032009 *  \nX25         -0.23611    0.11903  -1.984 0.053285 .  \nX26          0.29341    0.10141   2.893 0.005811 ** \nX28          0.22012    0.11068   1.989 0.052687 .  \nX32          0.10311    0.10640   0.969 0.337543    \nX34         -0.21144    0.12567  -1.682 0.099252 .  \nX36          0.15986    0.10697   1.494 0.141896    \nX37          0.24533    0.10131   2.422 0.019452 *  \nX38          0.29514    0.10844   2.722 0.009144 ** \nX40          0.15977    0.11203   1.426 0.160575    \nX41          0.30199    0.11911   2.535 0.014695 *  \nX42         -0.18966    0.11599  -1.635 0.108864    \nX44          0.18595    0.10397   1.788 0.080285 .  \nX45          0.20115    0.09857   2.041 0.047051 *  \nX46         -0.32878    0.12243  -2.685 0.010040 *  \nX47          0.17192    0.10575   1.626 0.110837    \nX48         -0.36013    0.10129  -3.556 0.000886 ***\nX49         -0.13339    0.11330  -1.177 0.245137    \nX51         -0.15802    0.10369  -1.524 0.134378    \nX54         -0.17854    0.11057  -1.615 0.113206    \nX55         -0.23024    0.11682  -1.971 0.054766 .  \nX56         -0.25930    0.10889  -2.381 0.021447 *  \nX57         -0.24371    0.10668  -2.284 0.027009 *  \nX58          0.31375    0.10379   3.023 0.004084 ** \nX60          0.28972    0.10872   2.665 0.010586 *  \nX61          0.14802    0.10430   1.419 0.162616    \nX65         -0.47949    0.11591  -4.137 0.000148 ***\nX66          0.11926    0.10296   1.158 0.252723    \nX67         -0.10825    0.09985  -1.084 0.283944    \nX69          0.29208    0.10506   2.780 0.007850 ** \nX71         -0.30513    0.11891  -2.566 0.013607 *  \nX72          0.17224    0.11238   1.533 0.132204    \nX74         -0.23912    0.11049  -2.164 0.035683 *  \nX75          0.25758    0.11616   2.217 0.031570 *  \nX76         -0.39372    0.11185  -3.520 0.000985 ***\nX79         -0.14147    0.11483  -1.232 0.224221    \nX80         -0.37070    0.11865  -3.124 0.003082 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2132 on 46 degrees of freedom\nMultiple R-squared:  0.7594,    Adjusted R-squared:  0.4821 \nF-statistic: 2.739 on 53 and 46 DF,  p-value: 0.0003334\n\n\nThe result ist good and bad. Good is that we now get the effect of y ~ x significant. Bad is that a lot of the other noisy variables are also significant, and the rate at which this occurs is much higher than we should expect (22/80 random predictors significant is much higher than the expected type I error rate).\nThe phenomenon is well-known in the stats literature, and the reason is that performing a stepwise / global selection + calculating a regression table for the selected model is hidden multiple testing and has inflated Type I error rates! Remember, you are implicitly trying out hundreds or thousand of models, and are taking the one that is showing the strongest effects.\nThere are methods to correct for the problem (keyword: post-selection inference), but none of them are readily available in R, and also, mostly those corrected p-values have lower power than the full model."
  },
  {
    "objectID": "3C-ModelSelection.html#non-parametric-r2---cross-validation",
    "href": "3C-ModelSelection.html#non-parametric-r2---cross-validation",
    "title": "7  Model selection",
    "section": "7.4 Non-parametric R2 - cross-validation",
    "text": "7.4 Non-parametric R2 - cross-validation\nCross-validation is the non-parametric alternative to AIC. Note that AIC is asymptotically equal to leave-one-out cross-validation.\nFor most advanced models, you will have to program the cross-validation by hand, but here an example for glm, using the cv.glm function:\n\nlibrary(boot)\n\n# Leave-one-out and 6-fold cross-validation prediction error for the mammals data set.\ndata(mammals, package=\"MASS\")\nmammals.glm = glm(log(brain) ~ log(body), data = mammals)\ncv.err = cv.glm(mammals, mammals.glm, K = 5)$delta\n\n\n# As this is a linear model we could calculate the leave-one-out \n# cross-validation estimate without any extra model-fitting.\nmuhat = fitted(mammals.glm)\nmammals.diag = glm.diag(mammals.glm)\n(cv.err = mean((mammals.glm$y - muhat)^2/(1 - mammals.diag$h)^2))\n\n[1] 0.491865\n\n# Leave-one-out and 11-fold cross-validation prediction error for \n# the nodal data set.  Since the response is a binary variable an\n# appropriate cost function is\ncost = function(r, pi = 0){ mean(abs(r - pi) > 0.5) }\n\nnodal.glm = glm(r ~ stage+xray+acid, binomial, data = nodal)\n(cv.err = cv.glm(nodal, nodal.glm, cost, K = nrow(nodal))$delta)\n\n[1] 0.1886792 0.1886792\n\n(cv.11.err = cv.glm(nodal, nodal.glm, cost, K = 11)$delta)\n\n[1] 0.2641509 0.2570310\n\n\nNote that cross-validation requires independence of data points. For non-independent data, it is possible to block the cross-validation, see Roberts, David R., et al. “Cross‐validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure.” Ecography 40.8 (2017): 913-929., methods implemented in package blockCV, see https://cran.r-project.org/web/packages/blockCV/vignettes/BlockCV_for_SDM.html."
  },
  {
    "objectID": "3C-ModelSelection.html#case-studies",
    "href": "3C-ModelSelection.html#case-studies",
    "title": "7  Model selection",
    "section": "7.5 Case studies",
    "text": "7.5 Case studies\n\n7.5.1 Exercise: Global Plant Trait Analysis #3\n\n\n\n\n\n\nExcercise\n\n\n\nRevisit exercises our previous analyses of the dataset plantHeight, and discuss / analyze:\nWhich would be the appropriate model, if we want to get a predictive model for plant height, based on all the variables in the data set? Note: some text-based variables may need to be included, so probably it’s the easiest if you start with a large model that you specify by hand. You can also include interactions. The syntax:\n\nfit <- lm((x1 + x2 + x3)^2)\n\nincludes all possible 2nd-order interactions between the variables in your model. You can extend this to x3, x4 but I would not recommend it, your model will get too large.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(EcoData)\nlibrary(MASS)\nhead(plantHeight)\n##   sort_number site         Genus_species        Family growthform height\n## 1        1402  193    Acer_macryophyllum   Sapindaceae       Tree   28.0\n## 2       25246  103    Quararibea_cordata     Malvaceae       Tree   26.6\n## 3       11648   54    Eragrostis_dielsii       Poaceae       Herb    0.3\n## 4        8168  144    Cistus_salvifolius     Cistaceae      Shrub    1.6\n## 5       22422  178          Phlox_bifida Polemoniaceae       Herb    0.2\n## 6       15925   59 Homalium_betulifolium    Salicaceae      Shrub    1.7\n##        loght       Country              Site    lat     long entered.by alt\n## 1  1.4471580           USA    Oregon - McDun 44.600 -123.334     Angela 179\n## 2  1.4248816          Peru              Manu 12.183  -70.550     Angela 386\n## 3 -0.5228787     Australia Central Australia 23.800  133.833   Michelle 553\n## 4  0.2041200        Israel           Hanadiv 32.555   34.938     Angela 115\n## 5 -0.6989700           USA     Indiana Dunes 41.617  -86.950   Michelle 200\n## 6  0.2304489 New Caledonia              <NA> 21.500  165.500      Laura  95\n##   temp diurn.temp isotherm temp.seas temp.max.warm temp.min.cold temp.ann.range\n## 1 10.8       11.8      4.4       5.2          27.0           0.3           26.7\n## 2 24.5       10.8      7.4       0.9          31.2          16.7           14.5\n## 3 20.9       16.3      4.8       6.0          37.0           3.6           33.4\n## 4 19.9        9.7      4.4       4.9          30.7           8.7           22.0\n## 5  9.7       10.7      2.8       9.7          28.6          -9.5           38.1\n## 6 22.6        7.4      5.4       2.2          29.0          15.5           13.5\n##   temp.mean.wetqr temp.mean.dryqr temp.mean.warmqr temp.mean.coldqr rain\n## 1             4.9            17.4             17.6              4.5 1208\n## 2            25.1            23.2             25.3             23.1 3015\n## 3            28.1            14.8             28.1             12.8  278\n## 4            13.6            25.3             25.7             13.6  598\n## 5            21.6            -3.3             21.6             -3.3  976\n## 6            25.4            20.4             25.4             19.7 1387\n##   rain.wetm rain.drym rain.seas rain.wetqr rain.dryqr rain.warmqr rain.coldqr\n## 1       217        13        69        601         68          75         560\n## 2       416        99        45       1177        340         928         359\n## 3        37         9        42        109         35         109          42\n## 4       159         0       115        408          0           2         408\n## 5       104        44        23        299        165         299         165\n## 6       216        59        46        600        186         600         212\n##    LAI  NPP hemisphere\n## 1 2.51  572          1\n## 2 4.26 1405         -1\n## 3 1.32  756         -1\n## 4 1.01  359          1\n## 5 3.26 1131          1\n## 6 6.99 1552         -1\nfullModel <- lm(loght ~ (growthform + Family + lat + long + alt + temp + NPP )^2, data = plantHeight)\nselectedModel = stepAIC(fullModel, trace = 0)\nsummary(selectedModel)\n## \n## Call:\n## lm(formula = loght ~ growthform + Family + lat + long + alt + \n##     temp + NPP + growthform:Family + growthform:lat + growthform:long + \n##     growthform:NPP + Family:lat + Family:long + Family:alt + \n##     Family:temp + lat:long + lat:temp + lat:NPP + long:alt + \n##     long:temp + long:NPP + alt:temp + temp:NPP, data = plantHeight)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.1897  0.0000  0.0000  0.0000  0.1998 \n## \n## Coefficients: (521 not defined because of singularities)\n##                                                 Estimate Std. Error t value\n## (Intercept)                                   -1.612e+01  2.152e+01  -0.749\n## growthformHerb                                 1.598e+02  1.822e+02   0.877\n## growthformHerb/Shrub                          -2.013e-01  3.854e+00  -0.052\n## growthformShrub                               -1.992e+02  2.590e+02  -0.769\n## growthformShrub/Tree                          -1.126e+02  1.952e+02  -0.577\n## growthformTree                                 2.053e+01  1.530e+01   1.342\n## FamilyAsteraceae                              -3.562e+02  1.352e+02  -2.636\n## FamilyAtherospermataceae                      -2.866e+01  2.295e+01  -1.248\n## FamilyBalsaminaceae                            1.530e+02  1.927e+02   0.794\n## FamilyBetulaceae                              -3.010e+01  1.056e+02  -0.285\n## FamilyBrassicaceae                            -1.271e+02  1.910e+02  -0.665\n## FamilyCactaceae                                2.620e+02  2.820e+02   0.929\n## FamilyCasuarinaceae                            1.995e+02  2.582e+02   0.773\n## FamilyChloranthaceae                           7.419e-01  9.460e-01   0.784\n## FamilyChrysobalanaceae                         5.392e+00  8.314e+00   0.649\n## FamilyCistaceae                                1.999e+02  2.586e+02   0.773\n## FamilyCornaceae                                3.361e+02  4.220e+02   0.796\n## FamilyCrassulaceae                             1.954e+02  2.551e+02   0.766\n## FamilyCunoniaceae                              1.716e-01  3.008e-01   0.571\n## FamilyCupressaceae                            -3.435e+01  3.185e+01  -1.079\n## FamilyCyperaceae                              -1.814e+01  2.054e+01  -0.883\n## FamilyDennstaedtiaceae                                NA         NA      NA\n## FamilyDicksoniaceae                            1.878e+02  2.490e+02   0.754\n## FamilyDipterocarpaceae                         4.334e+00  4.919e+00   0.881\n## FamilyEbenaceae                                5.281e+00  5.975e+00   0.884\n## FamilyElaeocarpaceae                           5.440e-01  5.242e-01   1.038\n## FamilyEricaceae                                2.186e+02  2.792e+02   0.783\n## FamilyEuphorbiaceae                            3.472e+00  4.301e+00   0.807\n## FamilyFabaceae - C                             2.644e+00  3.387e+00   0.781\n## FamilyFabaceae - M                            -2.768e+00  2.238e+00  -1.237\n## FamilyFabaceae - P                             5.128e+01  5.128e+01   1.000\n## FamilyFagaceae                                -2.036e+00  7.802e+00  -0.261\n## FamilyGentianaceae                            -1.963e+02  2.264e+02  -0.867\n## FamilyHeliconiaceae                           -1.193e+02  1.413e+02  -0.845\n## FamilyJuglandaceae                            -2.223e+01  2.069e+01  -1.074\n## FamilyJuncaginaceae                            2.855e+02  3.596e+02   0.794\n## FamilyLamiaceae                                1.876e+02  2.522e+02   0.744\n## FamilyLauraceae                               -2.595e+00  4.184e+00  -0.620\n## FamilyMaesaceae                                2.097e+02  2.652e+02   0.791\n## FamilyMalvaceae                                3.153e+00  4.933e+00   0.639\n## FamilyMelastomataceae                          2.135e+02  2.701e+02   0.790\n## FamilyMoraceae                                -5.805e+00  6.329e+00  -0.917\n## FamilyMyristicaceae                            8.819e-01  8.987e-01   0.981\n## FamilyMyrsinaceae                             -9.893e+00  1.109e+01  -0.892\n## FamilyMyrtaceae                               -3.193e+01  1.040e+01  -3.070\n## FamilyOchnaceae                                2.125e+02  2.692e+02   0.789\n## FamilyOnagraceae                                      NA         NA      NA\n## FamilyOrobanchaceae                           -1.075e+02  1.752e+02  -0.613\n## FamilyPhyllanthaceae                          -1.211e+01  1.357e+01  -0.892\n## FamilyPicrodendraceae                         -1.256e+01  1.307e+01  -0.961\n## FamilyPinaceae                                 1.522e+01  8.029e+00   1.895\n## FamilyPoaceae                                  1.059e+02  1.625e+02   0.652\n## FamilyPolemoniaceae                           -1.369e+02  1.613e+02  -0.849\n## FamilyPolygonaceae                            -1.292e+02  1.622e+02  -0.796\n## FamilyProteaceae                               1.685e+01  9.511e+00   1.772\n## FamilyRanunculaceae                            1.136e+02  1.438e+02   0.790\n## FamilyRhamnaceae                               1.944e+02  2.570e+02   0.756\n## FamilyRosaceae                                 1.148e+01  3.983e+00   2.883\n## FamilyRubiaceae                                2.165e+02  2.702e+02   0.801\n## FamilyRutaceae                                 1.979e+02  2.555e+02   0.774\n## FamilySalicaceae                               2.326e+02  2.849e+02   0.816\n## FamilySapindaceae                              4.753e+00  8.974e+00   0.530\n## FamilySapotaceae                              -1.511e+01  1.313e+01  -1.151\n## FamilyScrophulariaceae                         2.099e+02  2.660e+02   0.789\n## FamilyThymelaeaceae                           -9.324e+00  9.227e+00  -1.010\n## FamilyUlmaceae                                 1.783e+01  1.401e+01   1.273\n## FamilyUrticaceae                               8.939e+00  8.901e+00   1.004\n## FamilyViolaceae                                2.145e+02  2.712e+02   0.791\n## FamilyXanthorrhoeaceae                         2.016e+02  2.596e+02   0.777\n## FamilyZygophyllaceae                           1.871e+02  2.474e+02   0.756\n## lat                                            6.678e-01  6.451e-01   1.035\n## long                                           6.846e-03  3.808e-02   0.180\n## alt                                            5.440e-03  8.816e-03   0.617\n## temp                                          -1.851e-01  2.343e-01  -0.790\n## NPP                                            2.978e-03  2.716e-03   1.097\n## growthformHerb:FamilyAsteraceae               -1.535e+01  4.924e+01  -0.312\n## growthformHerb/Shrub:FamilyAsteraceae                 NA         NA      NA\n## growthformShrub:FamilyAsteraceae                      NA         NA      NA\n## growthformShrub/Tree:FamilyAsteraceae                 NA         NA      NA\n## growthformTree:FamilyAsteraceae                       NA         NA      NA\n## growthformHerb:FamilyAtherospermataceae               NA         NA      NA\n## growthformHerb/Shrub:FamilyAtherospermataceae         NA         NA      NA\n## growthformShrub:FamilyAtherospermataceae              NA         NA      NA\n## growthformShrub/Tree:FamilyAtherospermataceae         NA         NA      NA\n## growthformTree:FamilyAtherospermataceae               NA         NA      NA\n## growthformHerb:FamilyBalsaminaceae                    NA         NA      NA\n## growthformHerb/Shrub:FamilyBalsaminaceae              NA         NA      NA\n## growthformShrub:FamilyBalsaminaceae                   NA         NA      NA\n## growthformShrub/Tree:FamilyBalsaminaceae              NA         NA      NA\n## growthformTree:FamilyBalsaminaceae                    NA         NA      NA\n## growthformHerb:FamilyBetulaceae                       NA         NA      NA\n## growthformHerb/Shrub:FamilyBetulaceae                 NA         NA      NA\n## growthformShrub:FamilyBetulaceae               2.035e+02  2.795e+02   0.728\n## growthformShrub/Tree:FamilyBetulaceae                 NA         NA      NA\n## growthformTree:FamilyBetulaceae                       NA         NA      NA\n## growthformHerb:FamilyBrassicaceae                     NA         NA      NA\n## growthformHerb/Shrub:FamilyBrassicaceae               NA         NA      NA\n## growthformShrub:FamilyBrassicaceae                    NA         NA      NA\n## growthformShrub/Tree:FamilyBrassicaceae               NA         NA      NA\n## growthformTree:FamilyBrassicaceae                     NA         NA      NA\n## growthformHerb:FamilyCactaceae                        NA         NA      NA\n## growthformHerb/Shrub:FamilyCactaceae                  NA         NA      NA\n## growthformShrub:FamilyCactaceae                       NA         NA      NA\n## growthformShrub/Tree:FamilyCactaceae                  NA         NA      NA\n## growthformTree:FamilyCactaceae                        NA         NA      NA\n## growthformHerb:FamilyCasuarinaceae                    NA         NA      NA\n## growthformHerb/Shrub:FamilyCasuarinaceae              NA         NA      NA\n## growthformShrub:FamilyCasuarinaceae                   NA         NA      NA\n## growthformShrub/Tree:FamilyCasuarinaceae              NA         NA      NA\n## growthformTree:FamilyCasuarinaceae                    NA         NA      NA\n## growthformHerb:FamilyChloranthaceae                   NA         NA      NA\n## growthformHerb/Shrub:FamilyChloranthaceae             NA         NA      NA\n## growthformShrub:FamilyChloranthaceae                  NA         NA      NA\n## growthformShrub/Tree:FamilyChloranthaceae             NA         NA      NA\n## growthformTree:FamilyChloranthaceae                   NA         NA      NA\n## growthformHerb:FamilyChrysobalanaceae                 NA         NA      NA\n## growthformHerb/Shrub:FamilyChrysobalanaceae           NA         NA      NA\n## growthformShrub:FamilyChrysobalanaceae                NA         NA      NA\n## growthformShrub/Tree:FamilyChrysobalanaceae           NA         NA      NA\n## growthformTree:FamilyChrysobalanaceae                 NA         NA      NA\n## growthformHerb:FamilyCistaceae                        NA         NA      NA\n## growthformHerb/Shrub:FamilyCistaceae                  NA         NA      NA\n## growthformShrub:FamilyCistaceae                       NA         NA      NA\n## growthformShrub/Tree:FamilyCistaceae                  NA         NA      NA\n## growthformTree:FamilyCistaceae                        NA         NA      NA\n## growthformHerb:FamilyCornaceae                        NA         NA      NA\n## growthformHerb/Shrub:FamilyCornaceae                  NA         NA      NA\n## growthformShrub:FamilyCornaceae                       NA         NA      NA\n## growthformShrub/Tree:FamilyCornaceae                  NA         NA      NA\n## growthformTree:FamilyCornaceae                        NA         NA      NA\n## growthformHerb:FamilyCrassulaceae                     NA         NA      NA\n## growthformHerb/Shrub:FamilyCrassulaceae               NA         NA      NA\n## growthformShrub:FamilyCrassulaceae                    NA         NA      NA\n## growthformShrub/Tree:FamilyCrassulaceae               NA         NA      NA\n## growthformTree:FamilyCrassulaceae                     NA         NA      NA\n## growthformHerb:FamilyCunoniaceae                      NA         NA      NA\n## growthformHerb/Shrub:FamilyCunoniaceae                NA         NA      NA\n## growthformShrub:FamilyCunoniaceae                     NA         NA      NA\n## growthformShrub/Tree:FamilyCunoniaceae         2.124e+02  2.694e+02   0.789\n## growthformTree:FamilyCunoniaceae                      NA         NA      NA\n## growthformHerb:FamilyCupressaceae                     NA         NA      NA\n## growthformHerb/Shrub:FamilyCupressaceae               NA         NA      NA\n## growthformShrub:FamilyCupressaceae             2.214e+02  2.802e+02   0.790\n## growthformShrub/Tree:FamilyCupressaceae               NA         NA      NA\n## growthformTree:FamilyCupressaceae                     NA         NA      NA\n## growthformHerb:FamilyCyperaceae                       NA         NA      NA\n## growthformHerb/Shrub:FamilyCyperaceae                 NA         NA      NA\n## growthformShrub:FamilyCyperaceae                      NA         NA      NA\n## growthformShrub/Tree:FamilyCyperaceae                 NA         NA      NA\n## growthformTree:FamilyCyperaceae                       NA         NA      NA\n## growthformHerb:FamilyDennstaedtiaceae                 NA         NA      NA\n## growthformHerb/Shrub:FamilyDennstaedtiaceae           NA         NA      NA\n## growthformShrub:FamilyDennstaedtiaceae                NA         NA      NA\n## growthformShrub/Tree:FamilyDennstaedtiaceae           NA         NA      NA\n## growthformTree:FamilyDennstaedtiaceae                 NA         NA      NA\n## growthformHerb:FamilyDicksoniaceae                    NA         NA      NA\n## growthformHerb/Shrub:FamilyDicksoniaceae              NA         NA      NA\n## growthformShrub:FamilyDicksoniaceae                   NA         NA      NA\n## growthformShrub/Tree:FamilyDicksoniaceae              NA         NA      NA\n## growthformTree:FamilyDicksoniaceae                    NA         NA      NA\n## growthformHerb:FamilyDipterocarpaceae                 NA         NA      NA\n## growthformHerb/Shrub:FamilyDipterocarpaceae           NA         NA      NA\n## growthformShrub:FamilyDipterocarpaceae                NA         NA      NA\n## growthformShrub/Tree:FamilyDipterocarpaceae           NA         NA      NA\n## growthformTree:FamilyDipterocarpaceae                 NA         NA      NA\n## growthformHerb:FamilyEbenaceae                        NA         NA      NA\n## growthformHerb/Shrub:FamilyEbenaceae                  NA         NA      NA\n## growthformShrub:FamilyEbenaceae                       NA         NA      NA\n## growthformShrub/Tree:FamilyEbenaceae                  NA         NA      NA\n## growthformTree:FamilyEbenaceae                        NA         NA      NA\n## growthformHerb:FamilyElaeocarpaceae                   NA         NA      NA\n## growthformHerb/Shrub:FamilyElaeocarpaceae             NA         NA      NA\n## growthformShrub:FamilyElaeocarpaceae                  NA         NA      NA\n## growthformShrub/Tree:FamilyElaeocarpaceae             NA         NA      NA\n## growthformTree:FamilyElaeocarpaceae                   NA         NA      NA\n## growthformHerb:FamilyEricaceae                        NA         NA      NA\n## growthformHerb/Shrub:FamilyEricaceae                  NA         NA      NA\n## growthformShrub:FamilyEricaceae               -1.611e+00  4.441e+00  -0.363\n## growthformShrub/Tree:FamilyEricaceae                  NA         NA      NA\n## growthformTree:FamilyEricaceae                        NA         NA      NA\n## growthformHerb:FamilyEuphorbiaceae                    NA         NA      NA\n## growthformHerb/Shrub:FamilyEuphorbiaceae              NA         NA      NA\n## growthformShrub:FamilyEuphorbiaceae            2.201e+02  2.766e+02   0.796\n## growthformShrub/Tree:FamilyEuphorbiaceae              NA         NA      NA\n## growthformTree:FamilyEuphorbiaceae                    NA         NA      NA\n## growthformHerb:FamilyFabaceae - C                     NA         NA      NA\n## growthformHerb/Shrub:FamilyFabaceae - C               NA         NA      NA\n## growthformShrub:FamilyFabaceae - C                    NA         NA      NA\n## growthformShrub/Tree:FamilyFabaceae - C               NA         NA      NA\n## growthformTree:FamilyFabaceae - C                     NA         NA      NA\n## growthformHerb:FamilyFabaceae - M                     NA         NA      NA\n## growthformHerb/Shrub:FamilyFabaceae - M               NA         NA      NA\n## growthformShrub:FamilyFabaceae - M                    NA         NA      NA\n## growthformShrub/Tree:FamilyFabaceae - M        2.338e+02  2.857e+02   0.818\n## growthformTree:FamilyFabaceae - M                     NA         NA      NA\n## growthformHerb:FamilyFabaceae - P              1.161e+02  1.357e+02   0.855\n## growthformHerb/Shrub:FamilyFabaceae - P               NA         NA      NA\n## growthformShrub:FamilyFabaceae - P             2.457e+02  3.020e+02   0.814\n## growthformShrub/Tree:FamilyFabaceae - P               NA         NA      NA\n## growthformTree:FamilyFabaceae - P                     NA         NA      NA\n## growthformHerb:FamilyFagaceae                         NA         NA      NA\n## growthformHerb/Shrub:FamilyFagaceae                   NA         NA      NA\n## growthformShrub:FamilyFagaceae                 2.144e+02  2.735e+02   0.784\n## growthformShrub/Tree:FamilyFagaceae                   NA         NA      NA\n## growthformTree:FamilyFagaceae                         NA         NA      NA\n## growthformHerb:FamilyGentianaceae                     NA         NA      NA\n## growthformHerb/Shrub:FamilyGentianaceae               NA         NA      NA\n## growthformShrub:FamilyGentianaceae                    NA         NA      NA\n## growthformShrub/Tree:FamilyGentianaceae               NA         NA      NA\n## growthformTree:FamilyGentianaceae                     NA         NA      NA\n## growthformHerb:FamilyHeliconiaceae                    NA         NA      NA\n## growthformHerb/Shrub:FamilyHeliconiaceae              NA         NA      NA\n## growthformShrub:FamilyHeliconiaceae                   NA         NA      NA\n## growthformShrub/Tree:FamilyHeliconiaceae              NA         NA      NA\n## growthformTree:FamilyHeliconiaceae                    NA         NA      NA\n## growthformHerb:FamilyJuglandaceae                     NA         NA      NA\n## growthformHerb/Shrub:FamilyJuglandaceae               NA         NA      NA\n## growthformShrub:FamilyJuglandaceae                    NA         NA      NA\n## growthformShrub/Tree:FamilyJuglandaceae               NA         NA      NA\n## growthformTree:FamilyJuglandaceae                     NA         NA      NA\n## growthformHerb:FamilyJuncaginaceae                    NA         NA      NA\n## growthformHerb/Shrub:FamilyJuncaginaceae              NA         NA      NA\n## growthformShrub:FamilyJuncaginaceae                   NA         NA      NA\n## growthformShrub/Tree:FamilyJuncaginaceae              NA         NA      NA\n## growthformTree:FamilyJuncaginaceae                    NA         NA      NA\n## growthformHerb:FamilyLamiaceae                        NA         NA      NA\n## growthformHerb/Shrub:FamilyLamiaceae                  NA         NA      NA\n## growthformShrub:FamilyLamiaceae                       NA         NA      NA\n## growthformShrub/Tree:FamilyLamiaceae                  NA         NA      NA\n## growthformTree:FamilyLamiaceae                        NA         NA      NA\n## growthformHerb:FamilyLauraceae                        NA         NA      NA\n## growthformHerb/Shrub:FamilyLauraceae                  NA         NA      NA\n## growthformShrub:FamilyLauraceae                       NA         NA      NA\n## growthformShrub/Tree:FamilyLauraceae                  NA         NA      NA\n## growthformTree:FamilyLauraceae                        NA         NA      NA\n## growthformHerb:FamilyMaesaceae                        NA         NA      NA\n## growthformHerb/Shrub:FamilyMaesaceae                  NA         NA      NA\n## growthformShrub:FamilyMaesaceae                       NA         NA      NA\n## growthformShrub/Tree:FamilyMaesaceae                  NA         NA      NA\n## growthformTree:FamilyMaesaceae                        NA         NA      NA\n## growthformHerb:FamilyMalvaceae                        NA         NA      NA\n## growthformHerb/Shrub:FamilyMalvaceae                  NA         NA      NA\n## growthformShrub:FamilyMalvaceae                       NA         NA      NA\n## growthformShrub/Tree:FamilyMalvaceae                  NA         NA      NA\n## growthformTree:FamilyMalvaceae                        NA         NA      NA\n## growthformHerb:FamilyMelastomataceae                  NA         NA      NA\n## growthformHerb/Shrub:FamilyMelastomataceae            NA         NA      NA\n## growthformShrub:FamilyMelastomataceae                 NA         NA      NA\n## growthformShrub/Tree:FamilyMelastomataceae            NA         NA      NA\n## growthformTree:FamilyMelastomataceae                  NA         NA      NA\n## growthformHerb:FamilyMoraceae                         NA         NA      NA\n## growthformHerb/Shrub:FamilyMoraceae                   NA         NA      NA\n## growthformShrub:FamilyMoraceae                        NA         NA      NA\n## growthformShrub/Tree:FamilyMoraceae                   NA         NA      NA\n## growthformTree:FamilyMoraceae                         NA         NA      NA\n## growthformHerb:FamilyMyristicaceae                    NA         NA      NA\n## growthformHerb/Shrub:FamilyMyristicaceae              NA         NA      NA\n## growthformShrub:FamilyMyristicaceae                   NA         NA      NA\n## growthformShrub/Tree:FamilyMyristicaceae              NA         NA      NA\n## growthformTree:FamilyMyristicaceae                    NA         NA      NA\n## growthformHerb:FamilyMyrsinaceae                      NA         NA      NA\n## growthformHerb/Shrub:FamilyMyrsinaceae                NA         NA      NA\n## growthformShrub:FamilyMyrsinaceae                     NA         NA      NA\n## growthformShrub/Tree:FamilyMyrsinaceae         2.252e+02  2.829e+02   0.796\n## growthformTree:FamilyMyrsinaceae                      NA         NA      NA\n## growthformHerb:FamilyMyrtaceae                        NA         NA      NA\n## growthformHerb/Shrub:FamilyMyrtaceae                  NA         NA      NA\n## growthformShrub:FamilyMyrtaceae                2.171e+02  2.710e+02   0.801\n## growthformShrub/Tree:FamilyMyrtaceae                  NA         NA      NA\n## growthformTree:FamilyMyrtaceae                        NA         NA      NA\n## growthformHerb:FamilyOchnaceae                        NA         NA      NA\n## growthformHerb/Shrub:FamilyOchnaceae                  NA         NA      NA\n## growthformShrub:FamilyOchnaceae                       NA         NA      NA\n## growthformShrub/Tree:FamilyOchnaceae                  NA         NA      NA\n## growthformTree:FamilyOchnaceae                        NA         NA      NA\n## growthformHerb:FamilyOnagraceae                       NA         NA      NA\n## growthformHerb/Shrub:FamilyOnagraceae                 NA         NA      NA\n## growthformShrub:FamilyOnagraceae                      NA         NA      NA\n## growthformShrub/Tree:FamilyOnagraceae                 NA         NA      NA\n## growthformTree:FamilyOnagraceae                       NA         NA      NA\n## growthformHerb:FamilyOrobanchaceae                    NA         NA      NA\n## growthformHerb/Shrub:FamilyOrobanchaceae              NA         NA      NA\n## growthformShrub:FamilyOrobanchaceae                   NA         NA      NA\n## growthformShrub/Tree:FamilyOrobanchaceae              NA         NA      NA\n## growthformTree:FamilyOrobanchaceae                    NA         NA      NA\n## growthformHerb:FamilyPhyllanthaceae                   NA         NA      NA\n## growthformHerb/Shrub:FamilyPhyllanthaceae             NA         NA      NA\n## growthformShrub:FamilyPhyllanthaceae                  NA         NA      NA\n## growthformShrub/Tree:FamilyPhyllanthaceae             NA         NA      NA\n## growthformTree:FamilyPhyllanthaceae                   NA         NA      NA\n## growthformHerb:FamilyPicrodendraceae                  NA         NA      NA\n## growthformHerb/Shrub:FamilyPicrodendraceae            NA         NA      NA\n## growthformShrub:FamilyPicrodendraceae                 NA         NA      NA\n## growthformShrub/Tree:FamilyPicrodendraceae            NA         NA      NA\n## growthformTree:FamilyPicrodendraceae                  NA         NA      NA\n## growthformHerb:FamilyPinaceae                         NA         NA      NA\n## growthformHerb/Shrub:FamilyPinaceae                   NA         NA      NA\n## growthformShrub:FamilyPinaceae                        NA         NA      NA\n## growthformShrub/Tree:FamilyPinaceae                   NA         NA      NA\n## growthformTree:FamilyPinaceae                         NA         NA      NA\n## growthformHerb:FamilyPoaceae                  -2.601e+02  3.246e+02  -0.801\n## growthformHerb/Shrub:FamilyPoaceae                    NA         NA      NA\n## growthformShrub:FamilyPoaceae                         NA         NA      NA\n## growthformShrub/Tree:FamilyPoaceae                    NA         NA      NA\n## growthformTree:FamilyPoaceae                          NA         NA      NA\n## growthformHerb:FamilyPolemoniaceae                    NA         NA      NA\n## growthformHerb/Shrub:FamilyPolemoniaceae              NA         NA      NA\n## growthformShrub:FamilyPolemoniaceae                   NA         NA      NA\n## growthformShrub/Tree:FamilyPolemoniaceae              NA         NA      NA\n## growthformTree:FamilyPolemoniaceae                    NA         NA      NA\n## growthformHerb:FamilyPolygonaceae                     NA         NA      NA\n## growthformHerb/Shrub:FamilyPolygonaceae               NA         NA      NA\n## growthformShrub:FamilyPolygonaceae                    NA         NA      NA\n## growthformShrub/Tree:FamilyPolygonaceae               NA         NA      NA\n## growthformTree:FamilyPolygonaceae                     NA         NA      NA\n## growthformHerb:FamilyProteaceae                       NA         NA      NA\n## growthformHerb/Shrub:FamilyProteaceae                 NA         NA      NA\n## growthformShrub:FamilyProteaceae               2.174e+02  2.714e+02   0.801\n## growthformShrub/Tree:FamilyProteaceae                 NA         NA      NA\n## growthformTree:FamilyProteaceae                       NA         NA      NA\n## growthformHerb:FamilyRanunculaceae                    NA         NA      NA\n## growthformHerb/Shrub:FamilyRanunculaceae              NA         NA      NA\n## growthformShrub:FamilyRanunculaceae                   NA         NA      NA\n## growthformShrub/Tree:FamilyRanunculaceae              NA         NA      NA\n## growthformTree:FamilyRanunculaceae                    NA         NA      NA\n## growthformHerb:FamilyRhamnaceae                       NA         NA      NA\n## growthformHerb/Shrub:FamilyRhamnaceae                 NA         NA      NA\n## growthformShrub:FamilyRhamnaceae                      NA         NA      NA\n## growthformShrub/Tree:FamilyRhamnaceae                 NA         NA      NA\n## growthformTree:FamilyRhamnaceae                       NA         NA      NA\n## growthformHerb:FamilyRosaceae                  3.723e+02  4.458e+02   0.835\n## growthformHerb/Shrub:FamilyRosaceae                   NA         NA      NA\n## growthformShrub:FamilyRosaceae                 2.126e+02  2.715e+02   0.783\n## growthformShrub/Tree:FamilyRosaceae                   NA         NA      NA\n## growthformTree:FamilyRosaceae                         NA         NA      NA\n## growthformHerb:FamilyRubiaceae                        NA         NA      NA\n## growthformHerb/Shrub:FamilyRubiaceae                  NA         NA      NA\n## growthformShrub:FamilyRubiaceae                6.243e+00  3.379e+00   1.848\n## growthformShrub/Tree:FamilyRubiaceae                  NA         NA      NA\n## growthformTree:FamilyRubiaceae                        NA         NA      NA\n## growthformHerb:FamilyRutaceae                         NA         NA      NA\n## growthformHerb/Shrub:FamilyRutaceae                   NA         NA      NA\n## growthformShrub:FamilyRutaceae                        NA         NA      NA\n## growthformShrub/Tree:FamilyRutaceae                   NA         NA      NA\n## growthformTree:FamilyRutaceae                         NA         NA      NA\n## growthformHerb:FamilySalicaceae                       NA         NA      NA\n## growthformHerb/Shrub:FamilySalicaceae                 NA         NA      NA\n## growthformShrub:FamilySalicaceae               9.252e+00  6.665e+00   1.388\n## growthformShrub/Tree:FamilySalicaceae                 NA         NA      NA\n## growthformTree:FamilySalicaceae                       NA         NA      NA\n## growthformHerb:FamilySapindaceae                      NA         NA      NA\n## growthformHerb/Shrub:FamilySapindaceae                NA         NA      NA\n## growthformShrub:FamilySapindaceae                     NA         NA      NA\n## growthformShrub/Tree:FamilySapindaceae                NA         NA      NA\n## growthformTree:FamilySapindaceae                      NA         NA      NA\n## growthformHerb:FamilySapotaceae                       NA         NA      NA\n## growthformHerb/Shrub:FamilySapotaceae                 NA         NA      NA\n## growthformShrub:FamilySapotaceae                      NA         NA      NA\n## growthformShrub/Tree:FamilySapotaceae                 NA         NA      NA\n## growthformTree:FamilySapotaceae                       NA         NA      NA\n## growthformHerb:FamilyScrophulariaceae                 NA         NA      NA\n## growthformHerb/Shrub:FamilyScrophulariaceae           NA         NA      NA\n## growthformShrub:FamilyScrophulariaceae                NA         NA      NA\n## growthformShrub/Tree:FamilyScrophulariaceae           NA         NA      NA\n## growthformTree:FamilyScrophulariaceae                 NA         NA      NA\n## growthformHerb:FamilyThymelaeaceae                    NA         NA      NA\n## growthformHerb/Shrub:FamilyThymelaeaceae              NA         NA      NA\n## growthformShrub:FamilyThymelaeaceae                   NA         NA      NA\n## growthformShrub/Tree:FamilyThymelaeaceae              NA         NA      NA\n## growthformTree:FamilyThymelaeaceae                    NA         NA      NA\n## growthformHerb:FamilyUlmaceae                         NA         NA      NA\n## growthformHerb/Shrub:FamilyUlmaceae                   NA         NA      NA\n## growthformShrub:FamilyUlmaceae                        NA         NA      NA\n## growthformShrub/Tree:FamilyUlmaceae                   NA         NA      NA\n## growthformTree:FamilyUlmaceae                         NA         NA      NA\n## growthformHerb:FamilyUrticaceae                       NA         NA      NA\n## growthformHerb/Shrub:FamilyUrticaceae                 NA         NA      NA\n## growthformShrub:FamilyUrticaceae                      NA         NA      NA\n## growthformShrub/Tree:FamilyUrticaceae                 NA         NA      NA\n## growthformTree:FamilyUrticaceae                       NA         NA      NA\n## growthformHerb:FamilyViolaceae                        NA         NA      NA\n## growthformHerb/Shrub:FamilyViolaceae                  NA         NA      NA\n## growthformShrub:FamilyViolaceae                       NA         NA      NA\n## growthformShrub/Tree:FamilyViolaceae                  NA         NA      NA\n## growthformTree:FamilyViolaceae                        NA         NA      NA\n## growthformHerb:FamilyXanthorrhoeaceae                 NA         NA      NA\n## growthformHerb/Shrub:FamilyXanthorrhoeaceae           NA         NA      NA\n## growthformShrub:FamilyXanthorrhoeaceae                NA         NA      NA\n## growthformShrub/Tree:FamilyXanthorrhoeaceae           NA         NA      NA\n## growthformTree:FamilyXanthorrhoeaceae                 NA         NA      NA\n## growthformHerb:FamilyZygophyllaceae                   NA         NA      NA\n## growthformHerb/Shrub:FamilyZygophyllaceae             NA         NA      NA\n## growthformShrub:FamilyZygophyllaceae                  NA         NA      NA\n## growthformShrub/Tree:FamilyZygophyllaceae             NA         NA      NA\n## growthformTree:FamilyZygophyllaceae                   NA         NA      NA\n## growthformHerb:lat                            -7.667e+00  9.125e+00  -0.840\n## growthformHerb/Shrub:lat                              NA         NA      NA\n## growthformShrub:lat                            7.514e-02  3.299e-02   2.278\n## growthformShrub/Tree:lat                      -4.602e+00  4.806e+00  -0.957\n## growthformTree:lat                                    NA         NA      NA\n## growthformHerb:long                            3.209e-02  3.417e-02   0.939\n## growthformHerb/Shrub:long                             NA         NA      NA\n## growthformShrub:long                          -6.040e-03  1.358e-02  -0.445\n## growthformShrub/Tree:long                             NA         NA      NA\n## growthformTree:long                                   NA         NA      NA\n## growthformHerb:NPP                             4.933e-04  4.811e-04   1.025\n## growthformHerb/Shrub:NPP                              NA         NA      NA\n## growthformShrub:NPP                            6.473e-04  3.846e-04   1.683\n## growthformShrub/Tree:NPP                              NA         NA      NA\n## growthformTree:NPP                                    NA         NA      NA\n## FamilyAsteraceae:lat                           9.729e+00  8.577e+00   1.134\n## FamilyAtherospermataceae:lat                          NA         NA      NA\n## FamilyBalsaminaceae:lat                               NA         NA      NA\n## FamilyBetulaceae:lat                          -5.249e-01  1.875e+00  -0.280\n## FamilyBrassicaceae:lat                         6.681e+00  9.160e+00   0.729\n## FamilyCactaceae:lat                           -1.966e+00  8.672e-01  -2.267\n## FamilyCasuarinaceae:lat                               NA         NA      NA\n## FamilyChloranthaceae:lat                              NA         NA      NA\n## FamilyChrysobalanaceae:lat                    -7.988e-01  1.363e+00  -0.586\n## FamilyCistaceae:lat                                   NA         NA      NA\n## FamilyCornaceae:lat                                   NA         NA      NA\n## FamilyCrassulaceae:lat                                NA         NA      NA\n## FamilyCunoniaceae:lat                                 NA         NA      NA\n## FamilyCupressaceae:lat                                NA         NA      NA\n## FamilyCyperaceae:lat                                  NA         NA      NA\n## FamilyDennstaedtiaceae:lat                            NA         NA      NA\n## FamilyDicksoniaceae:lat                               NA         NA      NA\n## FamilyDipterocarpaceae:lat                            NA         NA      NA\n## FamilyEbenaceae:lat                           -2.703e-01  3.089e-01  -0.875\n## FamilyElaeocarpaceae:lat                              NA         NA      NA\n## FamilyEricaceae:lat                           -8.632e-01  6.646e-01  -1.299\n## FamilyEuphorbiaceae:lat                       -7.697e-01  7.758e-01  -0.992\n## FamilyFabaceae - C:lat                        -6.439e-01  7.319e-01  -0.880\n## FamilyFabaceae - M:lat                                NA         NA      NA\n## FamilyFabaceae - P:lat                        -2.849e+00  2.838e+00  -1.004\n## FamilyFagaceae:lat                            -6.391e-01  6.767e-01  -0.944\n## FamilyGentianaceae:lat                         7.953e+00  9.600e+00   0.828\n## FamilyHeliconiaceae:lat                               NA         NA      NA\n## FamilyJuglandaceae:lat                                NA         NA      NA\n## FamilyJuncaginaceae:lat                               NA         NA      NA\n## FamilyLamiaceae:lat                                   NA         NA      NA\n## FamilyLauraceae:lat                                   NA         NA      NA\n## FamilyMaesaceae:lat                                   NA         NA      NA\n## FamilyMalvaceae:lat                           -5.759e-01  7.352e-01  -0.783\n## FamilyMelastomataceae:lat                             NA         NA      NA\n## FamilyMoraceae:lat                                    NA         NA      NA\n## FamilyMyristicaceae:lat                               NA         NA      NA\n## FamilyMyrsinaceae:lat                                 NA         NA      NA\n## FamilyMyrtaceae:lat                           -2.630e-01  6.091e-01  -0.432\n## FamilyOchnaceae:lat                                   NA         NA      NA\n## FamilyOnagraceae:lat                                  NA         NA      NA\n## FamilyOrobanchaceae:lat                        6.419e+00  8.683e+00   0.739\n## FamilyPhyllanthaceae:lat                              NA         NA      NA\n## FamilyPicrodendraceae:lat                             NA         NA      NA\n## FamilyPinaceae:lat                            -1.017e+00  6.698e-01  -1.518\n## FamilyPoaceae:lat                              7.161e+00  8.523e+00   0.840\n## FamilyPolemoniaceae:lat                        6.911e+00  8.461e+00   0.817\n## FamilyPolygonaceae:lat                         6.776e+00  8.505e+00   0.797\n## FamilyProteaceae:lat                          -1.078e+00  6.906e-01  -1.560\n## FamilyRanunculaceae:lat                               NA         NA      NA\n## FamilyRhamnaceae:lat                                  NA         NA      NA\n## FamilyRosaceae:lat                            -8.875e-01  5.851e-01  -1.517\n## FamilyRubiaceae:lat                           -1.008e+00  7.723e-01  -1.305\n## FamilyRutaceae:lat                                    NA         NA      NA\n## FamilySalicaceae:lat                          -1.327e+00  1.062e+00  -1.249\n## FamilySapindaceae:lat                         -7.032e-01  7.657e-01  -0.918\n## FamilySapotaceae:lat                                  NA         NA      NA\n## FamilyScrophulariaceae:lat                            NA         NA      NA\n## FamilyThymelaeaceae:lat                               NA         NA      NA\n## FamilyUlmaceae:lat                            -1.490e+00  1.311e+00  -1.137\n## FamilyUrticaceae:lat                          -1.170e+00  1.217e+00  -0.962\n## FamilyViolaceae:lat                                   NA         NA      NA\n## FamilyXanthorrhoeaceae:lat                            NA         NA      NA\n## FamilyZygophyllaceae:lat                              NA         NA      NA\n## FamilyAsteraceae:long                          3.953e-02  8.917e-03   4.433\n## FamilyAtherospermataceae:long                         NA         NA      NA\n## FamilyBalsaminaceae:long                              NA         NA      NA\n## FamilyBetulaceae:long                          1.284e+00  1.382e+00   0.929\n## FamilyBrassicaceae:long                               NA         NA      NA\n## FamilyCactaceae:long                           3.726e-02  2.427e-02   1.535\n## FamilyCasuarinaceae:long                              NA         NA      NA\n## FamilyChloranthaceae:long                             NA         NA      NA\n## FamilyChrysobalanaceae:long                           NA         NA      NA\n## FamilyCistaceae:long                                  NA         NA      NA\n## FamilyCornaceae:long                                  NA         NA      NA\n## FamilyCrassulaceae:long                               NA         NA      NA\n## FamilyCunoniaceae:long                                NA         NA      NA\n## FamilyCupressaceae:long                               NA         NA      NA\n## FamilyCyperaceae:long                                 NA         NA      NA\n## FamilyDennstaedtiaceae:long                           NA         NA      NA\n## FamilyDicksoniaceae:long                              NA         NA      NA\n## FamilyDipterocarpaceae:long                           NA         NA      NA\n## FamilyEbenaceae:long                                  NA         NA      NA\n## FamilyElaeocarpaceae:long                             NA         NA      NA\n## FamilyEricaceae:long                           6.409e-02  2.972e-02   2.156\n## FamilyEuphorbiaceae:long                       4.371e-02  4.554e-02   0.960\n## FamilyFabaceae - C:long                               NA         NA      NA\n## FamilyFabaceae - M:long                               NA         NA      NA\n## FamilyFabaceae - P:long                               NA         NA      NA\n## FamilyFagaceae:long                                   NA         NA      NA\n## FamilyGentianaceae:long                               NA         NA      NA\n## FamilyHeliconiaceae:long                              NA         NA      NA\n## FamilyJuglandaceae:long                               NA         NA      NA\n## FamilyJuncaginaceae:long                              NA         NA      NA\n## FamilyLamiaceae:long                                  NA         NA      NA\n## FamilyLauraceae:long                                  NA         NA      NA\n## FamilyMaesaceae:long                                  NA         NA      NA\n## FamilyMalvaceae:long                           3.701e-02  4.705e-02   0.787\n## FamilyMelastomataceae:long                            NA         NA      NA\n## FamilyMoraceae:long                                   NA         NA      NA\n## FamilyMyristicaceae:long                              NA         NA      NA\n## FamilyMyrsinaceae:long                                NA         NA      NA\n## FamilyMyrtaceae:long                           5.316e-02  3.216e-02   1.653\n## FamilyOchnaceae:long                                  NA         NA      NA\n## FamilyOnagraceae:long                                 NA         NA      NA\n## FamilyOrobanchaceae:long                              NA         NA      NA\n## FamilyPhyllanthaceae:long                             NA         NA      NA\n## FamilyPicrodendraceae:long                            NA         NA      NA\n## FamilyPinaceae:long                            1.006e-03  3.483e-02   0.029\n## FamilyPoaceae:long                                    NA         NA      NA\n## FamilyPolemoniaceae:long                              NA         NA      NA\n## FamilyPolygonaceae:long                               NA         NA      NA\n## FamilyProteaceae:long                          3.544e-02  2.338e-02   1.516\n## FamilyRanunculaceae:long                              NA         NA      NA\n## FamilyRhamnaceae:long                                 NA         NA      NA\n## FamilyRosaceae:long                            6.083e-02  5.503e-02   1.105\n## FamilyRubiaceae:long                                  NA         NA      NA\n## FamilyRutaceae:long                                   NA         NA      NA\n## FamilySalicaceae:long                                 NA         NA      NA\n## FamilySapindaceae:long                         4.512e-02  2.639e-02   1.710\n## FamilySapotaceae:long                                 NA         NA      NA\n## FamilyScrophulariaceae:long                           NA         NA      NA\n## FamilyThymelaeaceae:long                              NA         NA      NA\n## FamilyUlmaceae:long                                   NA         NA      NA\n## FamilyUrticaceae:long                                 NA         NA      NA\n## FamilyViolaceae:long                                  NA         NA      NA\n## FamilyXanthorrhoeaceae:long                           NA         NA      NA\n## FamilyZygophyllaceae:long                             NA         NA      NA\n## FamilyAsteraceae:alt                           2.977e-02  1.249e-02   2.384\n## FamilyAtherospermataceae:alt                          NA         NA      NA\n## FamilyBalsaminaceae:alt                               NA         NA      NA\n## FamilyBetulaceae:alt                                  NA         NA      NA\n## FamilyBrassicaceae:alt                                NA         NA      NA\n## FamilyCactaceae:alt                           -4.211e-03  9.104e-03  -0.463\n## FamilyCasuarinaceae:alt                               NA         NA      NA\n## FamilyChloranthaceae:alt                              NA         NA      NA\n## FamilyChrysobalanaceae:alt                            NA         NA      NA\n## FamilyCistaceae:alt                                   NA         NA      NA\n## FamilyCornaceae:alt                                   NA         NA      NA\n## FamilyCrassulaceae:alt                                NA         NA      NA\n## FamilyCunoniaceae:alt                                 NA         NA      NA\n## FamilyCupressaceae:alt                                NA         NA      NA\n## FamilyCyperaceae:alt                                  NA         NA      NA\n## FamilyDennstaedtiaceae:alt                            NA         NA      NA\n## FamilyDicksoniaceae:alt                               NA         NA      NA\n## FamilyDipterocarpaceae:alt                            NA         NA      NA\n## FamilyEbenaceae:alt                                   NA         NA      NA\n## FamilyElaeocarpaceae:alt                              NA         NA      NA\n## FamilyEricaceae:alt                            2.057e-03  9.530e-03   0.216\n## FamilyEuphorbiaceae:alt                               NA         NA      NA\n## FamilyFabaceae - C:alt                                NA         NA      NA\n## FamilyFabaceae - M:alt                                NA         NA      NA\n## FamilyFabaceae - P:alt                                NA         NA      NA\n## FamilyFagaceae:alt                                    NA         NA      NA\n## FamilyGentianaceae:alt                                NA         NA      NA\n## FamilyHeliconiaceae:alt                               NA         NA      NA\n## FamilyJuglandaceae:alt                                NA         NA      NA\n## FamilyJuncaginaceae:alt                               NA         NA      NA\n## FamilyLamiaceae:alt                                   NA         NA      NA\n## FamilyLauraceae:alt                                   NA         NA      NA\n## FamilyMaesaceae:alt                                   NA         NA      NA\n## FamilyMalvaceae:alt                                   NA         NA      NA\n## FamilyMelastomataceae:alt                             NA         NA      NA\n## FamilyMoraceae:alt                                    NA         NA      NA\n## FamilyMyristicaceae:alt                               NA         NA      NA\n## FamilyMyrsinaceae:alt                                 NA         NA      NA\n## FamilyMyrtaceae:alt                           -2.001e-03  8.486e-03  -0.236\n## FamilyOchnaceae:alt                                   NA         NA      NA\n## FamilyOnagraceae:alt                                  NA         NA      NA\n## FamilyOrobanchaceae:alt                               NA         NA      NA\n## FamilyPhyllanthaceae:alt                              NA         NA      NA\n## FamilyPicrodendraceae:alt                             NA         NA      NA\n## FamilyPinaceae:alt                            -7.229e-03  8.920e-03  -0.810\n## FamilyPoaceae:alt                             -3.846e-03  8.857e-03  -0.434\n## FamilyPolemoniaceae:alt                               NA         NA      NA\n## FamilyPolygonaceae:alt                                NA         NA      NA\n## FamilyProteaceae:alt                          -8.882e-03  8.984e-03  -0.989\n## FamilyRanunculaceae:alt                               NA         NA      NA\n## FamilyRhamnaceae:alt                                  NA         NA      NA\n## FamilyRosaceae:alt                                    NA         NA      NA\n## FamilyRubiaceae:alt                                   NA         NA      NA\n## FamilyRutaceae:alt                                    NA         NA      NA\n## FamilySalicaceae:alt                                  NA         NA      NA\n## FamilySapindaceae:alt                                 NA         NA      NA\n## FamilySapotaceae:alt                                  NA         NA      NA\n## FamilyScrophulariaceae:alt                            NA         NA      NA\n## FamilyThymelaeaceae:alt                               NA         NA      NA\n## FamilyUlmaceae:alt                                    NA         NA      NA\n## FamilyUrticaceae:alt                                  NA         NA      NA\n## FamilyViolaceae:alt                                   NA         NA      NA\n## FamilyXanthorrhoeaceae:alt                            NA         NA      NA\n## FamilyZygophyllaceae:alt                              NA         NA      NA\n## FamilyAsteraceae:temp                          9.741e+00  2.649e+00   3.678\n## FamilyAtherospermataceae:temp                         NA         NA      NA\n## FamilyBalsaminaceae:temp                              NA         NA      NA\n## FamilyBetulaceae:temp                                 NA         NA      NA\n## FamilyBrassicaceae:temp                               NA         NA      NA\n## FamilyCactaceae:temp                                  NA         NA      NA\n## FamilyCasuarinaceae:temp                              NA         NA      NA\n## FamilyChloranthaceae:temp                             NA         NA      NA\n## FamilyChrysobalanaceae:temp                           NA         NA      NA\n## FamilyCistaceae:temp                                  NA         NA      NA\n## FamilyCornaceae:temp                                  NA         NA      NA\n## FamilyCrassulaceae:temp                               NA         NA      NA\n## FamilyCunoniaceae:temp                                NA         NA      NA\n## FamilyCupressaceae:temp                               NA         NA      NA\n## FamilyCyperaceae:temp                                 NA         NA      NA\n## FamilyDennstaedtiaceae:temp                           NA         NA      NA\n## FamilyDicksoniaceae:temp                              NA         NA      NA\n## FamilyDipterocarpaceae:temp                           NA         NA      NA\n## FamilyEbenaceae:temp                                  NA         NA      NA\n## FamilyElaeocarpaceae:temp                             NA         NA      NA\n## FamilyEricaceae:temp                                  NA         NA      NA\n## FamilyEuphorbiaceae:temp                              NA         NA      NA\n## FamilyFabaceae - C:temp                               NA         NA      NA\n## FamilyFabaceae - M:temp                               NA         NA      NA\n## FamilyFabaceae - P:temp                               NA         NA      NA\n## FamilyFagaceae:temp                                   NA         NA      NA\n## FamilyGentianaceae:temp                               NA         NA      NA\n## FamilyHeliconiaceae:temp                              NA         NA      NA\n## FamilyJuglandaceae:temp                               NA         NA      NA\n## FamilyJuncaginaceae:temp                              NA         NA      NA\n## FamilyLamiaceae:temp                                  NA         NA      NA\n## FamilyLauraceae:temp                                  NA         NA      NA\n## FamilyMaesaceae:temp                                  NA         NA      NA\n## FamilyMalvaceae:temp                                  NA         NA      NA\n## FamilyMelastomataceae:temp                            NA         NA      NA\n## FamilyMoraceae:temp                                   NA         NA      NA\n## FamilyMyristicaceae:temp                              NA         NA      NA\n## FamilyMyrsinaceae:temp                                NA         NA      NA\n## FamilyMyrtaceae:temp                           1.113e+00  3.408e-01   3.266\n## FamilyOchnaceae:temp                                  NA         NA      NA\n## FamilyOnagraceae:temp                                 NA         NA      NA\n## FamilyOrobanchaceae:temp                              NA         NA      NA\n## FamilyPhyllanthaceae:temp                             NA         NA      NA\n## FamilyPicrodendraceae:temp                            NA         NA      NA\n## FamilyPinaceae:temp                                   NA         NA      NA\n## FamilyPoaceae:temp                             6.425e-01  2.770e-01   2.320\n## FamilyPolemoniaceae:temp                              NA         NA      NA\n## FamilyPolygonaceae:temp                               NA         NA      NA\n## FamilyProteaceae:temp                                 NA         NA      NA\n## FamilyRanunculaceae:temp                              NA         NA      NA\n## FamilyRhamnaceae:temp                                 NA         NA      NA\n## FamilyRosaceae:temp                                   NA         NA      NA\n## FamilyRubiaceae:temp                                  NA         NA      NA\n## FamilyRutaceae:temp                                   NA         NA      NA\n## FamilySalicaceae:temp                                 NA         NA      NA\n## FamilySapindaceae:temp                                NA         NA      NA\n## FamilySapotaceae:temp                                 NA         NA      NA\n## FamilyScrophulariaceae:temp                           NA         NA      NA\n## FamilyThymelaeaceae:temp                              NA         NA      NA\n## FamilyUlmaceae:temp                                   NA         NA      NA\n## FamilyUrticaceae:temp                                 NA         NA      NA\n## FamilyViolaceae:temp                                  NA         NA      NA\n## FamilyXanthorrhoeaceae:temp                           NA         NA      NA\n## FamilyZygophyllaceae:temp                             NA         NA      NA\n## lat:long                                      -4.850e-04  3.239e-04  -1.497\n## lat:temp                                      -5.288e-03  1.011e-03  -5.231\n## lat:NPP                                       -4.384e-05  4.734e-05  -0.926\n## long:alt                                      -1.635e-06  2.683e-06  -0.609\n## long:temp                                     -1.650e-03  5.755e-04  -2.868\n## long:NPP                                       7.363e-06  2.523e-06   2.918\n## alt:temp                                      -5.835e-05  1.708e-05  -3.416\n## temp:NPP                                      -1.631e-04  8.021e-05  -2.034\n##                                               Pr(>|t|)    \n## (Intercept)                                   0.471210    \n## growthformHerb                                0.401018    \n## growthformHerb/Shrub                          0.959364    \n## growthformShrub                               0.459650    \n## growthformShrub/Tree                          0.576973    \n## growthformTree                                0.209409    \n## FamilyAsteraceae                              0.024915 *  \n## FamilyAtherospermataceae                      0.240297    \n## FamilyBalsaminaceae                           0.445806    \n## FamilyBetulaceae                              0.781378    \n## FamilyBrassicaceae                            0.520831    \n## FamilyCactaceae                               0.374789    \n## FamilyCasuarinaceae                           0.457542    \n## FamilyChloranthaceae                          0.451076    \n## FamilyChrysobalanaceae                        0.531209    \n## FamilyCistaceae                               0.457466    \n## FamilyCornaceae                               0.444251    \n## FamilyCrassulaceae                            0.461491    \n## FamilyCunoniaceae                             0.580889    \n## FamilyCupressaceae                            0.306038    \n## FamilyCyperaceae                              0.397873    \n## FamilyDennstaedtiaceae                              NA    \n## FamilyDicksoniaceae                           0.468095    \n## FamilyDipterocarpaceae                        0.398966    \n## FamilyEbenaceae                               0.397522    \n## FamilyElaeocarpaceae                          0.323838    \n## FamilyEricaceae                               0.451810    \n## FamilyEuphorbiaceae                           0.438290    \n## FamilyFabaceae - C                            0.453083    \n## FamilyFabaceae - M                            0.244394    \n## FamilyFabaceae - P                            0.340876    \n## FamilyFagaceae                                0.799425    \n## FamilyGentianaceae                            0.406314    \n## FamilyHeliconiaceae                           0.418080    \n## FamilyJuglandaceae                            0.307939    \n## FamilyJuncaginaceae                           0.445752    \n## FamilyLamiaceae                               0.474074    \n## FamilyLauraceae                               0.548932    \n## FamilyMaesaceae                               0.447295    \n## FamilyMalvaceae                               0.537119    \n## FamilyMelastomataceae                         0.447635    \n## FamilyMoraceae                                0.380649    \n## FamilyMyristicaceae                           0.349574    \n## FamilyMyrsinaceae                             0.393440    \n## FamilyMyrtaceae                               0.011832 *  \n## FamilyOchnaceae                               0.448174    \n## FamilyOnagraceae                                    NA    \n## FamilyOrobanchaceae                           0.553364    \n## FamilyPhyllanthaceae                          0.393311    \n## FamilyPicrodendraceae                         0.359305    \n## FamilyPinaceae                                0.087314 .  \n## FamilyPoaceae                                 0.529061    \n## FamilyPolemoniaceae                           0.415849    \n## FamilyPolygonaceae                            0.444282    \n## FamilyProteaceae                              0.106800    \n## FamilyRanunculaceae                           0.447850    \n## FamilyRhamnaceae                              0.466950    \n## FamilyRosaceae                                0.016301 *  \n## FamilyRubiaceae                               0.441496    \n## FamilyRutaceae                                0.456598    \n## FamilySalicaceae                              0.433258    \n## FamilySapindaceae                             0.607885    \n## FamilySapotaceae                              0.276654    \n## FamilyScrophulariaceae                        0.448464    \n## FamilyThymelaeaceae                           0.336091    \n## FamilyUlmaceae                                0.231765    \n## FamilyUrticaceae                              0.338942    \n## FamilyViolaceae                               0.447326    \n## FamilyXanthorrhoeaceae                        0.455414    \n## FamilyZygophyllaceae                          0.467051    \n## lat                                           0.325004    \n## long                                          0.860913    \n## alt                                           0.550965    \n## temp                                          0.447922    \n## NPP                                           0.298512    \n## growthformHerb:FamilyAsteraceae               0.761650    \n## growthformHerb/Shrub:FamilyAsteraceae               NA    \n## growthformShrub:FamilyAsteraceae                    NA    \n## growthformShrub/Tree:FamilyAsteraceae               NA    \n## growthformTree:FamilyAsteraceae                     NA    \n## growthformHerb:FamilyAtherospermataceae             NA    \n## growthformHerb/Shrub:FamilyAtherospermataceae       NA    \n## growthformShrub:FamilyAtherospermataceae            NA    \n## growthformShrub/Tree:FamilyAtherospermataceae       NA    \n## growthformTree:FamilyAtherospermataceae             NA    \n## growthformHerb:FamilyBalsaminaceae                  NA    \n## growthformHerb/Shrub:FamilyBalsaminaceae            NA    \n## growthformShrub:FamilyBalsaminaceae                 NA    \n## growthformShrub/Tree:FamilyBalsaminaceae            NA    \n## growthformTree:FamilyBalsaminaceae                  NA    \n## growthformHerb:FamilyBetulaceae                     NA    \n## growthformHerb/Shrub:FamilyBetulaceae               NA    \n## growthformShrub:FamilyBetulaceae              0.483172    \n## growthformShrub/Tree:FamilyBetulaceae               NA    \n## growthformTree:FamilyBetulaceae                     NA    \n## growthformHerb:FamilyBrassicaceae                   NA    \n## growthformHerb/Shrub:FamilyBrassicaceae             NA    \n## growthformShrub:FamilyBrassicaceae                  NA    \n## growthformShrub/Tree:FamilyBrassicaceae             NA    \n## growthformTree:FamilyBrassicaceae                   NA    \n## growthformHerb:FamilyCactaceae                      NA    \n## growthformHerb/Shrub:FamilyCactaceae                NA    \n## growthformShrub:FamilyCactaceae                     NA    \n## growthformShrub/Tree:FamilyCactaceae                NA    \n## growthformTree:FamilyCactaceae                      NA    \n## growthformHerb:FamilyCasuarinaceae                  NA    \n## growthformHerb/Shrub:FamilyCasuarinaceae            NA    \n## growthformShrub:FamilyCasuarinaceae                 NA    \n## growthformShrub/Tree:FamilyCasuarinaceae            NA    \n## growthformTree:FamilyCasuarinaceae                  NA    \n## growthformHerb:FamilyChloranthaceae                 NA    \n## growthformHerb/Shrub:FamilyChloranthaceae           NA    \n## growthformShrub:FamilyChloranthaceae                NA    \n## growthformShrub/Tree:FamilyChloranthaceae           NA    \n## growthformTree:FamilyChloranthaceae                 NA    \n## growthformHerb:FamilyChrysobalanaceae               NA    \n## growthformHerb/Shrub:FamilyChrysobalanaceae         NA    \n## growthformShrub:FamilyChrysobalanaceae              NA    \n## growthformShrub/Tree:FamilyChrysobalanaceae         NA    \n## growthformTree:FamilyChrysobalanaceae               NA    \n## growthformHerb:FamilyCistaceae                      NA    \n## growthformHerb/Shrub:FamilyCistaceae                NA    \n## growthformShrub:FamilyCistaceae                     NA    \n## growthformShrub/Tree:FamilyCistaceae                NA    \n## growthformTree:FamilyCistaceae                      NA    \n## growthformHerb:FamilyCornaceae                      NA    \n## growthformHerb/Shrub:FamilyCornaceae                NA    \n## growthformShrub:FamilyCornaceae                     NA    \n## growthformShrub/Tree:FamilyCornaceae                NA    \n## growthformTree:FamilyCornaceae                      NA    \n## growthformHerb:FamilyCrassulaceae                   NA    \n## growthformHerb/Shrub:FamilyCrassulaceae             NA    \n## growthformShrub:FamilyCrassulaceae                  NA    \n## growthformShrub/Tree:FamilyCrassulaceae             NA    \n## growthformTree:FamilyCrassulaceae                   NA    \n## growthformHerb:FamilyCunoniaceae                    NA    \n## growthformHerb/Shrub:FamilyCunoniaceae              NA    \n## growthformShrub:FamilyCunoniaceae                   NA    \n## growthformShrub/Tree:FamilyCunoniaceae        0.448590    \n## growthformTree:FamilyCunoniaceae                    NA    \n## growthformHerb:FamilyCupressaceae                   NA    \n## growthformHerb/Shrub:FamilyCupressaceae             NA    \n## growthformShrub:FamilyCupressaceae            0.447817    \n## growthformShrub/Tree:FamilyCupressaceae             NA    \n## growthformTree:FamilyCupressaceae                   NA    \n## growthformHerb:FamilyCyperaceae                     NA    \n## growthformHerb/Shrub:FamilyCyperaceae               NA    \n## growthformShrub:FamilyCyperaceae                    NA    \n## growthformShrub/Tree:FamilyCyperaceae               NA    \n## growthformTree:FamilyCyperaceae                     NA    \n## growthformHerb:FamilyDennstaedtiaceae               NA    \n## growthformHerb/Shrub:FamilyDennstaedtiaceae         NA    \n## growthformShrub:FamilyDennstaedtiaceae              NA    \n## growthformShrub/Tree:FamilyDennstaedtiaceae         NA    \n## growthformTree:FamilyDennstaedtiaceae               NA    \n## growthformHerb:FamilyDicksoniaceae                  NA    \n## growthformHerb/Shrub:FamilyDicksoniaceae            NA    \n## growthformShrub:FamilyDicksoniaceae                 NA    \n## growthformShrub/Tree:FamilyDicksoniaceae            NA    \n## growthformTree:FamilyDicksoniaceae                  NA    \n## growthformHerb:FamilyDipterocarpaceae               NA    \n## growthformHerb/Shrub:FamilyDipterocarpaceae         NA    \n## growthformShrub:FamilyDipterocarpaceae              NA    \n## growthformShrub/Tree:FamilyDipterocarpaceae         NA    \n## growthformTree:FamilyDipterocarpaceae               NA    \n## growthformHerb:FamilyEbenaceae                      NA    \n## growthformHerb/Shrub:FamilyEbenaceae                NA    \n## growthformShrub:FamilyEbenaceae                     NA    \n## growthformShrub/Tree:FamilyEbenaceae                NA    \n## growthformTree:FamilyEbenaceae                      NA    \n## growthformHerb:FamilyElaeocarpaceae                 NA    \n## growthformHerb/Shrub:FamilyElaeocarpaceae           NA    \n## growthformShrub:FamilyElaeocarpaceae                NA    \n## growthformShrub/Tree:FamilyElaeocarpaceae           NA    \n## growthformTree:FamilyElaeocarpaceae                 NA    \n## growthformHerb:FamilyEricaceae                      NA    \n## growthformHerb/Shrub:FamilyEricaceae                NA    \n## growthformShrub:FamilyEricaceae               0.724294    \n## growthformShrub/Tree:FamilyEricaceae                NA    \n## growthformTree:FamilyEricaceae                      NA    \n## growthformHerb:FamilyEuphorbiaceae                  NA    \n## growthformHerb/Shrub:FamilyEuphorbiaceae            NA    \n## growthformShrub:FamilyEuphorbiaceae           0.444658    \n## growthformShrub/Tree:FamilyEuphorbiaceae            NA    \n## growthformTree:FamilyEuphorbiaceae                  NA    \n## growthformHerb:FamilyFabaceae - C                   NA    \n## growthformHerb/Shrub:FamilyFabaceae - C             NA    \n## growthformShrub:FamilyFabaceae - C                  NA    \n## growthformShrub/Tree:FamilyFabaceae - C             NA    \n## growthformTree:FamilyFabaceae - C                   NA    \n## growthformHerb:FamilyFabaceae - M                   NA    \n## growthformHerb/Shrub:FamilyFabaceae - M             NA    \n## growthformShrub:FamilyFabaceae - M                  NA    \n## growthformShrub/Tree:FamilyFabaceae - M       0.432247    \n## growthformTree:FamilyFabaceae - M                   NA    \n## growthformHerb:FamilyFabaceae - P             0.412368    \n## growthformHerb/Shrub:FamilyFabaceae - P             NA    \n## growthformShrub:FamilyFabaceae - P            0.434788    \n## growthformShrub/Tree:FamilyFabaceae - P             NA    \n## growthformTree:FamilyFabaceae - P                   NA    \n## growthformHerb:FamilyFagaceae                       NA    \n## growthformHerb/Shrub:FamilyFagaceae                 NA    \n## growthformShrub:FamilyFagaceae                0.451214    \n## growthformShrub/Tree:FamilyFagaceae                 NA    \n## growthformTree:FamilyFagaceae                       NA    \n## growthformHerb:FamilyGentianaceae                   NA    \n## growthformHerb/Shrub:FamilyGentianaceae             NA    \n## growthformShrub:FamilyGentianaceae                  NA    \n## growthformShrub/Tree:FamilyGentianaceae             NA    \n## growthformTree:FamilyGentianaceae                   NA    \n## growthformHerb:FamilyHeliconiaceae                  NA    \n## growthformHerb/Shrub:FamilyHeliconiaceae            NA    \n## growthformShrub:FamilyHeliconiaceae                 NA    \n## growthformShrub/Tree:FamilyHeliconiaceae            NA    \n## growthformTree:FamilyHeliconiaceae                  NA    \n## growthformHerb:FamilyJuglandaceae                   NA    \n## growthformHerb/Shrub:FamilyJuglandaceae             NA    \n## growthformShrub:FamilyJuglandaceae                  NA    \n## growthformShrub/Tree:FamilyJuglandaceae             NA    \n## growthformTree:FamilyJuglandaceae                   NA    \n## growthformHerb:FamilyJuncaginaceae                  NA    \n## growthformHerb/Shrub:FamilyJuncaginaceae            NA    \n## growthformShrub:FamilyJuncaginaceae                 NA    \n## growthformShrub/Tree:FamilyJuncaginaceae            NA    \n## growthformTree:FamilyJuncaginaceae                  NA    \n## growthformHerb:FamilyLamiaceae                      NA    \n## growthformHerb/Shrub:FamilyLamiaceae                NA    \n## growthformShrub:FamilyLamiaceae                     NA    \n## growthformShrub/Tree:FamilyLamiaceae                NA    \n## growthformTree:FamilyLamiaceae                      NA    \n## growthformHerb:FamilyLauraceae                      NA    \n## growthformHerb/Shrub:FamilyLauraceae                NA    \n## growthformShrub:FamilyLauraceae                     NA    \n## growthformShrub/Tree:FamilyLauraceae                NA    \n## growthformTree:FamilyLauraceae                      NA    \n## growthformHerb:FamilyMaesaceae                      NA    \n## growthformHerb/Shrub:FamilyMaesaceae                NA    \n## growthformShrub:FamilyMaesaceae                     NA    \n## growthformShrub/Tree:FamilyMaesaceae                NA    \n## growthformTree:FamilyMaesaceae                      NA    \n## growthformHerb:FamilyMalvaceae                      NA    \n## growthformHerb/Shrub:FamilyMalvaceae                NA    \n## growthformShrub:FamilyMalvaceae                     NA    \n## growthformShrub/Tree:FamilyMalvaceae                NA    \n## growthformTree:FamilyMalvaceae                      NA    \n## growthformHerb:FamilyMelastomataceae                NA    \n## growthformHerb/Shrub:FamilyMelastomataceae          NA    \n## growthformShrub:FamilyMelastomataceae               NA    \n## growthformShrub/Tree:FamilyMelastomataceae          NA    \n## growthformTree:FamilyMelastomataceae                NA    \n## growthformHerb:FamilyMoraceae                       NA    \n## growthformHerb/Shrub:FamilyMoraceae                 NA    \n## growthformShrub:FamilyMoraceae                      NA    \n## growthformShrub/Tree:FamilyMoraceae                 NA    \n## growthformTree:FamilyMoraceae                       NA    \n## growthformHerb:FamilyMyristicaceae                  NA    \n## growthformHerb/Shrub:FamilyMyristicaceae            NA    \n## growthformShrub:FamilyMyristicaceae                 NA    \n## growthformShrub/Tree:FamilyMyristicaceae            NA    \n## growthformTree:FamilyMyristicaceae                  NA    \n## growthformHerb:FamilyMyrsinaceae                    NA    \n## growthformHerb/Shrub:FamilyMyrsinaceae              NA    \n## growthformShrub:FamilyMyrsinaceae                   NA    \n## growthformShrub/Tree:FamilyMyrsinaceae        0.444600    \n## growthformTree:FamilyMyrsinaceae                    NA    \n## growthformHerb:FamilyMyrtaceae                      NA    \n## growthformHerb/Shrub:FamilyMyrtaceae                NA    \n## growthformShrub:FamilyMyrtaceae               0.441684    \n## growthformShrub/Tree:FamilyMyrtaceae                NA    \n## growthformTree:FamilyMyrtaceae                      NA    \n## growthformHerb:FamilyOchnaceae                      NA    \n## growthformHerb/Shrub:FamilyOchnaceae                NA    \n## growthformShrub:FamilyOchnaceae                     NA    \n## growthformShrub/Tree:FamilyOchnaceae                NA    \n## growthformTree:FamilyOchnaceae                      NA    \n## growthformHerb:FamilyOnagraceae                     NA    \n## growthformHerb/Shrub:FamilyOnagraceae               NA    \n## growthformShrub:FamilyOnagraceae                    NA    \n## growthformShrub/Tree:FamilyOnagraceae               NA    \n## growthformTree:FamilyOnagraceae                     NA    \n## growthformHerb:FamilyOrobanchaceae                  NA    \n## growthformHerb/Shrub:FamilyOrobanchaceae            NA    \n## growthformShrub:FamilyOrobanchaceae                 NA    \n## growthformShrub/Tree:FamilyOrobanchaceae            NA    \n## growthformTree:FamilyOrobanchaceae                  NA    \n## growthformHerb:FamilyPhyllanthaceae                 NA    \n## growthformHerb/Shrub:FamilyPhyllanthaceae           NA    \n## growthformShrub:FamilyPhyllanthaceae                NA    \n## growthformShrub/Tree:FamilyPhyllanthaceae           NA    \n## growthformTree:FamilyPhyllanthaceae                 NA    \n## growthformHerb:FamilyPicrodendraceae                NA    \n## growthformHerb/Shrub:FamilyPicrodendraceae          NA    \n## growthformShrub:FamilyPicrodendraceae               NA    \n## growthformShrub/Tree:FamilyPicrodendraceae          NA    \n## growthformTree:FamilyPicrodendraceae                NA    \n## growthformHerb:FamilyPinaceae                       NA    \n## growthformHerb/Shrub:FamilyPinaceae                 NA    \n## growthformShrub:FamilyPinaceae                      NA    \n## growthformShrub/Tree:FamilyPinaceae                 NA    \n## growthformTree:FamilyPinaceae                       NA    \n## growthformHerb:FamilyPoaceae                  0.441587    \n## growthformHerb/Shrub:FamilyPoaceae                  NA    \n## growthformShrub:FamilyPoaceae                       NA    \n## growthformShrub/Tree:FamilyPoaceae                  NA    \n## growthformTree:FamilyPoaceae                        NA    \n## growthformHerb:FamilyPolemoniaceae                  NA    \n## growthformHerb/Shrub:FamilyPolemoniaceae            NA    \n## growthformShrub:FamilyPolemoniaceae                 NA    \n## growthformShrub/Tree:FamilyPolemoniaceae            NA    \n## growthformTree:FamilyPolemoniaceae                  NA    \n## growthformHerb:FamilyPolygonaceae                   NA    \n## growthformHerb/Shrub:FamilyPolygonaceae             NA    \n## growthformShrub:FamilyPolygonaceae                  NA    \n## growthformShrub/Tree:FamilyPolygonaceae             NA    \n## growthformTree:FamilyPolygonaceae                   NA    \n## growthformHerb:FamilyProteaceae                     NA    \n## growthformHerb/Shrub:FamilyProteaceae               NA    \n## growthformShrub:FamilyProteaceae              0.441738    \n## growthformShrub/Tree:FamilyProteaceae               NA    \n## growthformTree:FamilyProteaceae                     NA    \n## growthformHerb:FamilyRanunculaceae                  NA    \n## growthformHerb/Shrub:FamilyRanunculaceae            NA    \n## growthformShrub:FamilyRanunculaceae                 NA    \n## growthformShrub/Tree:FamilyRanunculaceae            NA    \n## growthformTree:FamilyRanunculaceae                  NA    \n## growthformHerb:FamilyRhamnaceae                     NA    \n## growthformHerb/Shrub:FamilyRhamnaceae               NA    \n## growthformShrub:FamilyRhamnaceae                    NA    \n## growthformShrub/Tree:FamilyRhamnaceae               NA    \n## growthformTree:FamilyRhamnaceae                     NA    \n## growthformHerb:FamilyRosaceae                 0.423206    \n## growthformHerb/Shrub:FamilyRosaceae                 NA    \n## growthformShrub:FamilyRosaceae                0.451569    \n## growthformShrub/Tree:FamilyRosaceae                 NA    \n## growthformTree:FamilyRosaceae                       NA    \n## growthformHerb:FamilyRubiaceae                      NA    \n## growthformHerb/Shrub:FamilyRubiaceae                NA    \n## growthformShrub:FamilyRubiaceae               0.094423 .  \n## growthformShrub/Tree:FamilyRubiaceae                NA    \n## growthformTree:FamilyRubiaceae                      NA    \n## growthformHerb:FamilyRutaceae                       NA    \n## growthformHerb/Shrub:FamilyRutaceae                 NA    \n## growthformShrub:FamilyRutaceae                      NA    \n## growthformShrub/Tree:FamilyRutaceae                 NA    \n## growthformTree:FamilyRutaceae                       NA    \n## growthformHerb:FamilySalicaceae                     NA    \n## growthformHerb/Shrub:FamilySalicaceae               NA    \n## growthformShrub:FamilySalicaceae              0.195221    \n## growthformShrub/Tree:FamilySalicaceae               NA    \n## growthformTree:FamilySalicaceae                     NA    \n## growthformHerb:FamilySapindaceae                    NA    \n## growthformHerb/Shrub:FamilySapindaceae              NA    \n## growthformShrub:FamilySapindaceae                   NA    \n## growthformShrub/Tree:FamilySapindaceae              NA    \n## growthformTree:FamilySapindaceae                    NA    \n## growthformHerb:FamilySapotaceae                     NA    \n## growthformHerb/Shrub:FamilySapotaceae               NA    \n## growthformShrub:FamilySapotaceae                    NA    \n## growthformShrub/Tree:FamilySapotaceae               NA    \n## growthformTree:FamilySapotaceae                     NA    \n## growthformHerb:FamilyScrophulariaceae               NA    \n## growthformHerb/Shrub:FamilyScrophulariaceae         NA    \n## growthformShrub:FamilyScrophulariaceae              NA    \n## growthformShrub/Tree:FamilyScrophulariaceae         NA    \n## growthformTree:FamilyScrophulariaceae               NA    \n## growthformHerb:FamilyThymelaeaceae                  NA    \n## growthformHerb/Shrub:FamilyThymelaeaceae            NA    \n## growthformShrub:FamilyThymelaeaceae                 NA    \n## growthformShrub/Tree:FamilyThymelaeaceae            NA    \n## growthformTree:FamilyThymelaeaceae                  NA    \n## growthformHerb:FamilyUlmaceae                       NA    \n## growthformHerb/Shrub:FamilyUlmaceae                 NA    \n## growthformShrub:FamilyUlmaceae                      NA    \n## growthformShrub/Tree:FamilyUlmaceae                 NA    \n## growthformTree:FamilyUlmaceae                       NA    \n## growthformHerb:FamilyUrticaceae                     NA    \n## growthformHerb/Shrub:FamilyUrticaceae               NA    \n## growthformShrub:FamilyUrticaceae                    NA    \n## growthformShrub/Tree:FamilyUrticaceae               NA    \n## growthformTree:FamilyUrticaceae                     NA    \n## growthformHerb:FamilyViolaceae                      NA    \n## growthformHerb/Shrub:FamilyViolaceae                NA    \n## growthformShrub:FamilyViolaceae                     NA    \n## growthformShrub/Tree:FamilyViolaceae                NA    \n## growthformTree:FamilyViolaceae                      NA    \n## growthformHerb:FamilyXanthorrhoeaceae               NA    \n## growthformHerb/Shrub:FamilyXanthorrhoeaceae         NA    \n## growthformShrub:FamilyXanthorrhoeaceae              NA    \n## growthformShrub/Tree:FamilyXanthorrhoeaceae         NA    \n## growthformTree:FamilyXanthorrhoeaceae               NA    \n## growthformHerb:FamilyZygophyllaceae                 NA    \n## growthformHerb/Shrub:FamilyZygophyllaceae           NA    \n## growthformShrub:FamilyZygophyllaceae                NA    \n## growthformShrub/Tree:FamilyZygophyllaceae           NA    \n## growthformTree:FamilyZygophyllaceae                 NA    \n## growthformHerb:lat                            0.420417    \n## growthformHerb/Shrub:lat                            NA    \n## growthformShrub:lat                           0.045980 *  \n## growthformShrub/Tree:lat                      0.360912    \n## growthformTree:lat                                  NA    \n## growthformHerb:long                           0.369731    \n## growthformHerb/Shrub:long                           NA    \n## growthformShrub:long                          0.665856    \n## growthformShrub/Tree:long                           NA    \n## growthformTree:long                                 NA    \n## growthformHerb:NPP                            0.329320    \n## growthformHerb/Shrub:NPP                            NA    \n## growthformShrub:NPP                           0.123317    \n## growthformShrub/Tree:NPP                            NA    \n## growthformTree:NPP                                  NA    \n## FamilyAsteraceae:lat                          0.283138    \n## FamilyAtherospermataceae:lat                        NA    \n## FamilyBalsaminaceae:lat                             NA    \n## FamilyBetulaceae:lat                          0.785252    \n## FamilyBrassicaceae:lat                        0.482497    \n## FamilyCactaceae:lat                           0.046821 *  \n## FamilyCasuarinaceae:lat                             NA    \n## FamilyChloranthaceae:lat                            NA    \n## FamilyChrysobalanaceae:lat                    0.570866    \n## FamilyCistaceae:lat                                 NA    \n## FamilyCornaceae:lat                                 NA    \n## FamilyCrassulaceae:lat                              NA    \n## FamilyCunoniaceae:lat                               NA    \n## FamilyCupressaceae:lat                              NA    \n## FamilyCyperaceae:lat                                NA    \n## FamilyDennstaedtiaceae:lat                          NA    \n## FamilyDicksoniaceae:lat                             NA    \n## FamilyDipterocarpaceae:lat                          NA    \n## FamilyEbenaceae:lat                           0.402213    \n## FamilyElaeocarpaceae:lat                            NA    \n## FamilyEricaceae:lat                           0.223129    \n## FamilyEuphorbiaceae:lat                       0.344510    \n## FamilyFabaceae - C:lat                        0.399668    \n## FamilyFabaceae - M:lat                              NA    \n## FamilyFabaceae - P:lat                        0.339143    \n## FamilyFagaceae:lat                            0.367245    \n## FamilyGentianaceae:lat                        0.426756    \n## FamilyHeliconiaceae:lat                             NA    \n## FamilyJuglandaceae:lat                              NA    \n## FamilyJuncaginaceae:lat                             NA    \n## FamilyLamiaceae:lat                                 NA    \n## FamilyLauraceae:lat                                 NA    \n## FamilyMaesaceae:lat                                 NA    \n## FamilyMalvaceae:lat                           0.451583    \n## FamilyMelastomataceae:lat                           NA    \n## FamilyMoraceae:lat                                  NA    \n## FamilyMyristicaceae:lat                             NA    \n## FamilyMyrsinaceae:lat                               NA    \n## FamilyMyrtaceae:lat                           0.675074    \n## FamilyOchnaceae:lat                                 NA    \n## FamilyOnagraceae:lat                                NA    \n## FamilyOrobanchaceae:lat                       0.476749    \n## FamilyPhyllanthaceae:lat                            NA    \n## FamilyPicrodendraceae:lat                           NA    \n## FamilyPinaceae:lat                            0.159861    \n## FamilyPoaceae:lat                             0.420438    \n## FamilyPolemoniaceae:lat                       0.433041    \n## FamilyPolygonaceae:lat                        0.444087    \n## FamilyProteaceae:lat                          0.149712    \n## FamilyRanunculaceae:lat                             NA    \n## FamilyRhamnaceae:lat                                NA    \n## FamilyRosaceae:lat                            0.160271    \n## FamilyRubiaceae:lat                           0.221123    \n## FamilyRutaceae:lat                                  NA    \n## FamilySalicaceae:lat                          0.240215    \n## FamilySapindaceae:lat                         0.380059    \n## FamilySapotaceae:lat                                NA    \n## FamilyScrophulariaceae:lat                          NA    \n## FamilyThymelaeaceae:lat                             NA    \n## FamilyUlmaceae:lat                            0.282140    \n## FamilyUrticaceae:lat                          0.358817    \n## FamilyViolaceae:lat                                 NA    \n## FamilyXanthorrhoeaceae:lat                          NA    \n## FamilyZygophyllaceae:lat                            NA    \n## FamilyAsteraceae:long                         0.001268 ** \n## FamilyAtherospermataceae:long                       NA    \n## FamilyBalsaminaceae:long                            NA    \n## FamilyBetulaceae:long                         0.374743    \n## FamilyBrassicaceae:long                             NA    \n## FamilyCactaceae:long                          0.155711    \n## FamilyCasuarinaceae:long                            NA    \n## FamilyChloranthaceae:long                           NA    \n## FamilyChrysobalanaceae:long                         NA    \n## FamilyCistaceae:long                                NA    \n## FamilyCornaceae:long                                NA    \n## FamilyCrassulaceae:long                             NA    \n## FamilyCunoniaceae:long                              NA    \n## FamilyCupressaceae:long                             NA    \n## FamilyCyperaceae:long                               NA    \n## FamilyDennstaedtiaceae:long                         NA    \n## FamilyDicksoniaceae:long                            NA    \n## FamilyDipterocarpaceae:long                         NA    \n## FamilyEbenaceae:long                                NA    \n## FamilyElaeocarpaceae:long                           NA    \n## FamilyEricaceae:long                          0.056466 .  \n## FamilyEuphorbiaceae:long                      0.359718    \n## FamilyFabaceae - C:long                             NA    \n## FamilyFabaceae - M:long                             NA    \n## FamilyFabaceae - P:long                             NA    \n## FamilyFagaceae:long                                 NA    \n## FamilyGentianaceae:long                             NA    \n## FamilyHeliconiaceae:long                            NA    \n## FamilyJuglandaceae:long                             NA    \n## FamilyJuncaginaceae:long                            NA    \n## FamilyLamiaceae:long                                NA    \n## FamilyLauraceae:long                                NA    \n## FamilyMaesaceae:long                                NA    \n## FamilyMalvaceae:long                          0.449768    \n## FamilyMelastomataceae:long                          NA    \n## FamilyMoraceae:long                                 NA    \n## FamilyMyristicaceae:long                            NA    \n## FamilyMyrsinaceae:long                              NA    \n## FamilyMyrtaceae:long                          0.129339    \n## FamilyOchnaceae:long                                NA    \n## FamilyOnagraceae:long                               NA    \n## FamilyOrobanchaceae:long                            NA    \n## FamilyPhyllanthaceae:long                           NA    \n## FamilyPicrodendraceae:long                          NA    \n## FamilyPinaceae:long                           0.977523    \n## FamilyPoaceae:long                                  NA    \n## FamilyPolemoniaceae:long                            NA    \n## FamilyPolygonaceae:long                             NA    \n## FamilyProteaceae:long                         0.160569    \n## FamilyRanunculaceae:long                            NA    \n## FamilyRhamnaceae:long                               NA    \n## FamilyRosaceae:long                           0.294900    \n## FamilyRubiaceae:long                                NA    \n## FamilyRutaceae:long                                 NA    \n## FamilySalicaceae:long                               NA    \n## FamilySapindaceae:long                        0.118118    \n## FamilySapotaceae:long                               NA    \n## FamilyScrophulariaceae:long                         NA    \n## FamilyThymelaeaceae:long                            NA    \n## FamilyUlmaceae:long                                 NA    \n## FamilyUrticaceae:long                               NA    \n## FamilyViolaceae:long                                NA    \n## FamilyXanthorrhoeaceae:long                         NA    \n## FamilyZygophyllaceae:long                           NA    \n## FamilyAsteraceae:alt                          0.038360 *  \n## FamilyAtherospermataceae:alt                        NA    \n## FamilyBalsaminaceae:alt                             NA    \n## FamilyBetulaceae:alt                                NA    \n## FamilyBrassicaceae:alt                              NA    \n## FamilyCactaceae:alt                           0.653610    \n## FamilyCasuarinaceae:alt                             NA    \n## FamilyChloranthaceae:alt                            NA    \n## FamilyChrysobalanaceae:alt                          NA    \n## FamilyCistaceae:alt                                 NA    \n## FamilyCornaceae:alt                                 NA    \n## FamilyCrassulaceae:alt                              NA    \n## FamilyCunoniaceae:alt                               NA    \n## FamilyCupressaceae:alt                              NA    \n## FamilyCyperaceae:alt                                NA    \n## FamilyDennstaedtiaceae:alt                          NA    \n## FamilyDicksoniaceae:alt                             NA    \n## FamilyDipterocarpaceae:alt                          NA    \n## FamilyEbenaceae:alt                                 NA    \n## FamilyElaeocarpaceae:alt                            NA    \n## FamilyEricaceae:alt                           0.833476    \n## FamilyEuphorbiaceae:alt                             NA    \n## FamilyFabaceae - C:alt                              NA    \n## FamilyFabaceae - M:alt                              NA    \n## FamilyFabaceae - P:alt                              NA    \n## FamilyFagaceae:alt                                  NA    \n## FamilyGentianaceae:alt                              NA    \n## FamilyHeliconiaceae:alt                             NA    \n## FamilyJuglandaceae:alt                              NA    \n## FamilyJuncaginaceae:alt                             NA    \n## FamilyLamiaceae:alt                                 NA    \n## FamilyLauraceae:alt                                 NA    \n## FamilyMaesaceae:alt                                 NA    \n## FamilyMalvaceae:alt                                 NA    \n## FamilyMelastomataceae:alt                           NA    \n## FamilyMoraceae:alt                                  NA    \n## FamilyMyristicaceae:alt                             NA    \n## FamilyMyrsinaceae:alt                               NA    \n## FamilyMyrtaceae:alt                           0.818358    \n## FamilyOchnaceae:alt                                 NA    \n## FamilyOnagraceae:alt                                NA    \n## FamilyOrobanchaceae:alt                             NA    \n## FamilyPhyllanthaceae:alt                            NA    \n## FamilyPicrodendraceae:alt                           NA    \n## FamilyPinaceae:alt                            0.436576    \n## FamilyPoaceae:alt                             0.673346    \n## FamilyPolemoniaceae:alt                             NA    \n## FamilyPolygonaceae:alt                              NA    \n## FamilyProteaceae:alt                          0.346160    \n## FamilyRanunculaceae:alt                             NA    \n## FamilyRhamnaceae:alt                                NA    \n## FamilyRosaceae:alt                                  NA    \n## FamilyRubiaceae:alt                                 NA    \n## FamilyRutaceae:alt                                  NA    \n## FamilySalicaceae:alt                                NA    \n## FamilySapindaceae:alt                               NA    \n## FamilySapotaceae:alt                                NA    \n## FamilyScrophulariaceae:alt                          NA    \n## FamilyThymelaeaceae:alt                             NA    \n## FamilyUlmaceae:alt                                  NA    \n## FamilyUrticaceae:alt                                NA    \n## FamilyViolaceae:alt                                 NA    \n## FamilyXanthorrhoeaceae:alt                          NA    \n## FamilyZygophyllaceae:alt                            NA    \n## FamilyAsteraceae:temp                         0.004264 ** \n## FamilyAtherospermataceae:temp                       NA    \n## FamilyBalsaminaceae:temp                            NA    \n## FamilyBetulaceae:temp                               NA    \n## FamilyBrassicaceae:temp                             NA    \n## FamilyCactaceae:temp                                NA    \n## FamilyCasuarinaceae:temp                            NA    \n## FamilyChloranthaceae:temp                           NA    \n## FamilyChrysobalanaceae:temp                         NA    \n## FamilyCistaceae:temp                                NA    \n## FamilyCornaceae:temp                                NA    \n## FamilyCrassulaceae:temp                             NA    \n## FamilyCunoniaceae:temp                              NA    \n## FamilyCupressaceae:temp                             NA    \n## FamilyCyperaceae:temp                               NA    \n## FamilyDennstaedtiaceae:temp                         NA    \n## FamilyDicksoniaceae:temp                            NA    \n## FamilyDipterocarpaceae:temp                         NA    \n## FamilyEbenaceae:temp                                NA    \n## FamilyElaeocarpaceae:temp                           NA    \n## FamilyEricaceae:temp                                NA    \n## FamilyEuphorbiaceae:temp                            NA    \n## FamilyFabaceae - C:temp                             NA    \n## FamilyFabaceae - M:temp                             NA    \n## FamilyFabaceae - P:temp                             NA    \n## FamilyFagaceae:temp                                 NA    \n## FamilyGentianaceae:temp                             NA    \n## FamilyHeliconiaceae:temp                            NA    \n## FamilyJuglandaceae:temp                             NA    \n## FamilyJuncaginaceae:temp                            NA    \n## FamilyLamiaceae:temp                                NA    \n## FamilyLauraceae:temp                                NA    \n## FamilyMaesaceae:temp                                NA    \n## FamilyMalvaceae:temp                                NA    \n## FamilyMelastomataceae:temp                          NA    \n## FamilyMoraceae:temp                                 NA    \n## FamilyMyristicaceae:temp                            NA    \n## FamilyMyrsinaceae:temp                              NA    \n## FamilyMyrtaceae:temp                          0.008481 ** \n## FamilyOchnaceae:temp                                NA    \n## FamilyOnagraceae:temp                               NA    \n## FamilyOrobanchaceae:temp                            NA    \n## FamilyPhyllanthaceae:temp                           NA    \n## FamilyPicrodendraceae:temp                          NA    \n## FamilyPinaceae:temp                                 NA    \n## FamilyPoaceae:temp                            0.042787 *  \n## FamilyPolemoniaceae:temp                            NA    \n## FamilyPolygonaceae:temp                             NA    \n## FamilyProteaceae:temp                               NA    \n## FamilyRanunculaceae:temp                            NA    \n## FamilyRhamnaceae:temp                               NA    \n## FamilyRosaceae:temp                                 NA    \n## FamilyRubiaceae:temp                                NA    \n## FamilyRutaceae:temp                                 NA    \n## FamilySalicaceae:temp                               NA    \n## FamilySapindaceae:temp                              NA    \n## FamilySapotaceae:temp                               NA    \n## FamilyScrophulariaceae:temp                         NA    \n## FamilyThymelaeaceae:temp                            NA    \n## FamilyUlmaceae:temp                                 NA    \n## FamilyUrticaceae:temp                               NA    \n## FamilyViolaceae:temp                                NA    \n## FamilyXanthorrhoeaceae:temp                         NA    \n## FamilyZygophyllaceae:temp                           NA    \n## lat:long                                      0.165174    \n## lat:temp                                      0.000384 ***\n## lat:NPP                                       0.376311    \n## long:alt                                      0.555973    \n## long:temp                                     0.016734 *  \n## long:NPP                                      0.015356 *  \n## alt:temp                                      0.006587 ** \n## temp:NPP                                      0.069341 .  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.14 on 10 degrees of freedom\n##   (15 observations deleted due to missingness)\n## Multiple R-squared:  0.9981, Adjusted R-squared:  0.9695 \n## F-statistic: 34.87 on 152 and 10 DF,  p-value: 5.06e-07\n\nR2 = 0.99 … very high. In general, AIC should not overfit. In practice, however, it can overfit if there are unmodelled correlations in the data, or if you use variables that are (indirectly) identical to your response.\n\n\n\n\n\n7.5.2 Exercise: Life Satisfaction #3\n\n\n\n\n\n\nExcercise\n\n\n\nRevisit our previous analysis on life satisfaction in the chapter on causal inference. Now, build a predictive model of life satistisfaction. If you want, you can also tune this model in a validation or cross-validation split.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTODO"
  },
  {
    "objectID": "4A-GLMs.html",
    "href": "4A-GLMs.html",
    "title": "8  GL(M)Ms",
    "section": "",
    "text": "Generalized linear models (GLMs) extend the linear model (LM) to other (i.e. non-normal) distributions. The idea is the following:\n\nWe want to keep the regression formula \\(y \\sim f(x)\\) of the lm with all it’s syntax, inlcuding splines, random effects and all that. In the GLM world, this is called the “linear predictor”.\nHowever, in the GLM, we now allow other residual distributions than normal, e.g. binomial, Poisson, Gamma, etc.\nSome families require a particular range of the predictions (e.g. binomial requires predictions between zero and one). To achieve this, we use a so-called link function to bring the results of the linear predictor to the desired range.\n\nIn R, the distribution is specified via the family() function, which distinguishes the glm from the lm function. If you look at the help of family, you will see that the link function is an argument of family(). If no link is provided, the default link for the respective family is chosen.\n\n?family\n\nA full GLM structure is thus:\n\\[\ny \\sim family[link^{-1}(f(x) + RE)]\n\\]\n\n\n\n\n\n\nNote\n\n\n\nAs you see, in noted the link function as \\(link^{-1}\\) in this equation. The reason is that traditionally, the link function is applied to the left hand side of the equation, i.e.\n\\[\nlink(y) \\sim x\n\\]\nThis is important to keep in mind when interpreting the names of the link function - the log link, for example, means that \\(log(y) = x\\), which actually means that we assume \\(y = exp(x)\\), i.e. the result of the linear predictor enters the exponential function, which assures that we have strictly positive predictions.\n\n\nThe function \\(f(x)\\) itself can have all components that we discussed before, in particular\n\nYou can add random effects as before (using functions lme4::glmer or glmmTMB::glmmTMB)\nYou can also use splines using mgcv::gam"
  },
  {
    "objectID": "4A-GLMs.html#important-glm-variants",
    "href": "4A-GLMs.html#important-glm-variants",
    "title": "8  GL(M)Ms",
    "section": "8.2 Important GLM variants",
    "text": "8.2 Important GLM variants\nThe most important are\n\nBernoulli / Binomial family = logistic regression with logit link\nPoisson family = Poisson regression with log link\nBeta regresion for continous 0/1 data\n\nOf course, there are many additional distributions that you could consider for your response. Here an overview of the common choices:\n\n\n\n\n\nScreenshot taken from Wikipedia: https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function. Content licensed under the Creative Commons Attribution-ShareAlike License 3.0.\n\n8.2.1 Count data - Poisson regression\nThe standard model for count data (1,2,3) is the Poisson regression, which implements\n\nA log-link function -> \\(y = exp(ax + b)\\)\nA Poisson distribution\n\nAs an example, we use the data set birdfeeding from the EcoData package - the dataset consists of observations of foods given to nestlings to parents as a function of the attractiveness of the nestling.\n\nlibrary(EcoData)\n#str(birdfeeding)\nplot(feeding ~ attractiveness, data = birdfeeding)\n\n\n\n\nTo fit a Poisson regression to this data, we use the glm() function, specifying family = \"poisson\". Note again that the log link function is the default (see ?family), so it does not have to be specified.\n\nfit = glm(feeding ~ attractiveness, data = birdfeeding, family = \"poisson\")\nsummary(fit)\n\n\nCall:\nglm(formula = feeding ~ attractiveness, family = \"poisson\", data = birdfeeding)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.55377  -0.72834   0.03699   0.59093   1.54584  \n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)     1.47459    0.19443   7.584 3.34e-14 ***\nattractiveness  0.14794    0.05437   2.721  0.00651 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 25.829  on 24  degrees of freedom\nResidual deviance: 18.320  on 23  degrees of freedom\nAIC: 115.42\n\nNumber of Fisher Scoring iterations: 4\n\n\nThe output is very similar to the lm(), however, as the residuals are not any more assumed to scatter normally, all statistics based on R2 have been replaced by the deviance (deviance = - 2 logL(saturated) - logL(fitted)). So, we have\n\nDeviance residuals on top\nInstead of R2, we get null vs. residual deviance, and AIC. Based on the deviance, we can calculate a pseudo R2, e.g. McFadden, which is 1-[LogL(M)/LogL(M0))]\n\n\n\n\n\n\n\nNote\n\n\n\nAs already mentioned, the deviance is a generalization of the sum of squares and plays, for example, a similar role as the residual sum of squares in ANOVA.\nThe deviance of our model M1 is defined as \\(Deviance_{M_1} = 2 \\cdot (logL(M_{saturated}) - logL(M_1))\\)\nExample:\nDeviance of M1\n\nfit1 = glm(feeding ~ attractiveness, data = birdfeeding, family = \"poisson\")\nsummary(fit1)$deviance\n\n[1] 18.32001\n\nfitSat = glm(feeding ~ as.factor(1:nrow(birdfeeding)), data = birdfeeding, family = \"poisson\")\n2*(logLik(fitSat) - logLik(fit1))\n\n'log Lik.' 18.32001 (df=25)\n\n\nDeviance of the null model (null model is for example needed to calculate the pseudo-R2 ):\n\nsummary(fit1)$null.deviance\n\n[1] 25.82928\n\nfitNull = glm(feeding ~ 1, data = birdfeeding, family = \"poisson\")\n2*(logLik(fitSat) - logLik(fitNull))\n\n'log Lik.' 25.82928 (df=25)\n\n\nCalculation of Pseudo-R2 with deviance or logL\nCommon pseudo-R2 such as McFadden or Nagelkerke use the the logL instead of the deviance, whereas the pseudo-R2 by Cohen uses the deviance. However, R2 calculated based on deviance or logL differs considerably, as the former is calculated in reference to the maximal achievable fit (saturated model) while the latter just compares the improvement of fit compared to the null model:\n\n# Based on logL\nMcFadden = 1-(logLik(fit1))/(logLik(fitNull))\nprint(McFadden)\n\n'log Lik.' 0.06314239 (df=2)\n\n# Based on deviance\nCohen = 1- (logLik(fitSat) - logLik(fit1)) / (logLik(fitSat) - logLik(fitNull))\nprint(Cohen)\n\n'log Lik.' 0.2907272 (df=25)\n\n\nNote: Unfortunately, there is some confusion about the exact definition, as deviance is sometimes defined (especially outside of the context of GL(M)Ms) simply as \\(Deviance = -2LogL(M_1)\\)\n\n\nIf we want to calculate model predictions, we have to transform to the response scale. Here we have a log link, i.e. we have to transform with exp(linear response).\n\nexp(1.47459 + 3 * 0.14794)\n\n[1] 6.810122\n\n\nAlternatively (and preferably), you can use the predict() function with type = \"response\"\n\ndat = data.frame(attractiveness = 3)\npredict(fit, newdata = dat) # linear predictor\n\n       1 \n1.918397 \n\npredict(fit, newdata = dat, type = \"response\") # response scale\n\n       1 \n6.810034 \n\n\nEffect plots work as before. Note that the effects package always transforms the y axis according to the link, so we have log scaling on the y axis, and the effect lines remain straight\n\nlibrary(effects)\nplot(allEffects(fit))\n\n\n\n\n\n8.2.1.1 Notes on the Poisson regression\nPoisson vs. log transformed count data: For count data, even if the distribution is switched, the log link is nearly always the appropriate link function. Before GLMs were widely available, was common to it lm with log transformed counts, which is basically log link + normal distribution\nLog offset: If there is a variable that that controls the number of counts (e.g. time, area), this variable is usually added as a offset in the following form\n\nfit = glm(y ~ x + offset(log(area)), family = \"poisson\")\n\nAs the log-link connects the linear predictor as in \\(y = exp(x)\\), and \\(exp(x + log(area)) = exp(x) \\cdot area\\), this makes the expected counts proportional to area, or whatever variable is added as a log offset.\nInteractions: As for all GLMs with nonlinear link functions, interpretation of the interactions is more complicated. See notes on this below.\n\n\n\n8.2.2 0/1 or k/n data - logistic regression\nThe standard model to fit binomial (0/1 or k/n) data is the logistic regression, which combines the binomial distribution with a logit link function. To get to know this model, let’s have a look at the titanic data set in EcoData:\n\nlibrary(EcoData)\n#str(titanic)\n#mosaicplot( ~ survived + sex + pclass, data = titanic)\ntitanic$pclass = as.factor(titanic$pclass)\n\nWe want to analyze how survival in the titanic accident depended on other predictors. We could fit an lm, but the residual checks make it very evident that the data with a 0/1 response don’t fit to the assumption of an lm:\n\nfit = lm(survived ~ sex * age, data = titanic)\nsummary(fit)\n\n\nCall:\nlm(formula = survived ~ sex * age, data = titanic)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8901 -0.2291 -0.1564  0.2612  0.9744 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.637645   0.046165  13.812  < 2e-16 ***\nsexmale     -0.321308   0.059757  -5.377 9.35e-08 ***\nage          0.004006   0.001435   2.792  0.00534 ** \nsexmale:age -0.007641   0.001823  -4.192 3.01e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4115 on 1042 degrees of freedom\n  (263 observations deleted due to missingness)\nMultiple R-squared:  0.3017,    Adjusted R-squared:  0.2997 \nF-statistic:   150 on 3 and 1042 DF,  p-value: < 2.2e-16\n\npar(mfrow = c(2, 2))\nplot(fit)\n\n\n\n\nThus, let’s move to the logistic regression, which assumes a 0/1 response + logit link. In principle, this is distribution is called Bernoulli, but in R both 0/1 and k/n are called “binomial”, as Bernoulli is the special case of binomial where n = 1.\n\nm1 = glm(survived ~ sex*age, family = \"binomial\", data = titanic)\nsummary(m1)\n\n\nCall:\nglm(formula = survived ~ sex * age, family = \"binomial\", data = titanic)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0247  -0.7158  -0.5776   0.7707   2.2960  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.493381   0.254188   1.941 0.052257 .  \nsexmale     -1.154139   0.339337  -3.401 0.000671 ***\nage          0.022516   0.008535   2.638 0.008342 ** \nsexmale:age -0.046276   0.011216  -4.126 3.69e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1414.6  on 1045  degrees of freedom\nResidual deviance: 1083.4  on 1042  degrees of freedom\n  (263 observations deleted due to missingness)\nAIC: 1091.4\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe syntax here is for 0/1 data. If you have k/n data, you can either specify the response as cbind(k, n-k), or you can fit the glm with k ~ x, weights = n\n\n\nThe interpretation of the regression table remains unchanged. To transform to predictions, we have to use the inverse logit, which is in R:\n\nplogis(0.493381 + 0.022516 * 20)  # Women, age 20.\n\n[1] 0.7198466\n\nplogis(0.493381 -1.154139 + 20*(0.022516-0.046276)) # Men, age 20\n\n[1] 0.2430632\n\n\nAlternatively, we can again use the predict function\n\nnewDat = data.frame(sex = as.factor(c(\"female\", \"male\")), age = c(20,20))\npredict(m1, newdata = newDat) # Linear predictor.\n\n         1          2 \n 0.9436919 -1.1359580 \n\npredict(m1, newdata = newDat, type = \"response\")  # Response scale.\n\n        1         2 \n0.7198448 0.2430633 \n\n\nFinally, the effect plots - note again the scaling of the y axis, which is now logit.\n\nlibrary(effects)\nplot(allEffects(m1))\n\n\n\n\nIf you do an ANOVA on a glm, you should take care that you perform a Chisq and not an F-test. You notice that you have the right one if the ANOVA doesn’t supply SumSq statistics, but deviance. In the anova function, we have to set this by hand\n\nanova(m1, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: survived\n\nTerms added sequentially (first to last)\n\n        Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                     1045     1414.6              \nsex      1  312.612      1044     1102.0 < 2.2e-16 ***\nage      1    0.669      1043     1101.3    0.4133    \nsex:age  1   17.903      1042     1083.4 2.324e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nnote that anova works for glm and glmmTMB, but not for lme4::glmer.\nThe function car::Anova works for all models and uses a ChiSq test automatically. Given that you probably want to use a type II or III Anova anyway, you should prefer it.\n\ncar::Anova(m1)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: survived\n        LR Chisq Df Pr(>Chisq)    \nsex      310.044  1  < 2.2e-16 ***\nage        0.669  1     0.4133    \nsex:age   17.903  1  2.324e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nOf course, when using this with random effects, the caveats that we discussed when introducing random effects apply: this function does not account for changes in the degrees of freedom created by changing the fixed effect structure. Something like the lmerTest package which uses a df approximation does not exist for GLMMs. Thus, the values that you obtain here are calculated under the assumption that the RE structure is the same.\nIf you see large changes in your RE structure, or if you want to select on the REs, you can move to a simulated LRT.\n\n8.2.2.1 Notes on the logistic regression\nOffset: there is no exact solution for making 0/1 data dependent on a scaling factor via an offset, which is often desirable, for example in the context of survival analysis with different exposure times. An approximate solution is to use an offset together with the log-log link (instead of logit).\nInteractions: As for all GLMs with nonlinear link functions, interpretation of the interactions is more complicated. See notes in this below.\nOverdispersion: 0/1 poisson responses cannot be overdispersed, but k/n responses can be. However, 0/1 responses can show overdispersion if grouped to k/n. Note next section on residual checks, as well as comments on testing binomial GLMs in the\nDHARMa vignette.\n\n\n\n\n\n\nExample - Elk Data\n\n\n\nYou will be given a data set of habitat use of Elks in Canada. Measured is the presence of Elks (0/1), and a number of other predictors. Description of variables:\n\ndist_roads - distance of the location to the next road\nNDVI - normalized difference vegetation index, essentially greeness of vegetation on the site\nruggedness of the terrain\ndem - digital eleveation model = elevation above sea level\npresence - presence of the elk\nhabitat - open or forest\n\nPerform either:\n\nA predictive analysis, i.e. a model to predict where Elks can be found.\nA causal analysis, trying to understand the effect of roads on Elk presence.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA. Predictive analysis\n\nload(file = \"hiddenData/elk_data.RData\")\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:EcoData':\n\n    snails\n\nfit <- glm(presence ~ dist_roads  + dem + ruggedness, data = elk_data, family = \"binomial\")\npredictive_model = MASS::stepAIC(fit, direction = \"both\")\n\nStart:  AIC=5109.03\npresence ~ dist_roads + dem + ruggedness\n\n             Df Deviance    AIC\n- dist_roads  1   5101.9 5107.9\n<none>            5101.0 5109.0\n- ruggedness  1   5171.4 5177.4\n- dem         1   5241.3 5247.3\n\nStep:  AIC=5107.94\npresence ~ dem + ruggedness\n\n             Df Deviance    AIC\n<none>            5101.9 5107.9\n+ dist_roads  1   5101.0 5109.0\n- ruggedness  1   5172.0 5176.0\n- dem         1   5324.8 5328.8\n\nsummary(predictive_model)\n\n\nCall:\nglm(formula = presence ~ dem + ruggedness, family = \"binomial\", \n    data = elk_data)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.97202  -1.10220   0.04971   1.13879   1.74823  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -6.7890658  0.4917287 -13.807   <2e-16 ***\ndem          0.0042951  0.0002994  14.343   <2e-16 ***\nruggedness  -0.0289100  0.0035076  -8.242   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5334.5  on 3847  degrees of freedom\nResidual deviance: 5101.9  on 3845  degrees of freedom\nAIC: 5107.9\n\nNumber of Fisher Scoring iterations: 4\n\n\nB. Causal analysis\nThe predictive model has actually dropped the variable of interest (distance to roads) which shows the risks of tools that select for the best predictive model such as AIC selection: Collinear variables that we need to adjust our effects, are often dropped.\nFor the causal model, we really need to think about the causal relationships between the variables:\nWe are interested in the effect of dist_roads on presence:\n\nsummary(glm(presence ~ dist_roads, data = elk_data, family = \"binomial\"))\n\n\nCall:\nglm(formula = presence ~ dist_roads, family = \"binomial\", data = elk_data)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.56301  -1.14680  -0.09152   1.17040   1.35579  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -4.101e-01  6.026e-02  -6.806  1.0e-11 ***\ndist_roads   3.204e-04  3.977e-05   8.056  7.9e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5334.5  on 3847  degrees of freedom\nResidual deviance: 5268.1  on 3846  degrees of freedom\nAIC: 5272.1\n\nNumber of Fisher Scoring iterations: 4\n\n\nPositive effect of dist_roads on elk, or in other words, more elks closer to the roads? Does that make sense? No, we expect a negative effect!\nAltitude (dem) and the ruggedness probably affect both variables, presence and dist_roads, and thus they should be considered as confounders:\n\nfit = glm(presence ~ dist_roads+ dem + ruggedness, data = elk_data, family = \"binomial\")\n\nThe effect of dist_roads is now negative!\nLet’s check the residuals:\n\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nres <- simulateResiduals(fit, plot = TRUE)\n\n\n\nplot(res, quantreg = TRUE)\n\n\n\nplotResiduals(res, form = elk_data$dem, quantreg = TRUE)\n\n\n\nplotResiduals(res, form = elk_data$ruggedness, quantreg = TRUE)\n\n\n\n\nThe functional forms of our confounders are not perfect.\nSince we are not really interested in them, a cool trick is to use a GAM which automatically adjusts the functional for of the fitted curve to flexibly take care of the confounders. Our main predictor dist_roads is still modelled as a linear effect.\n\nlibrary(mgcv)\n\nLoading required package: nlme\n\n\nThis is mgcv 1.8-40. For overview type 'help(\"mgcv-package\")'.\n\nfit2 <- gam(presence ~ dist_roads + s(dem) + s(ruggedness), data = elk_data, family = \"binomial\")\nsummary(fit2)\n\n\nFamily: binomial \nLink function: logit \n\nFormula:\npresence ~ dist_roads + s(dem) + s(ruggedness)\n\nParametric coefficients:\n              Estimate Std. Error z value Pr(>|z|)   \n(Intercept)  1.783e-01  8.229e-02   2.167  0.03025 * \ndist_roads  -1.798e-04  5.771e-05  -3.115  0.00184 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                edf Ref.df Chi.sq p-value    \ns(dem)        8.283  8.845  220.3  <2e-16 ***\ns(ruggedness) 8.510  8.918  128.3  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.114   Deviance explained = 9.27%\nUBRE = 0.26754  Scale est. = 1         n = 3848\n\n\nLet’s take another look at the residual plots, in particular for the confounders.\n\nres <- simulateResiduals(fit2, plot = TRUE)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nRegistered S3 method overwritten by 'mgcViz':\n  method from  \n  +.gg   GGally\n\n\n\n\nplot(res, quantreg = TRUE)\n\n\n\nplotResiduals(res, form = elk_data$dem, quantreg = TRUE)\n\n\n\nplotResiduals(res, form = elk_data$ruggedness, quantreg = TRUE)\n\n\n\n\nNow, everything looks perfect\n\n\n\n\n\n\n8.2.3 Continous positive response - Gamma regression\n\nlibrary(faraway)\n\n\nAttaching package: 'faraway'\n\n\nThe following objects are masked from 'package:EcoData':\n\n    melanoma, rats\n\n\n\nfit <- lm(log(resist) ~ x1 + x2 + x3 + x4, data = wafer)\nsummary(fit)\n\n\nCall:\nlm(formula = log(resist) ~ x1 + x2 + x3 + x4, data = wafer)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.17572 -0.06222  0.01749  0.08765  0.10841 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.44048    0.05982  90.948  < 2e-16 ***\nx1+          0.12277    0.05350   2.295 0.042432 *  \nx2+         -0.29986    0.05350  -5.604 0.000159 ***\nx3+          0.17844    0.05350   3.335 0.006652 ** \nx4+         -0.05615    0.05350  -1.049 0.316515    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.107 on 11 degrees of freedom\nMultiple R-squared:  0.8164,    Adjusted R-squared:  0.7496 \nF-statistic: 12.22 on 4 and 11 DF,  p-value: 0.0004915\n\nlibrary(DHARMa)\nsimulateResiduals(fit, plot = T)\n\nWe had to increase `err` for some of the quantiles. See fit$calibr$err\nWe had to increase `err` for some of the quantiles. See fit$calibr$err\nWe had to increase `err` for some of the quantiles. See fit$calibr$err\n\n\n\n\n\nObject of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. \n \nScaled residual values: 0.052 0.272 0.408 0.676 0.82 0.768 0.84 0.172 0.524 0.72 0.592 0.808 0.8 0.204 0.096 0.372\n\n\nAlternative: Gamma regresision\n\nfit <- glm(formula = resist ~ x1 + x2 + x3 + x4,\n           family  = Gamma(link = \"log\"),\n           data    = wafer)\nsummary(fit)\n\n\nCall:\nglm(formula = resist ~ x1 + x2 + x3 + x4, family = Gamma(link = \"log\"), \n    data = wafer)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.17548  -0.06486   0.01423   0.08399   0.10898  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.44552    0.05856  92.983  < 2e-16 ***\nx1+          0.12115    0.05238   2.313 0.041090 *  \nx2+         -0.30049    0.05238  -5.736 0.000131 ***\nx3+          0.17979    0.05238   3.432 0.005601 ** \nx4+         -0.05757    0.05238  -1.099 0.295248    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Gamma family taken to be 0.01097542)\n\n    Null deviance: 0.69784  on 15  degrees of freedom\nResidual deviance: 0.12418  on 11  degrees of freedom\nAIC: 152.91\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor Gamma regression, different link functions are used. The canonical link, which is also the default in R, is the inverse link 1/x. However, also log and identity link are commonly used.\n\n\n\n\n8.2.4 Continous proportions - Beta regression and other options\nWe already covered discrete proportions, which can be modelled with a logistic regression. For continous proportions, this model is not suitable, because we don’t know how many “trials” we have. There are a few other options to model this data, in particular the beta regression. Let’s have a look.\nThere are two main ways to fit this data:\n\nTransform the response, or fit the GLM with a logit link or the arcsine transformation on the response\nA Beta regression\n\nFor further options, see here. I would generally recommend that the Beta regression is currently the preferred way and the first thing I would try. A good way to fit it is the package glmmTMB.\n\n\n\n\n\n\nCase study\n\n\n\nHere a case study, using the EcoData dataset\n\n?elemental\n\ntraditional lm, arc sine transformed response of proportions, see critique in https://esajournals.onlinelibrary.wiley.com/doi/10.1890/10-0340.1\n\nm1 <- lm(N_arc ~ Year + Site , data = elemental[elemental$Species == \"ABBA\", ])\nsummary(m1)\n\n\nCall:\nlm(formula = N_arc ~ Year + Site, data = elemental[elemental$Species == \n    \"ABBA\", ])\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.016553 -0.006151 -0.001069  0.004322  0.028149 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.719e+01  4.323e+00  -3.975 0.000127 ***\nYear         8.570e-03  2.145e-03   3.996 0.000117 ***\nSiteDP       3.880e-03  5.972e-03   0.650 0.517275    \nSiteTN       1.043e-02  6.191e-03   1.685 0.094856 .  \nSiteUNI      4.646e-04  5.971e-03   0.078 0.938124    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.00826 on 109 degrees of freedom\nMultiple R-squared:  0.2968,    Adjusted R-squared:  0.271 \nF-statistic:  11.5 on 4 and 109 DF,  p-value: 7.936e-08\n\n\nNow a beta regression\n\nlibrary(glmmTMB)\n\nWarning in checkMatrixPackageVersion(): Package version inconsistency detected.\nTMB was built with Matrix version 1.4.1\nCurrent Matrix version is 1.5.4.1\nPlease re-install 'TMB' from source using install.packages('TMB', type = 'source') or ask CRAN for a binary version of 'TMB' matching CRAN's 'Matrix' package\n\n\nWarning in checkDepPackageVersion(dep_pkg = \"TMB\"): Package version inconsistency detected.\nglmmTMB was built with TMB version 1.9.3\nCurrent TMB version is 1.9.1\nPlease re-install glmmTMB from source or restore original 'TMB' package (see '?reinstalling' for more information)\n\nm2 <- glmmTMB(N_dec ~ Year + Site, family = beta_family, data = elemental[elemental$Species == \"ABBA\", ] )\nsummary(m2)\n\n Family: beta  ( logit )\nFormula:          N_dec ~ Year + Site\nData: elemental[elemental$Species == \"ABBA\", ]\n\n     AIC      BIC   logLik deviance df.resid \n -1148.0  -1131.6    580.0  -1160.0      108 \n\n\nDispersion parameter for beta family (): 3.9e+03 \n\nConditional model:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -361.33063   82.66891  -4.371 1.24e-05 ***\nYear           0.17684    0.04101   4.313 1.61e-05 ***\nSiteDP         0.08905    0.12851   0.693   0.4883    \nSiteTN         0.22115    0.13190   1.677   0.0936 .  \nSiteUNI        0.01559    0.12865   0.121   0.9035    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "4A-GLMs.html#residual-and-their-solutions-in-glms",
    "href": "4A-GLMs.html#residual-and-their-solutions-in-glms",
    "title": "8  GL(M)Ms",
    "section": "8.3 Residual and their solutions in GLMs",
    "text": "8.3 Residual and their solutions in GLMs\nFirst of all: everything we said about model selection and residual checks for LMs also apply for GLMs, with only very few additions, so you should check your model in principle as before. However, there are a few tweaks you have to be aware of.\nLet’s look again at the titanic example\n\nm1 = glm(survived ~ sex*age, family = \"binomial\", data = titanic)\n\nHow can we check the residuals of this model? Due to an unfortunate programming choice in R (Nerds: Check class(m1)), the standard residual plots still work\n\npar(mfrow = c(2, 2))\nplot(m1)\n\n\n\n\nbut they look horrible, because they still check for normality of the residuals, while we are interested in the question of whether the residuals are binomially distributed.\n\n8.3.1 DHARMA residual plots for GL(M)Ms\nThe DHARMa package that we already introduced solves this problem\n\nlibrary(DHARMa)\nres = simulateResiduals(m1)\n\nStandard plot:\n\nplot(res)\n\n\n\n\nOut of the help page: The function creates a plot with two panels. The left panel is a uniform Q-Q plot (calling plotQQunif), and the right panel shows residuals against predicted values (calling plotResiduals), with outliers highlighted in red.\nVery briefly, we would expect that a correctly specified model shows:\n\nA straight 1-1 line, as well as not significant of the displayed tests in the Q-Q-plot (left) -> Evidence for a correct overall residual distribution (for more details on the interpretation of this plot, see help).\nVisual homogeneity of residuals in both vertical and horizontal direction, as well as no significance of quantile tests in the Residual vs. predicted plot (for more details on the interpretation of this plot, see help).\n\nDeviations from these expectations can be interpreted similarly to a linear regression. See the vignette for detailed examples.\nWith that in mind, we can say that there is nothing special to see here. Also residuals against predictors shows no particular problem:\n\npar(mfrow = c(1, 2))\nplotResiduals(m1, form = model.frame(m1)$age)\nplotResiduals(m1, form = model.frame(m1)$sex)\n\n\n\n\nHowever, residuals against the missing predictor pclass show a clear problem:\n\ndataUsed = as.numeric(rownames(model.frame(m1)))\nplotResiduals(m1, form = titanic$pclass[dataUsed])\n\n\n\n\nThus, I should add passenger class to the model\n\nm2 = glm(survived ~ sex*age + pclass, family = \"binomial\", data = titanic)\nsummary(m2)\n\n\nCall:\nglm(formula = survived ~ sex * age + pclass, family = \"binomial\", \n    data = titanic)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3844  -0.6721  -0.4063   0.7041   2.5440  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.790839   0.362822   7.692 1.45e-14 ***\nsexmale     -1.029755   0.358593  -2.872  0.00408 ** \nage         -0.004084   0.009461  -0.432  0.66598    \npclass2     -1.424582   0.241513  -5.899 3.67e-09 ***\npclass3     -2.388178   0.236380 -10.103  < 2e-16 ***\nsexmale:age -0.052891   0.012025  -4.398 1.09e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1414.62  on 1045  degrees of freedom\nResidual deviance:  961.92  on 1040  degrees of freedom\n  (263 observations deleted due to missingness)\nAIC: 973.92\n\nNumber of Fisher Scoring iterations: 5\n\nplotResiduals(m2, form = model.frame(m2)$pclass)\n\n\n\n\nNow, residuals look fine. Of course, if your model gets more complicated, you may want to do additional checks, for example for the distribution of random effects etc.\n\n\n8.3.2 Dispersion Problems in GLMs\nOne thing that is different between GLMs and LM is that GLMs can display overall dispersion problems. The most common GLMs to show overdispersion are the Poisson and the logistic regression.\nThe reason is that simple GLM distributions such as the Poisson or the Binomial (for k/n data) do not have a parameter for adjusting the spread of the observed data around the regression line (dispersion), but their variance is a fixed as function of the mean.\nThere are good reasons for why this is the case (Poisson and Binomial describe particular processes, e.g. coin flip, for which the variance is a fixed function of the mean), but the fact is that when applying these GLMs on real data, we often find overdispersion (more dispersion than expected), and more rarely, underdispersion (less dispersion than expected).\nTo remove the assumptions of a fixed dispersion, there are three options, of which you should definitely take the third one:\n\nQuasi-distributions, which are available in glm. Those add a term to the likelihood that corrects the p-values for the dispersion, but they are not distributions .-> Can’t check residuals, no AIC. -> Discouraged.\nObservation-level random effect (OLRE) - Add a separate random effect per observation. This effectively creates a normal random variate at the level of the linear predictor, increases variance on the responses.\nA GLM distribution with variable dispersion, for Poisson usually the negative binomial.\n\nThe reason why we should prefer the 3rd option is that it allows better residual checks and to model the dispersion as a function of the predictors, see next section.\n\n\n\n\n\n\nNote\n\n\n\nOverdispersion is often created by model misfit. Thus, before moving to a variable dispersion GLM, you should check for / correct model misfit.\n\n\n\n8.3.2.1 Recognizing overdispersion\nTo understand how to recognize overdispersion, let’s look at an example. We’ll use the Salamanders dataset from the package glmmTMB, staring with a simple Poisson glm:\n\nlibrary(glmmTMB)\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'lme4'\n\n\nThe following object is masked from 'package:nlme':\n\n    lmList\n\nlibrary(DHARMa)\n\nm1 = glm(count ~ spp + mined, family = poisson, data = Salamanders)\n\nOverdispersion will be automatically highlighted in the standard DHARMa plots\n\nres = simulateResiduals(m1, plot = T)\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\n\nYou see the dispersion problem by:\n\nDispersion test in the left plot significant\nQQ plot S-shaped\nQuantile lines in the right plots outside their expected quantiles\n\nYou can get a more detailed output with the testDispersion function, which also displays the direction of the dispersion problem (over or underdispersion)\n\ntestDispersion(res)\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 3.9152, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\n\nOK, often the dispersion problem is caused by structural problems. Let’s add a random effect for site, which makes sense. We can do so using the function lme4::glmer, which adds the same extensions to glm as lmer adds to lm. This means we can use the usual random effect syntax we have used before.\n\nm2 = glmer(count ~ spp + mined + (1|site), \n           family = poisson, data = Salamanders)\n\nThe standard dispersion test is OK\n\nres = simulateResiduals(m2, plot = T)\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\n\nBut when random effects are added, you should prefer to calcualte conditional residuals, because this test is more powerful. For lme4 models, we can switch via re.form = T\n\nres = simulateResiduals(m2, plot = T, re.form = NULL)\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\n\nThis test shows that there is still some overdispersion. Actually, what the plots also show is heteroskedasticity, and we should probably deal with that as well, but we will only learn how in the next chapter. For now, let’s switch to a negative binomial model. This could be fit with lme4, but it is more convenient to use the package glmmTMB, which has the same syntax as lme4, but more advanced options.\n\nm4 = glmmTMB(count ~ spp + mined + (1|site), family = nbinom2, data = Salamanders)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\nres = simulateResiduals(m4, plot = T)\n\n\n\n\nUnfortunately, glmmTMB doesn’t allow to calculate conditional residuals, so we have to be satisfied with the fact that the unconditional residuals look great.\n\n\n\n8.3.3 Zero-inflation\nAnother common problem in count data (Poisson / negative binomial), but also other GLMs (e.g. binomial, beta) is that the observed data has more zeros than expected by the fitted distribution. For the beta, 1-inflation, and for the k/n binomial, n-inflation is also common, and tested for / addressed in the same way.\nTo deal with this zero-inflation, one usually adds an additional model component that controls how many zeros are produced. The default way to do this is assuming two separate processes which act after one another:\n\nFirst, we have the normal GLM, predicting what values we would expect\nOn top of that, we have a logistic regression, which decides whether the GLM prediction or a zero should be observed\n\nNote that the result of 1 can also be zero, so there are two explanations for a zero in the data. Zero-inflated GLMMs can, for example, be fit with glmmTMB, using the ziformula argument.\n\n8.3.3.1 Recognizing zero-inflation\n\n\n\n\n\n\nDanger\n\n\n\nThe fact that you have a lot of zeros in your data does not indicate zero-inflation. Zero-inflation is with respect to the fitted model. You can only check for zero-inflation after fitting a model.\n\n\nLet’s look at our last model - DHARMa has a special function to check for zero-inflation\n\ntestZeroInflation(res)\n\n\n\n\n\n    DHARMa zero-inflation test via comparison to expected zeros with\n    simulation under H0 = fitted model\n\ndata:  simulationOutput\nratioObsSim = 1.0172, p-value = 0.744\nalternative hypothesis: two.sided\n\n\nThis shows no sign of zero-inflation. There are, however, two problems with this test:\n\nglmmTMB models only allow unconditional residuals, which means that dispersion and zero-inflation tests are less powerfull\nWhen there is really zero-inflation, variable dispersion models such as the negative Binomial often simply increase the dispersion to account for the zeros, leading to no apparent zero-inflation in the residuals, but rather underdispersion.\n\nThus, for zero-inflation, model selection, or simply fitting a ZIP model is often more reliable than residual checks. You can compare a zero-inflation model via AIC or likelihood ratio test to your base model, or simply check if the ZIP term in glmmTMB is significant.\n\nm5 = glmmTMB(count ~ spp + mined + (1|site), family = nbinom2, ziformula = ~1,  data = Salamanders)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\nsummary(m5)\n\n Family: nbinom2  ( log )\nFormula:          count ~ spp + mined + (1 | site)\nZero inflation:         ~1\nData: Salamanders\n\n     AIC      BIC   logLik deviance df.resid \n  1674.4   1723.5   -826.2   1652.4      633 \n\nRandom effects:\n\nConditional model:\n Groups Name        Variance Std.Dev.\n site   (Intercept) 0.2944   0.5426  \nNumber of obs: 644, groups:  site, 23\n\nDispersion parameter for nbinom2 family (): 0.942 \n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -1.6832     0.2742  -6.140 8.28e-10 ***\nsppPR        -1.3197     0.2875  -4.591 4.42e-06 ***\nsppDM         0.3686     0.2235   1.649 0.099047 .  \nsppEC-A      -0.7098     0.2530  -2.806 0.005016 ** \nsppEC-L       0.5714     0.2191   2.608 0.009105 ** \nsppDES-L      0.7929     0.2166   3.660 0.000252 ***\nsppDF         0.3120     0.2329   1.340 0.180329    \nminedno       2.2633     0.2838   7.975 1.53e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nZero-inflation model:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)   -16.41    4039.11  -0.004    0.997\n\n\nIn this case, we have no evidence for zero-inflation. To see an example where you can find zero-inflation, do the Owl case study below."
  },
  {
    "objectID": "4A-GLMs.html#interpreting-interactions-in-glms",
    "href": "4A-GLMs.html#interpreting-interactions-in-glms",
    "title": "8  GL(M)Ms",
    "section": "8.4 Interpreting interactions in GLMs",
    "text": "8.4 Interpreting interactions in GLMs\nA significant problem with interpreting GLMs is the interpretation of slopes in the presence of other variables, in particular interactions. To understand this problem, let’s first confirm to ourselves: if we simulate data under the model assumptions, parameters will be recovered as expected.\n\nlibrary(effects)\nset.seed(123)\ntrt = as.factor(sample(c(\"ctrl\", \"trt\"), 5000, replace= T))\nconcentration =  runif(5000)\n\nresponse = plogis(0 + 1 * (as.numeric(trt) - 1) + 1*concentration)\nsurvival = rbinom(5000, 1, prob = response)\n\ndat = data.frame(trt = trt, \n                 concentration = concentration,\n                 survival = survival)\n\nm1 = glm(survival ~ trt * concentration, data = dat, family = \"binomial\")\nsummary(m1)\n\n\nCall:\nglm(formula = survival ~ trt * concentration, family = \"binomial\", \n    data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0767  -1.2595   0.6357   0.8578   1.1457  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)           0.07491    0.08240   0.909    0.363    \ntrttrt                0.93393    0.12731   7.336 2.20e-13 ***\nconcentration         0.89385    0.14728   6.069 1.29e-09 ***\ntrttrt:concentration  0.13378    0.23675   0.565    0.572    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5920.1  on 4999  degrees of freedom\nResidual deviance: 5622.9  on 4996  degrees of freedom\nAIC: 5630.9\n\nNumber of Fisher Scoring iterations: 4\n\nplot(allEffects(m1))\n\n\n\n\nThe problem with this, however, is the condition that we “simulate data under model assumptions”, which includes the nonlinear link function. Let’s have a look what happens if we simulate data differently: in this case, we just assume that treatment changes the overall probability of survival (from 45% to 90%), and the concentration increases the survival by up to 10% for each group. We may think that we don’t have an interaction in this case, but the model finds one\n\nresponse = 0.45 * as.numeric(trt) + 0.1*concentration\nsurvival = rbinom(5000, 1, response)\n\ndat = data.frame(trt = trt, \n                 concentration = concentration,\n                 survival = survival)\n\nm2 = glm(survival ~ trt * concentration, \n            data = dat, family = \"binomial\")\nsummary(m2)\n\n\nCall:\nglm(formula = survival ~ trt * concentration, family = \"binomial\", \n    data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.8600  -1.0714   0.2821   1.0405   1.3101  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)          -0.30719    0.08142  -3.773 0.000161 ***\ntrttrt                2.46941    0.18030  13.696  < 2e-16 ***\nconcentration         0.64074    0.14149   4.529 5.94e-06 ***\ntrttrt:concentration  1.37625    0.39167   3.514 0.000442 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5848.4  on 4999  degrees of freedom\nResidual deviance: 4355.1  on 4996  degrees of freedom\nAIC: 4363.1\n\nNumber of Fisher Scoring iterations: 6\n\nplot(allEffects(m2))\n\n\n\n\nIt looks in the effect plots as if the slope is changing as well, but note that this because the effect plots scale the y axis according to the link - absolutely, the effect of concentration is 10% for both groups.\nThe reason is simple: if we plot the plogis function, it becomes obvious that at different base levels (which would be controlled by trt in our case), moving a unit in concentration has a different effect.\n\n\n\n\n\nIf we turn this around, this means that if want the model to have the same effect of concentration at the response scale for both treatments, we must implement an interaction.\nWhether this is a feature or a bug of GLMs depends a bit on the viewpoint. One could argue that, looking at survival, for example, it doesn’t make sense that the concentration should have an effect of absolute 10% on top of the baseline created by trt for either 45% and 90% survival, and if we see such an effect, we should interpret this as an interaction, because relatively speaking, and increase of 45% to 55% is less important than an increase of 90% to 100%.\nStill, this also means that main effects and interactions can change if you change the link function, and default links are not always natural from a biological viewpoint.\nThere are several ways to get better interpretable interactions. One option is that we could fit the last model with a binomial distribution, but with an identity link\n\nm3 = glm(survival ~ trt * concentration, \n            data = dat, family = binomial(link = \"identity\"))\n\nWarning: step size truncated: out of bounds\n\nWarning: step size truncated: out of bounds\n\nWarning: step size truncated: out of bounds\n\nWarning: step size truncated: out of bounds\n\nWarning: step size truncated: out of bounds\n\nWarning: step size truncated: out of bounds\n\nWarning: step size truncated: out of bounds\n\nWarning: step size truncated: out of bounds\n\nWarning: step size truncated: out of bounds\n\nWarning: step size truncated: out of bounds\n\nWarning: step size truncated: out of bounds\n\n\nWarning: glm.fit: algorithm stopped at boundary value\n\nsummary(m3)\n\n\nCall:\nglm(formula = survival ~ trt * concentration, family = binomial(link = \"identity\"), \n    data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.2649  -1.0716   0.3023   1.0398   1.3103  \n\nCoefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)           0.42369    0.02010  21.079  < 2e-16 ***\ntrttrt                0.48388    0.02174  22.254  < 2e-16 ***\nconcentration         0.15935    0.03483   4.575 4.77e-06 ***\ntrttrt:concentration -0.06690    0.03582  -1.868   0.0618 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5848.4  on 4999  degrees of freedom\nResidual deviance: 4347.2  on 4996  degrees of freedom\nAIC: 4355.2\n\nNumber of Fisher Scoring iterations: 12\n\nplot(allEffects(m3))\n\n\n\n\nNow, we can interpret effects and interactions exactly like in a linear regression.\nAnother option is to look at the predicted effects at the response scale, e.g. via the effect plots, and interpret from there if we have an interaction according to what you would define as one biologically. One option to do this is the margins package.\n\n\n\n\n\n\nNote\n\n\n\nIf effect directions change in sign, they will do so under any link function (as they are always monotonous), so changes in effect direction are robust to this problem."
  },
  {
    "objectID": "4A-GLMs.html#new-considerations-for-glmms",
    "href": "4A-GLMs.html#new-considerations-for-glmms",
    "title": "8  GL(M)Ms",
    "section": "8.5 New considerations for GLMMs",
    "text": "8.5 New considerations for GLMMs\nAs we saw, in principle, random effects can be added to GLMs very much in the same way as before. Note, however, that there is a conceptual difference:\n\nIn an LMM, we had y = f(x) + RE + residual, where both RE and residual error were normal distributions\nIn a GLMM, we have y = dist <- link^-2 <- f(x) + RE, so the normally distributed RE goes into another distribution via the link function\n\nThis has a number of consequences that may be unexpected if you don’t think about it.\n\n8.5.1 Unconditinal vs. marginal predictions\nOne of those is that, if you have a nonlinear link function, predictions based on the fixed effects (unconditional) will not correspond to the mean of your data, or the average of the model predictions including the random effect (marginal predictions). To understand this, imagine we have a logistic regression, and our fixed effects predict a mean of 1. Then the unconditional prediction at the response scale\n\nplogis(1)\n\n[1] 0.7310586\n\n\nNow, let’s try to see what the average (marginal) prediction over all random effects is. We add a random effect with 20 groups, sd = 1. In this case, there will be additional variation around the random intercept.\n\nmean(plogis(1 + rnorm(20, sd = 1)))\n\n[1] 0.6966425\n\n\nThe value is considerably lower than the unconditional prediction, and this value is also what you would approximately get if you take the mean of your data.\nWhether this is a problem or not is a matter of perspective. From statistical viewpoint, assuming that your model assumptions correspond to the data-generating model, there is nothing wrong with the unconditional (fixed effect) prediction not corresponding to the mean of the data. From a practical side, however, many people are unsatisfied with this, because they want to show predictions against data. Unfortunately (to my knowledge), there is no function to automatically create marginal predictions. A quick fix would be to create conditional predictions, add random effecots on top as above (with the estimated variances) and push them through the corresponding link function."
  },
  {
    "objectID": "4B-Heteroskedasticity.html",
    "href": "4B-Heteroskedasticity.html",
    "title": "9  Dispersion models",
    "section": "",
    "text": "Modelling dispersion means that we describe how the expected variance changes as a function of the mean or predictor variables. The residual problem that we address by that is know as heteroskedasticity."
  },
  {
    "objectID": "4B-Heteroskedasticity.html#heteroskedasticity-in-the-linear-model",
    "href": "4B-Heteroskedasticity.html#heteroskedasticity-in-the-linear-model",
    "title": "9  Dispersion models",
    "section": "9.1 Heteroskedasticity in the linear model",
    "text": "9.1 Heteroskedasticity in the linear model\nThe easiest way to understand heteroskedasticity is in the linear model. Let’s look at an extreme example that we create by simulating some data\n\nset.seed(125)\n\ndata = data.frame(treatment = factor(rep(c(\"A\", \"B\", \"C\"), each = 15)))\ndata$observation = c(7, 2 ,4)[as.numeric(data$treatment)] +\n  rnorm( length(data$treatment), sd = as.numeric(data$treatment)^2 )\nboxplot(observation ~ treatment, data = data)\n\n\n\n\nEspecially p-values and confidence intervals of lm() and ANOVA can react quite strongly to such differences in residual variation. So, running a standard lm() / ANOVA on this data is not a good idea - in this case, we see that all regression effects are not significant, as is the ANOVA, suggesting that there is no difference between groups.\n\nfit = lm(observation ~ treatment, data = data)\nsummary(fit)\n\n\nCall:\nlm(formula = observation ~ treatment, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.2897  -1.0514   0.3531   2.4465  19.8602 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    7.043      1.731   4.069 0.000204 ***\ntreatmentB    -3.925      2.448  -1.603 0.116338    \ntreatmentC    -1.302      2.448  -0.532 0.597601    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.704 on 42 degrees of freedom\nMultiple R-squared:  0.05973,   Adjusted R-squared:  0.01495 \nF-statistic: 1.334 on 2 and 42 DF,  p-value: 0.2744\n\nsummary(aov(fit))\n\n            Df Sum Sq Mean Sq F value Pr(>F)\ntreatment    2  119.9   59.95   1.334  0.274\nResiduals   42 1887.6   44.94               \n\n\nSo, what can we do?\n\n9.1.1 Transformation\nOne option is to search for a transformation of the response that improves the problem - If heteroskedasticity correlates with the mean value, one can typically decrease it by some sqrt or log transformation, but often difficult, because this may also conflict with keeping the distribution normal.\n\n\n9.1.2 Modelling the variance / dispersion\nThe second, more general option, is to model the variance - Modelling the variance to fit a model where the variance is not fixed. We will discuss two packages in R that allow to model the dispersion\nThe first (traditional) option is to use nlme::gls. GLS = Generalized Least Squares. In the gls function, you can specify a dependency of the residual variance on a predictor or the response via the weight argument. There are different types of dependencies that you can specify, see ?varFunc. In our case, we will use the varIdent function, which allows to specify a different variance per treatment.\n\nlibrary(nlme)\n\nfit = gls(observation ~ treatment, data = data, \n          weights = varIdent(form = ~ 1 | treatment))\nsummary(fit)\n\nGeneralized least squares fit by REML\n  Model: observation ~ treatment \n  Data: data \n       AIC      BIC    logLik\n  243.9258 254.3519 -115.9629\n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | treatment \n Parameter estimates:\n        A         B         C \n 1.000000  4.714512 11.821868 \n\nCoefficients:\n                Value Std.Error   t-value p-value\n(Intercept)  7.042667 0.2348387 29.989388  0.0000\ntreatmentB  -3.925011 1.1317816 -3.467994  0.0012\ntreatmentC  -1.302030 2.7861462 -0.467323  0.6427\n\n Correlation: \n           (Intr) trtmnB\ntreatmentB -0.207       \ntreatmentC -0.084  0.017\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-2.4587934 -0.6241702  0.1687727  0.6524558  1.9480170 \n\nResidual standard error: 0.9095262 \nDegrees of freedom: 45 total; 42 residual\n\n\nIf we check the ANOVA, we see that, unlike before, we get a significant effect of the treatment\n\nanova(fit)\n\nDenom. DF: 42 \n            numDF  F-value p-value\n(Intercept)     1 899.3761  <.0001\ntreatment       2   6.0962  0.0047\n\n\nThe second option for modeling variances is to use the glmmTMB package. Here, you can specify an extra regression formula for the dispersion (= residual variance). If we fit this:\n\nlibrary(glmmTMB)\n\nfit = glmmTMB(observation ~ treatment, data = data, \n              dispformula = ~ treatment)\nsummary(fit)\n\n Family: gaussian  ( identity )\nFormula:          observation ~ treatment\nDispersion:                   ~treatment\nData: data\n\n     AIC      BIC   logLik deviance df.resid \n   248.7    259.5   -118.3    236.7       39 \n\n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   7.0427     0.2269  31.042  < 2e-16 ***\ntreatmentB   -3.9250     1.0934  -3.590 0.000331 ***\ntreatmentC   -1.3020     2.6917  -0.484 0.628582    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDispersion model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -0.2587     0.3651  -0.708    0.479    \ntreatmentB    3.1013     0.5164   6.006 1.91e-09 ***\ntreatmentC    4.9399     0.5164   9.566  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe get 2 regression tables as outputs - one for the effects, and one for the dispersion (= residual variance). We see, as expected, that the dispersion is higher in groups B and C compared to A. An advantage over gls is that we get confidence intervals and p-values for these differences on top!\n\n\n\n\n\n\nExercise variance modelling\n\n\n\nTake this plot of Ozone ~ Solar.R using the airquality data. Clearly there is heteroskedasticity in the relationship:\n\nplot(Ozone ~ Solar.R, data = airquality)\n\n\n\n\nWe can also see this when we fit the regression model:\n\nm1 = lm(Ozone ~ Solar.R, data = airquality)\npar(mfrow = c(2, 2))\nplot(m1)\n\nWe could of course consider other predictors, but let’s say we want to fit this model specifically\n\nTry to get the variance stable with a transformation.\nUse the gls function (package nlme) with the untransformed response to make the variance dependent on Solar.R. Hint: Read in varClasses and decide how to model this.\nUse glmmTMB to model heteroskedasticity.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nTransformation\n\n\nm3 = lm(sqrt(Ozone) ~ Solar.R, data = airquality)\n\n\nGLS\n\n\nm3 = nlme::gls(Ozone ~ Solar.R, weights = varPower(form = ~Solar.R), data = airquality[complete.cases(airquality),])\n\n\nglmmTMB\n\n\nm4 = glmmTMB(Ozone ~ Solar.R, dispformula = ~Solar.R, data = airquality)"
  },
  {
    "objectID": "4B-Heteroskedasticity.html#heteroskedasticity-in-glmms",
    "href": "4B-Heteroskedasticity.html#heteroskedasticity-in-glmms",
    "title": "9  Dispersion models",
    "section": "9.2 Heteroskedasticity in GLMMs",
    "text": "9.2 Heteroskedasticity in GLMMs\nGLM(M)s can be heteroskedastic as well, i.e. dispersion depends on some predictors. In glmmTMB, you can make the dispersion of the negative Binomial dependent on a formula via the dispformula argument, in the same way as in nlme for the linear model.\nVariance problems would show up when plotting residuals against predicted and predictors. On the previous page, we saw some variance problems in the Salamander model. We could add a variable dispersion model via\n\nm3 = glmmTMB(count ~ spp + mined + (1|site), family = nbinom1,\n             dispformula = ~ spp + mined ,  data = Salamanders)\nsummary(m3)\n\n Family: nbinom1  ( log )\nFormula:          count ~ spp + mined + (1 | site)\nDispersion:             ~spp + mined\nData: Salamanders\n\n     AIC      BIC   logLik deviance df.resid \n  1654.4   1730.3   -810.2   1620.4      627 \n\nRandom effects:\n\nConditional model:\n Groups Name        Variance Std.Dev.\n site   (Intercept) 0.2283   0.4778  \nNumber of obs: 644, groups:  site, 23\n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -1.5288     0.2799  -5.462 4.70e-08 ***\nsppPR        -1.3304     0.3480  -3.822 0.000132 ***\nsppDM         0.2695     0.2004   1.345 0.178561    \nsppEC-A      -0.7525     0.2772  -2.714 0.006641 ** \nsppEC-L       0.6228     0.2109   2.952 0.003155 ** \nsppDES-L      0.7113     0.1976   3.600 0.000318 ***\nsppDF         0.1470     0.2171   0.677 0.498259    \nminedno       2.1348     0.2825   7.557 4.14e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDispersion model:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept)  -0.2834     0.6414  -0.442   0.6586  \nsppPR         0.3160     0.7501   0.421   0.6735  \nsppDM         0.1979     0.5712   0.346   0.7289  \nsppEC-A       0.3592     0.6477   0.554   0.5792  \nsppEC-L       1.0830     0.5215   2.077   0.0378 *\nsppDES-L      0.7951     0.5370   1.481   0.1387  \nsppDF         0.3769     0.6109   0.617   0.5373  \nminedno       0.5583     0.4187   1.334   0.1823  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlibrary(DHARMa)\nres = simulateResiduals(m3, plot = T)\n\n\n\npar(mfrow = c(1, 2))\nplotResiduals(res, Salamanders$spp)\nplotResiduals(res, Salamanders$mined)"
  },
  {
    "objectID": "4B-Heteroskedasticity.html#excursion-outliers-robust-and-quantile-regression",
    "href": "4B-Heteroskedasticity.html#excursion-outliers-robust-and-quantile-regression",
    "title": "9  Dispersion models",
    "section": "9.3 Excursion: outliers, robust and quantile regression",
    "text": "9.3 Excursion: outliers, robust and quantile regression\nWhat can we do if, after accounting for the functional relationship, response transformation and variance modelling, residual diagnostic 2 shows non-normality, in particular strong outliers? Here simulated example data with strong outliers / deviations from normality:\n\nset.seed(123)\n\nn = 100\nconcentration = runif(n, -1, 1)\ngrowth = 2 * concentration + rnorm(n, sd = 0.5) +\n  rbinom(n, 1, 0.05) * rnorm(n, mean = 6*concentration, sd = 6)\nplot(growth ~ concentration)\n\n\n\n\nFitting the model, we see that the distribution is to wide:\n\nfit = lm(growth ~ concentration)\npar(mfrow = c(2, 2))\nplot(fit)\n\n\n\n\nWhat can we do to deal with such distributional problems and outliers?\n\nRemoving - Bad option, hard to defend, reviewers don’t like this - if at all, better show robustness with and without outlier, but result is sometimes not robust.\nChange the distribution - Fit a model with a different distribution, i.e. GLM or other.\nRobust regressions.\nQuantile regression - A special type of regression that does not assume a particular residual distribution.\n\n\n9.3.1 Robust regression\nRobust methods generally refer to methods that are robust to violation of assumptions, e.g. outliers. More specifically, standard robust regressions typically downweight datap oints that have a too high influence on the fit. See https://cran.r-project.org/web/views/Robust.html for a list of robust packages in R.\n\n# This is the classic method.\nlibrary(MASS)\n\nfit = rlm(growth ~ concentration) \nsummary(fit)\n\n\nCall: rlm(formula = growth ~ concentration)\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1986 -0.3724  0.0377  0.3391  7.0902 \n\nCoefficients:\n              Value   Std. Error t value\n(Intercept)   -0.0978  0.0594    -1.6453\nconcentration  2.0724  0.1048    19.7721\n\nResidual standard error: 0.534 on 98 degrees of freedom\n\n# No p-values and not sure if we can trust the confidence intervals.\n# Would need to boostrap by hand!\n\n# This is another option that gives us p-values directly.\nlibrary(robustbase)\n\nfit = lmrob(growth ~ concentration) \nsummary(fit)\n\n\nCall:\nlmrob(formula = growth ~ concentration)\n \\--> method = \"MM\"\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.2877 -0.4311 -0.0654  0.2788  7.0384 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -0.04448    0.05160  -0.862    0.391    \nconcentration  2.00588    0.08731  22.974   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 0.5549 \nMultiple R-squared:  0.8431,    Adjusted R-squared:  0.8415 \nConvergence in 7 IRWLS iterations\n\nRobustness weights: \n 9 observations c(27,40,47,52,56,76,80,91,100)\n     are outliers with |weight| = 0 ( < 0.001); \n 5 weights are ~= 1. The remaining 86 ones are summarized as\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6673  0.9015  0.9703  0.9318  0.9914  0.9989 \nAlgorithmic parameters: \n       tuning.chi                bb        tuning.psi        refine.tol \n        1.548e+00         5.000e-01         4.685e+00         1.000e-07 \n          rel.tol         scale.tol         solve.tol       eps.outlier \n        1.000e-07         1.000e-10         1.000e-07         1.000e-03 \n            eps.x warn.limit.reject warn.limit.meanrw \n        1.819e-12         5.000e-01         5.000e-01 \n     nResample         max.it       best.r.s       k.fast.s          k.max \n           500             50              2              1            200 \n   maxit.scale      trace.lev            mts     compute.rd fast.s.large.n \n           200              0           1000              0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\n\n\n\n9.3.2 Quantile regression\nQuantile regressions don’t fit a line with an error spreading around it, but try to fit a quantile (e.g. the 0.5 quantile, the median) regardless of the distribution. Thus, they work even if the usual assumptions don’t hold.\n\nlibrary(qgam)\n\ndat = data.frame(growth = growth, concentration = concentration)\n\nfit = qgam(growth ~ concentration, data = dat, qu = 0.5) \n\nEstimating learning rate. Each dot corresponds to a loss evaluation. \nqu = 0.5................done \n\nsummary(fit)\n\n\nFamily: elf \nLink function: identity \n\nFormula:\ngrowth ~ concentration\n\nParametric coefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   -0.08167    0.05823  -1.403    0.161    \nconcentration  2.04781    0.09500  21.556   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.427   Deviance explained = 48.8%\n-REML = 157.82  Scale est. = 1         n = 100"
  },
  {
    "objectID": "4C-CorrelationStructures.html",
    "href": "4C-CorrelationStructures.html",
    "title": "10  Correlation structures",
    "section": "",
    "text": "Except for the random effects, we have so far assumed that residual errors are independent. However, that must not always be the case - we may find that residuals show autocorrelation.\n\n\n\n\n\n\nNote\n\n\n\nCorrelation means that one variable correlates with another. Autocorrelation means that data points of one variable that are close to each other have similar values. This implies that autocorrelation is only defined if there is a “distance relationship” between observations.\n\n\nAutocorrelation can always occur if we have a distance relationship between observations. Apart from random effects, where distance is expressed by group, common examples of continuous distance relationships include:\n\nRandom effects (distance = group)\nSpatial distance.\nTemporal distance.\nPhylogenetic distance.\n\nHere a visualization from Roberts et al., 2016 (reproduced as OA, copyright: the authors).\n\n\n\n\n\n\n\nIf we find autocorrelation in the residuals of our model, there can be several reasons, which we can address by different structures.\n\n\n\n\n\n\nImportant\n\n\n\nIn the context of regression models, we are never interested in the autocorrelation of the response / predictors per se, but only in the residuals. Thus, it doesn’t make sense to assume that you need a spatial model only because you have a spatially autocorrelated signal.\n\n\n\nAutocorrelation can occur because we have a spatially correlated misfit, i.e. there is a trend in the given space (e.g. time, space, phylogeny). If this is the case, de-trending the model (with a linear regression term or a spline) will remove the residual autocorrelation. We should always de-trend first because we consider moving to a model with a residual correlation structure.\nOnly after accounting for the trend, we should test if there is a residual spatial / temporal / phylogenetic autocorrelation. If that is the case, we would usually use a so-called conditional autoregressive (CAR) structures. In these models, we make parametric assumptions for how the correlation between data points falls off with distance. When we speak about spatial / temporal / phylogenetic regressions, we usually refer to these or similar models.\n\n\n\n\nTo de-trend, you can just use standard regression terms or splines on time or space. For the rest of this chapter, we will concentrate on how to specify “real” correlation structures. However, in the case studies, you should always de-trend first.\nTo account for “real” autocorrelation of residuals, similar as for the variance modelling, we can add correlation structures\n\nfor normal responses in nlme::gls, see https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/corClasses.html\nfor GLMs using glmmTMB, see https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html.\n\nThe following pages provide examples and further comments on how to do this.\n\n\n\n\n\n\nNote\n\n\n\nEspecially for spatial models, both nlme and glmmTMB are relatively slow. Therea are a large number of specialized packages that deal in particular with the problem of spatial models, including MASS::glmmPQL, BRMS, INLA, spaMM, and many more. To keep things simple and concentrate on the principles, however, we will stick with the packages you already know."
  },
  {
    "objectID": "4C-CorrelationStructures.html#temporal-correlation-structures",
    "href": "4C-CorrelationStructures.html#temporal-correlation-structures",
    "title": "10  Correlation structures",
    "section": "10.2 Temporal Correlation Structures",
    "text": "10.2 Temporal Correlation Structures\nTo introduce temporal autoregressive models, let’s simulate some data first. The most simple (and common) temporal structure is the AR1 model, aka autoregressive model with lag 1. The AR1 assumes that the next data point (or residual) originates from a weighted mean of the last data point and a residual normal distribution in the form\n\\[\nx_{t+1} = a \\cdot x_t + (1-a) \\cdot \\epsilon\n\\]\nLet’s simulate some data according to this model\n\n# simulate temporally autocorrelated data\nAR1sim<-function(n, a){\n  x = rep(NA, n)\n  x[1] = 0\n  for(i in 2:n){\n    x[i] = a * x[i-1] + (1-a) * rnorm(1)\n  }\n  return(x)\n}\n\nset.seed(123)\nobs = AR1sim(1000, 0.9)\nplot(obs)\n\n\n\n\nAs we can see, we have a temporal correlation here. As we have not modeled / specified any further predictors, the correlation in the signal will transform directly in the correlations of the residuals if we fit a model:\n\nfit = lm(obs~1)\nsummary(fit)\n\n\nCall:\nlm(formula = obs ~ 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58485 -0.14315  0.00559  0.13729  0.66317 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept) 0.017019   0.006745   2.523   0.0118 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2133 on 999 degrees of freedom\n\n\nNote that the estimate of the intercept is significant, although we started the simulation at zero. Let’s look at the residuals, which have the same autocorrelation as the data.\n\nplot(residuals(fit))\n\n\n\n\nWe can quantify the autocorrelation by the acf function, which quantifies correlation between observations as a function of their lag (temporal distance). Note that although we modeled only a lag of 1, we will get correlations with many lags, because the correlation effect “trickles down”.\n\nacf(residuals(fit))\n\n\n\n\nTo check what the actual underlying model is, it may be useful to plot the partial correlation coefficient, which is estimated by fitting autoregressive models of successively higher orders and checking their residuals.\n\npacf(residuals(fit))\n\n\n\n\nHere, we see that we actually only have a correlation with lag 1. You can also check for temporal correlation with the DHARMa package\n\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\ntestTemporalAutocorrelation(fit, time = 1:1000)\n\n\n\n\n\n    Durbin-Watson test\n\ndata:  simulationOutput$scaledResiduals ~ 1\nDW = 0.25973, p-value < 2.2e-16\nalternative hypothesis: true autocorrelation is not 0\n\n\n\n\n\n\n\n\nNote\n\n\n\nRemember: in general, for spatial / temporal data, there are two processes that can created residual autocorreation:\n\nThere is a spatial misfit trend in time / space, which creates a correlation in space / time.\nThere truly is a spatial correlation, after accounting for the trend.\n\nUnfortunately, the distinction between a larger trend and a correlation is quite fluid. Nevertheless, one should always first check for and remove the trend, typically by including time/space as a predictor, potentially in a flexible way (GAMs come in handy). After this is done, we can fit a model with a temporally/spatially correlated error.\n\n\nLet’s see how we can fit the AR1 model to data. First, with nlme\n\nlibrary(nlme)\n\nfitGLS = gls(obs~1, corr = corAR1(0.771, form = ~ 1))\nsummary(fitGLS)\n\nGeneralized least squares fit by REML\n  Model: obs ~ 1 \n  Data: NULL \n        AIC       BIC  logLik\n  -1773.104 -1758.384 889.552\n\nCorrelation Structure: AR(1)\n Formula: ~1 \n Parameter estimate(s):\n      Phi \n0.8865404 \n\nCoefficients:\n                 Value  Std.Error   t-value p-value\n(Intercept) 0.01620833 0.02741235 0.5912784  0.5545\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-2.72610052 -0.66439347  0.02987808  0.64460052  3.09922846 \n\nResidual standard error: 0.2142402 \nDegrees of freedom: 1000 total; 999 residual\n\n\nSecond, with glmmTMB\n\nlibrary(glmmTMB)\n\nWarning in checkMatrixPackageVersion(): Package version inconsistency detected.\nTMB was built with Matrix version 1.4.1\nCurrent Matrix version is 1.5.4.1\nPlease re-install 'TMB' from source using install.packages('TMB', type = 'source') or ask CRAN for a binary version of 'TMB' matching CRAN's 'Matrix' package\n\n\nWarning in checkDepPackageVersion(dep_pkg = \"TMB\"): Package version inconsistency detected.\nglmmTMB was built with TMB version 1.9.3\nCurrent TMB version is 1.9.1\nPlease re-install glmmTMB from source or restore original 'TMB' package (see '?reinstalling' for more information)\n\ntime <- factor(1:1000) # time variable\ngroup = factor(rep(1,1000)) # group (for multiple time series)\n\nfitGLMMTMB = glmmTMB(obs ~ ar1(time + 0 | group))\n\nWarning in glmmTMB(obs ~ ar1(time + 0 | group)): use of the 'data' argument is\nrecommended\n\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\nsummary(fitGLMMTMB)\n\n Family: gaussian  ( identity )\nFormula:          obs ~ ar1(time + 0 | group)\n\n     AIC      BIC   logLik deviance df.resid \n -1776.7  -1757.1    892.4  -1784.7      996 \n\nRandom effects:\n\nConditional model:\n Groups   Name  Variance  Std.Dev. Corr      \n group    time1 0.0449382 0.21199  0.89 (ar1)\n Residual       0.0002047 0.01431            \nNumber of obs: 1000, groups:  group, 1\n\nDispersion estimate for gaussian family (sigma^2): 0.000205 \n\nConditional model:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  0.01619    0.02740   0.591    0.555\n\n\nIf you check the results, you can see that\n\nBoth models correctly estimate the AR1 parameter\nThe p-value for the intercept is in both models n.s., as expected\n\n\n\n\n\n\n\nTrend and autocorrelation with glmmTMB\n\n\n\nAs I mentioned earlier, first detrend and then add correlation structure if there is autocorrelation. After both steps we should no longer see any pattern in the conditional residuals. Unfortunately, checking the conditional residuals is a bit complicated because glmmTMB does not support conditional simulations, while lme4 does, but it does not support correlation structures. However, there is a workaround.\nLet’s start with a small simulation with a time trend and autocorrelation:\n\ntime = 1:1000/100\ny = time +2*(sin(time/0.4)) + rnorm(1000)\ndata = \n  data.frame(y = y, time = time, timeF = as.factor(1:1000), group = as.factor(1))\nplot(y ~time)\n\n\n\n\n\nDetrend\n\n\nfit1 = glmmTMB(y~time, data = data)\nres = simulateResiduals(fit1, plot = TRUE)\n\n\n\ntestTemporalAutocorrelation(res, time = data$time)\n\n\n\n\n\n    Durbin-Watson test\n\ndata:  simulationOutput$scaledResiduals ~ 1\nDW = 0.66181, p-value < 2.2e-16\nalternative hypothesis: true autocorrelation is not 0\n\n\nTest for temporal autocorrelation is siginificant -> add autoregressive structure\n\nAdd autoregressive structure\n\n\nfit2 = glmmTMB(y~time + ar1(0+timeF|group), data = data)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\nres = simulateResiduals(fit2, plot = TRUE)\n\n\n\n\nThe residual plot did not change because glmmTMB:::simulate.glmmTMB does not generate conditional predictions. But we can generate them ourselves:\n\nCreate conditional predictions and simulations\n\nWe can create a custom DHARMa object with our own simulations:\n\npred = predict(fit2, re.form = NULL)\nsimulations = sapply(1:250, function(i) rnorm(1000, pred, summary(fit2)$sigma))\nres = createDHARMa(simulations, data$y, pred)\nplot(res)\n\n\n\n\nVoila, the residuals look good now!\n\n\n\n\n\n\n\n\nExcercise\n\n\n\nLook at the hurricane study that we used before, which is, after all, temporal data. This data set is located in DHARMa.\n\nlibrary(DHARMa)\n\nfit = glmmTMB(alldeaths ~ scale(MasFem) *\n                          (scale(Minpressure_Updated_2014) + scale(NDAM)),\n                           data = hurricanes, family = nbinom2)\n\n# Residual checks with DHARMa.\nres = simulateResiduals(fit)\n\n# Checking for temporal autocorrelation\nres2 = recalculateResiduals(res, group = hurricanes$Year)\ntestTemporalAutocorrelation(res2, time = unique(hurricanes$Year))\n\n\n\n\n\n    Durbin-Watson test\n\ndata:  simulationOutput$scaledResiduals ~ 1\nDW = 2.5518, p-value = 0.04758\nalternative hypothesis: true autocorrelation is not 0\n\n\nAdd an AR1 term to the model!\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nhurricanes$yearF <- factor(hurricanes$Year)\nhurricanes$group = factor(rep(1,nrow(hurricanes)))\n\nfit = glmmTMB(alldeaths ~ scale(MasFem) *\n                          (scale(Minpressure_Updated_2014) + scale(NDAM))\n                          + ar1(yearF + 0 | group),\n                           data = hurricanes, family = nbinom2)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\n\nWarning in fitTMB(TMBStruc): Model convergence problem; non-positive-definite\nHessian matrix. See vignette('troubleshooting')\n\nsummary(fit)\n\n Family: nbinom2  ( log )\nFormula:          \nalldeaths ~ scale(MasFem) * (scale(Minpressure_Updated_2014) +  \n    scale(NDAM)) + ar1(yearF + 0 | group)\nData: hurricanes\n\n     AIC      BIC   logLik deviance df.resid \n      NA       NA       NA       NA       83 \n\nRandom effects:\n\nConditional model:\n Groups Name      Variance  Std.Dev.  Corr      \n group  yearF1950 1.647e-09 4.058e-05 0.77 (ar1)\nNumber of obs: 92, groups:  group, 1\n\nDispersion parameter for nbinom2 family (): 0.787 \n\nConditional model:\n                                              Estimate Std. Error z value\n(Intercept)                                     2.5034     0.1231  20.341\nscale(MasFem)                                   0.1237     0.1210   1.022\nscale(Minpressure_Updated_2014)                -0.5425     0.1603  -3.384\nscale(NDAM)                                     0.8988     0.2190   4.105\nscale(MasFem):scale(Minpressure_Updated_2014)   0.3758     0.1731   2.171\nscale(MasFem):scale(NDAM)                       0.6629     0.1999   3.316\n                                              Pr(>|z|)    \n(Intercept)                                    < 2e-16 ***\nscale(MasFem)                                 0.306925    \nscale(Minpressure_Updated_2014)               0.000715 ***\nscale(NDAM)                                   4.05e-05 ***\nscale(MasFem):scale(Minpressure_Updated_2014) 0.029903 *  \nscale(MasFem):scale(NDAM)                     0.000915 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "4C-CorrelationStructures.html#spatial-correlation-structures",
    "href": "4C-CorrelationStructures.html#spatial-correlation-structures",
    "title": "10  Correlation structures",
    "section": "10.3 Spatial Correlation Structures",
    "text": "10.3 Spatial Correlation Structures\nSpatial models work very similar to the temporal models. This time, we start directly with an example, using a data set with the thickness of coal seams, that we try to predict with a spatial (soil) predictor.\n\nlibrary(EcoData)\nplot(thick ~ soil, data = thickness)\n\n\n\n\nLet’s fit a simple LM to this\n\nfit = lm(thick ~ soil, data = thickness)\nsummary(fit)\n\n\nCall:\nlm(formula = thick ~ soil, data = thickness)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0414 -1.1975  0.0876  1.4836  4.9584 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  31.9420     3.1570  10.118 1.54e-15 ***\nsoil          2.2552     0.8656   2.605   0.0111 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.278 on 73 degrees of freedom\nMultiple R-squared:  0.08508,   Adjusted R-squared:  0.07254 \nF-statistic: 6.788 on 1 and 73 DF,  p-value: 0.01111\n\n\nDHARMa checks:\n\nres = simulateResiduals(fit)\ntestSpatialAutocorrelation(res, x = thickness$north, y = thickness$east)\n\n\n\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res\nobserved = 0.210870, expected = -0.013514, sd = 0.021940, p-value <\n2.2e-16\nalternative hypothesis: Distance-based autocorrelation\n\n\nFor spatial data, we often look at spatial variograms, which are similar to an acf but in spatial directions\n\nlibrary(gstat)\ntann.dir.vgm = variogram(residuals(fit) ~ 1,\n                         loc =~ east + north, data = thickness,\n                         alpha = c(0, 45, 90, 135))\nplot(tann.dir.vgm)\n\n\n\n\nBoth the DHARMa plots and the variograms are more indicative of a spatial trend. Let’s remove this with a 2d-spine, called a tensor spline:\n\nlibrary(mgcv)\n\nfit1 = gam(thick ~ soil + te(east, north) , data = thickness)\nsummary(fit1)\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nthick ~ soil + te(east, north)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 39.68933    0.26498 149.780   <2e-16 ***\nsoil         0.12363    0.07275   1.699   0.0952 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df     F p-value    \nte(east,north) 21.09  22.77 721.3  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.996   Deviance explained = 99.7%\nGCV = 0.033201  Scale est. = 0.022981  n = 75\n\nplot(fit1, pages = 0, lwd = 2)\n\n\n\n\nWe can check the model again, and the problem is gone\n\nres = simulateResiduals(fit1)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nRegistered S3 method overwritten by 'mgcViz':\n  method from  \n  +.gg   GGally\n\ntestSpatialAutocorrelation(res, x = thickness$north, y = thickness$east)\n\n\n\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res\nobserved = -0.024242, expected = -0.013514, sd = 0.021860, p-value =\n0.6236\nalternative hypothesis: Distance-based autocorrelation\n\n\nAlmost the same, but simpler:\n\nfit = lm(thick ~ soil + north + I(north^2), data = thickness)\n\nIf we would have still seen a signal, we should have fit an autoregressive model. Here it’s not necessary, but just to show you the syntax - first nlme:\n\nfit2 = gls(thick ~ soil , correlation = corExp(form =~ east + north) , data = thickness)\nsummary(fit2)\n\nGeneralized least squares fit by REML\n  Model: thick ~ soil \n  Data: thickness \n       AIC      BIC    logLik\n  164.3474 173.5092 -78.17368\n\nCorrelation Structure: Exponential spatial correlation\n Formula: ~east + north \n Parameter estimate(s):\n   range \n719.4122 \n\nCoefficients:\n               Value Std.Error  t-value p-value\n(Intercept) 42.81488  5.314542 8.056176  0.0000\nsoil         0.02662  0.199737 0.133289  0.8943\n\n Correlation: \n     (Intr)\nsoil -0.12 \n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-1.5811122 -0.7276873 -0.5028102 -0.2092991  0.3217326 \n\nResidual standard error: 5.573088 \nDegrees of freedom: 75 total; 73 residual\n\n\nSecond, for glmmTMB. Here, we again have to prepare the data first\n\nthickness$pos <- numFactor(thickness$east, \n                           thickness$north)\nthickness$group <- factor(rep(1, nrow(thickness)))\n\nfit3 = glmmTMB(thick ~ soil + exp(pos + 0 | group) , data = thickness)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\n\nThe output of summary is a bit chunky, which is why I suppress it here\n\nsummary(fit3)\n\nIf you wonder why there is such a large correlation matrix displayed: both the AR1 and the exp(pos + 0 | group) structure impose a particular correlation structure on the random effects. Per default, glmmTMB shows correlations of random effects if they are estimated. In the case of the AR1 structure, the programmers apparently surpressed this, and just showed the stimate of the AR1 parameter. Here, however, they didn’t implement this feature, so you see the entire correlation structure, which is, admittedly, less helpful and should be changed.\n\n\n\n\n\n\nPlant counts in Regensburg\n\n\n\nUse the dataset EcoData::plantcounts. Our scientific question is if richness ~ agrarea. Help on the dataset, as well as a few initial plots, is in the help of ?plantcounts.\nThis is count data, so start with a Poisson or Neg Binom GLM. The quadrats are not all equally sized, so you should include an offest to account for area. Then, check for spatial autocorrelation.\nIf you find autocorrelation that cannot be removed with a gam, the problem is that the gls function that we have used so far only extends lm, and not glm models. In this case, you can either read up in https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html how to specify a spatial covariance in glmmTMB, or just log transform your counts + 1, and fit a gls.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n?EcoData::plantcounts\n\nplants_sf <- plantcounts\nstr(plants_sf)\n\n'data.frame':   285 obs. of  6 variables:\n $ tk      : int  65341 65342 65343 65344 65351 65352 65353 65354 65361 65362 ...\n $ area    : num  33.6 33.6 33.6 33.6 33.6 ...\n $ richness: int  767 770 741 756 550 434 433 448 527 505 ...\n $ agrarea : num  0.488 0.431 0.484 0.598 0.422 ...\n $ lon     : num  11.4 11.5 11.4 11.5 11.5 ...\n $ lat     : num  49.5 49.5 49.4 49.4 49.5 ...\n\nplants_sf$agrarea_scaled <- scale(plants_sf$agrarea)\n\nplants_sf$longitude <- plants_sf$lon\nplants_sf$latitude <- plants_sf$lat\nlibrary(sf)\nplants_sf <- sf::st_as_sf(plants_sf, coords = c('longitude', 'latitude'), crs\n                          = st_crs(\"+proj=longlat +ellps=bessel\n                                   +towgs84=606,23,413,0,0,0,0 +no_defs\"))\n\nlibrary(mapview)\nmapview(plants_sf[\"richness\"], map.types = \"OpenTopoMap\")\n\n\n\n\n\nfit <-  glmmTMB::glmmTMB(richness ~ agrarea_scaled + offset(log(area)),\n                family = nbinom1, data = plants_sf)\nsummary(fit)\n\n Family: nbinom1  ( log )\nFormula:          richness ~ agrarea_scaled + offset(log(area))\nData: plants_sf\n\n     AIC      BIC   logLik deviance df.resid \n  3348.8   3359.8  -1671.4   3342.8      282 \n\n\nDispersion parameter for nbinom1 family (): 14.3 \n\nConditional model:\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)     2.66825    0.01047  254.79  < 2e-16 ***\nagrarea_scaled -0.03316    0.01021   -3.25  0.00117 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlibrary(DHARMa)\nres <- simulateResiduals(fit)\nplot(res)\n\n\n\ntestSpatialAutocorrelation(res, x = plants_sf$lon, y =  plants_sf$lat)\n\n\n\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res\nobserved = 0.0958792, expected = -0.0035211, sd = 0.0047788, p-value <\n2.2e-16\nalternative hypothesis: Distance-based autocorrelation\n\nfit2<-mgcv::gam(richness ~ agrarea_scaled + te(lon, lat),\n            offset(log(area)), family = nb, data = plants_sf)\nsummary(fit2)\n\n\nFamily: Negative Binomial(67.736) \nLink function: log \n\nFormula:\nrichness ~ agrarea_scaled + te(lon, lat)\n\nParametric coefficients:\n                Estimate Std. Error z value Pr(>|z|)    \n(Intercept)     6.183373   0.004096 1509.72  < 2e-16 ***\nagrarea_scaled -0.024366   0.005355   -4.55 5.37e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n              edf Ref.df Chi.sq p-value    \nte(lon,lat) 22.53  23.76  850.3  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.413   Deviance explained = 50.2%\n-REML = 5622.6  Scale est. = 1         n = 285\n\nplot(fit2)\n\n\n\nlibrary(mgcViz)\nb <- getViz(fit2)\nprint(plot(b, allTerms = F), pages = 1) # Calls print.plotGam()\n\n\n\n#plotRGL(sm(b, 1), residuals = TRUE)\n\nres <- simulateResiduals(fit2)\nplot(res)\n\n\n\ntestSpatialAutocorrelation(res, x = plants_sf$lon, y =  plants_sf$lat)\n\n\n\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res\nobserved = -0.0030357, expected = -0.0035211, sd = 0.0047800, p-value =\n0.9191\nalternative hypothesis: Distance-based autocorrelation"
  },
  {
    "objectID": "4C-CorrelationStructures.html#phylogenetic-structures-pgls",
    "href": "4C-CorrelationStructures.html#phylogenetic-structures-pgls",
    "title": "10  Correlation structures",
    "section": "10.4 Phylogenetic Structures (PGLS)",
    "text": "10.4 Phylogenetic Structures (PGLS)\nThis is mostly taken from https://lukejharmon.github.io/ilhabela/instruction/2015/07/03/PGLS/. The two datasets associated with this example are in the EcoData package.\nPerform analysis:\n\nlibrary(EcoData)\nlibrary(ape)\nlibrary(geiger)\nlibrary(nlme)\nlibrary(phytools)\nlibrary(DHARMa)\n\nTo plot the phylogenetic tree, use\n\nplot(anolisTree)\n\nRegress species traits\n\n# Check whether names are matching in both files.\nname.check(anolisTree, anolisData)\n\n$tree_not_data\n  [1] \"ahli\"            \"alayoni\"         \"alfaroi\"         \"aliniger\"       \n  [5] \"allisoni\"        \"allogus\"         \"altitudinalis\"   \"alumina\"        \n  [9] \"alutaceus\"       \"angusticeps\"     \"argenteolus\"     \"argillaceus\"    \n [13] \"armouri\"         \"bahorucoensis\"   \"baleatus\"        \"baracoae\"       \n [17] \"barahonae\"       \"barbatus\"        \"barbouri\"        \"bartschi\"       \n [21] \"bremeri\"         \"breslini\"        \"brevirostris\"    \"caudalis\"       \n [25] \"centralis\"       \"chamaeleonides\"  \"chlorocyanus\"    \"christophei\"    \n [29] \"clivicola\"       \"coelestinus\"     \"confusus\"        \"cooki\"          \n [33] \"cristatellus\"    \"cupeyalensis\"    \"cuvieri\"         \"cyanopleurus\"   \n [37] \"cybotes\"         \"darlingtoni\"     \"distichus\"       \"dolichocephalus\"\n [41] \"equestris\"       \"etheridgei\"      \"eugenegrahami\"   \"evermanni\"      \n [45] \"fowleri\"         \"garmani\"         \"grahami\"         \"guafe\"          \n [49] \"guamuhaya\"       \"guazuma\"         \"gundlachi\"       \"haetianus\"      \n [53] \"hendersoni\"      \"homolechis\"      \"imias\"           \"inexpectatus\"   \n [57] \"insolitus\"       \"isolepis\"        \"jubar\"           \"krugi\"          \n [61] \"lineatopus\"      \"longitibialis\"   \"loysiana\"        \"lucius\"         \n [65] \"luteogularis\"    \"macilentus\"      \"marcanoi\"        \"marron\"         \n [69] \"mestrei\"         \"monticola\"       \"noblei\"          \"occultus\"       \n [73] \"olssoni\"         \"opalinus\"        \"ophiolepis\"      \"oporinus\"       \n [77] \"paternus\"        \"placidus\"        \"poncensis\"       \"porcatus\"       \n [81] \"porcus\"          \"pulchellus\"      \"pumilis\"         \"quadriocellifer\"\n [85] \"reconditus\"      \"ricordii\"        \"rubribarbus\"     \"sagrei\"         \n [89] \"semilineatus\"    \"sheplani\"        \"shrevei\"         \"singularis\"     \n [93] \"smallwoodi\"      \"strahmi\"         \"stratulus\"       \"valencienni\"    \n [97] \"vanidicus\"       \"vermiculatus\"    \"websteri\"        \"whitemani\"      \n\n$data_not_tree\n  [1] \"1\"   \"10\"  \"100\" \"11\"  \"12\"  \"13\"  \"14\"  \"15\"  \"16\"  \"17\"  \"18\"  \"19\" \n [13] \"2\"   \"20\"  \"21\"  \"22\"  \"23\"  \"24\"  \"25\"  \"26\"  \"27\"  \"28\"  \"29\"  \"3\"  \n [25] \"30\"  \"31\"  \"32\"  \"33\"  \"34\"  \"35\"  \"36\"  \"37\"  \"38\"  \"39\"  \"4\"   \"40\" \n [37] \"41\"  \"42\"  \"43\"  \"44\"  \"45\"  \"46\"  \"47\"  \"48\"  \"49\"  \"5\"   \"50\"  \"51\" \n [49] \"52\"  \"53\"  \"54\"  \"55\"  \"56\"  \"57\"  \"58\"  \"59\"  \"6\"   \"60\"  \"61\"  \"62\" \n [61] \"63\"  \"64\"  \"65\"  \"66\"  \"67\"  \"68\"  \"69\"  \"7\"   \"70\"  \"71\"  \"72\"  \"73\" \n [73] \"74\"  \"75\"  \"76\"  \"77\"  \"78\"  \"79\"  \"8\"   \"80\"  \"81\"  \"82\"  \"83\"  \"84\" \n [85] \"85\"  \"86\"  \"87\"  \"88\"  \"89\"  \"9\"   \"90\"  \"91\"  \"92\"  \"93\"  \"94\"  \"95\" \n [97] \"96\"  \"97\"  \"98\"  \"99\" \n\n# Plot traits.\nplot(anolisData[, c(\"awesomeness\", \"hostility\")])\n\n\n\nplot(hostility ~ awesomeness, data = anolisData)\nfit = lm(hostility ~ awesomeness, data = anolisData)\nsummary(fit)\n\n\nCall:\nlm(formula = hostility ~ awesomeness, data = anolisData)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7035 -0.3065 -0.0416  0.2440  0.7884 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.10843    0.03953   2.743  0.00724 ** \nawesomeness -0.88116    0.03658 -24.091  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3807 on 98 degrees of freedom\nMultiple R-squared:  0.8555,    Adjusted R-squared:  0.8541 \nF-statistic: 580.4 on 1 and 98 DF,  p-value: < 2.2e-16\n\nabline(fit)\n\n\n\n\nCheck for phylogenetic signal in residuals.\n\n# Calculate weight matrix for phylogenetic distance.\nw = 1/cophenetic(anolisTree)\ndiag(w) = 0\n\nMoran.I(residuals(fit), w)\n\n$observed\n[1] 0.05067625\n\n$expected\n[1] -0.01010101\n\n$sd\n[1] 0.00970256\n\n$p.value\n[1] 3.751199e-10\n\n\nConclusion: signal in the residuals, a normal lm will not work.\nYou can also check with DHARMa, using this works also for GLMMs\n\nres = simulateResiduals(fit)\ntestSpatialAutocorrelation(res, distMat = cophenetic(anolisTree))\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res\nobserved = 0.0509093, expected = -0.0101010, sd = 0.0097304, p-value =\n3.609e-10\nalternative hypothesis: Distance-based autocorrelation\n\n\nAn old-school method to deal with the problem are the so-called Phylogenetically Independent Contrasts (PICs) (Felsenstein, J. (1985) “Phylogenies and the comparative method”. American Naturalist, 125, 1–15.). The idea here is to transform your data in a way that an lm is still appropriate. For completeness, I show the method here.\n\n# Extract columns.\nhost = anolisData[, \"hostility\"]\nawe = anolisData[, \"awesomeness\"]\n\n# Give them names.\nnames(host) = names(awe) = rownames(anolisData)\n\n# Calculate PICs.\nhPic = pic(host, anolisTree)\n\nWarning in pic(host, anolisTree): the names of argument 'x' and the tip labels\nof the tree did not match: the former were ignored in the analysis.\n\naPic = pic(awe, anolisTree)\n\nWarning in pic(awe, anolisTree): the names of argument 'x' and the tip labels of\nthe tree did not match: the former were ignored in the analysis.\n\n# Make a model.\npicModel = lm(hPic ~ aPic - 1)\n\nsummary(picModel) # Yes, significant.\n\n\nCall:\nlm(formula = hPic ~ aPic - 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.30230 -0.23485  0.06003  0.34772  0.92222 \n\nCoefficients:\n     Estimate Std. Error t value Pr(>|t|)    \naPic -0.91964    0.03887  -23.66   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4263 on 98 degrees of freedom\nMultiple R-squared:  0.851, Adjusted R-squared:  0.8495 \nF-statistic: 559.9 on 1 and 98 DF,  p-value: < 2.2e-16\n\n# plot results.\nplot(hPic ~ aPic)\nabline(a = 0, b = coef(picModel))\n\n\n\n\nNow, new school, with a PGLS\n\npglsModel = gls(hostility ~ awesomeness,\n                 correlation = corBrownian(phy = anolisTree, form =~ species),\n                 data = anolisData, method = \"ML\")\nsummary(pglsModel)\n\nGeneralized least squares fit by maximum likelihood\n  Model: hostility ~ awesomeness \n  Data: anolisData \n       AIC      BIC    logLik\n  42.26092 50.07643 -18.13046\n\nCorrelation Structure: corBrownian\n Formula: ~species \n Parameter estimate(s):\nnumeric(0)\n\nCoefficients:\n                 Value  Std.Error    t-value p-value\n(Intercept)  0.1158895 0.12500397   0.927087  0.3562\nawesomeness -0.9196414 0.03886501 -23.662451  0.0000\n\n Correlation: \n            (Intr)\nawesomeness -0.065\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.49512017 -0.75193433 -0.06672209  0.56527753  2.04613817 \n\nResidual standard error: 0.4220369 \nDegrees of freedom: 100 total; 98 residual\n\ncoef(pglsModel)\n\n(Intercept) awesomeness \n  0.1158895  -0.9196414 \n\nplot(hostility ~ awesomeness, data = anolisData)\nabline(pglsModel, col = \"red\")\n\n\n\n\nOK, same result, but PGLS is WAY more flexible than PICs. For example, we can include a discrete predictor:\n\npglsModel2 = gls(hostility ~ ecomorph,\n                    correlation = corBrownian(phy = anolisTree, form =~ species),\n                    data = anolisData, method = \"ML\")\nsummary(pglsModel2)\n\nGeneralized least squares fit by maximum likelihood\n  Model: hostility ~ ecomorph \n  Data: anolisData \n       AIC     BIC    logLik\n  235.1126 255.954 -109.5563\n\nCorrelation Structure: corBrownian\n Formula: ~species \n Parameter estimate(s):\nnumeric(0)\n\nCoefficients:\n                 Value Std.Error    t-value p-value\n(Intercept)  0.2280018 0.3630767  0.6279713  0.5316\necomorphGB  -0.2737370 0.2128984 -1.2857635  0.2017\necomorphT   -0.2773801 0.3872137 -0.7163490  0.4756\necomorphTC  -0.5457771 0.2449466 -2.2281475  0.0283\necomorphTG  -0.2645627 0.2084928 -1.2689297  0.2076\necomorphTW  -0.5388436 0.2370223 -2.2733878  0.0253\necomorphU   -0.3013944 0.2264264 -1.3310922  0.1864\n\n Correlation: \n           (Intr) ecmrGB ecmrpT ecmrTC ecmrTG ecmrTW\necomorphGB -0.385                                   \necomorphT  -0.276  0.360                            \necomorphTC -0.369  0.626  0.349                     \necomorphTG -0.426  0.638  0.431  0.608              \necomorphTW -0.372  0.626  0.377  0.588  0.641       \necomorphU  -0.395  0.597  0.394  0.587  0.647  0.666\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-2.57909973 -0.62394508  0.03716963  0.49997446  2.33859983 \n\nResidual standard error: 1.05295 \nDegrees of freedom: 100 total; 93 residual\n\nanova(pglsModel2)\n\nDenom. DF: 93 \n            numDF   F-value p-value\n(Intercept)     1 0.0555807  0.8141\necomorph        6 1.2170027  0.3046\n\n# We can even include multiple predictors:\n\npglsModel3 = gls(hostility ~ ecomorph * awesomeness,\n                correlation = corBrownian(phy = anolisTree, form =~ species),\n                data = anolisData, method = \"ML\")\nsummary(pglsModel3)\n\nGeneralized least squares fit by maximum likelihood\n  Model: hostility ~ ecomorph * awesomeness \n  Data: anolisData \n       AIC      BIC    logLik\n  53.36917 92.44673 -11.68459\n\nCorrelation Structure: corBrownian\n Formula: ~species \n Parameter estimate(s):\nnumeric(0)\n\nCoefficients:\n                            Value  Std.Error    t-value p-value\n(Intercept)             0.2740102 0.14336154   1.911323  0.0593\necomorphGB             -0.2079698 0.08757937  -2.374644  0.0198\necomorphT              -0.1751884 0.15478802  -1.131795  0.2609\necomorphTC             -0.2030466 0.10752002  -1.888454  0.0623\necomorphTG             -0.1260964 0.08339737  -1.511994  0.1342\necomorphTW             -0.1600076 0.09700188  -1.649531  0.1027\necomorphU              -0.1244498 0.09457082  -1.315943  0.1917\nawesomeness            -1.0131496 0.08971063 -11.293529  0.0000\necomorphGB:awesomeness  0.0750120 0.08289316   0.904924  0.3680\necomorphT:awesomeness   0.1373797 0.11770513   1.167152  0.2464\necomorphTC:awesomeness  0.1161086 0.11490811   1.010447  0.3151\necomorphTG:awesomeness  0.1666831 0.09824670   1.696577  0.0934\necomorphTW:awesomeness  0.0120495 0.11532810   0.104480  0.9170\necomorphU:awesomeness   0.0283477 0.10510376   0.269711  0.7880\n\n Correlation: \n                       (Intr) ecmrGB ecmrpT ecmrTC ecmrTG ecmrTW ecmrpU awsmns\necomorphGB             -0.398                                                 \necomorphT              -0.289  0.372                                          \necomorphTC             -0.361  0.598  0.357                                   \necomorphTG             -0.435  0.647  0.447  0.579                            \necomorphTW             -0.377  0.644  0.391  0.579  0.657                     \necomorphU              -0.403  0.589  0.424  0.546  0.658  0.666              \nawesomeness            -0.104  0.123  0.045  0.078  0.046  0.005  0.108       \necomorphGB:awesomeness  0.129 -0.280 -0.095 -0.171 -0.151 -0.191 -0.184 -0.682\necomorphT:awesomeness   0.082 -0.085 -0.074 -0.071 -0.036 -0.011 -0.111 -0.716\necomorphTC:awesomeness  0.102 -0.120 -0.092 -0.359 -0.079 -0.091 -0.136 -0.695\necomorphTG:awesomeness  0.090 -0.073 -0.023 -0.058 -0.056 -0.036 -0.140 -0.811\necomorphTW:awesomeness  0.051 -0.124  0.029 -0.054 -0.023 -0.052 -0.006 -0.666\necomorphU:awesomeness   0.101 -0.129 -0.129 -0.143 -0.133 -0.122 -0.283 -0.672\n                       ecmGB: ecmrT: ecmTC: ecmTG: ecmTW:\necomorphGB                                               \necomorphT                                                \necomorphTC                                               \necomorphTG                                               \necomorphTW                                               \necomorphU                                                \nawesomeness                                              \necomorphGB:awesomeness                                   \necomorphT:awesomeness   0.516                            \necomorphTC:awesomeness  0.519  0.530                     \necomorphTG:awesomeness  0.611  0.684  0.609              \necomorphTW:awesomeness  0.535  0.536  0.482  0.569       \necomorphU:awesomeness   0.515  0.535  0.644  0.626  0.480\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-1.6656909 -0.7164061 -0.1305515  0.6718348  1.7699106 \n\nResidual standard error: 0.3956912 \nDegrees of freedom: 100 total; 86 residual\n\nanova(pglsModel3)\n\nDenom. DF: 86 \n                     numDF  F-value p-value\n(Intercept)              1   0.3640  0.5479\necomorph                 6   7.9691  <.0001\nawesomeness              1 517.8319  <.0001\necomorph:awesomeness     6   0.8576  0.5295\n\n\nWe can also assume that the error structure follows an Ornstein-Uhlenbeck model rather than Brownian motion. When trying this, however, I noted that the model does not converge due to a scaling problem. We can do a quick fix by making the branch lengths longer. This will not affect the analysis other than rescaling a nuisance parameter.\n\ntempTree = anolisTree\ntempTree$edge.length = tempTree$edge.length * 100\npglsModelLambda = gls(hostility ~ awesomeness,\n                      correlation = corPagel(1, phy = tempTree, fixed = FALSE,\n                                             form =~ species),\n                      data = anolisData, method = \"ML\")\nsummary(pglsModelLambda)\n\nGeneralized least squares fit by maximum likelihood\n  Model: hostility ~ awesomeness \n  Data: anolisData \n       AIC      BIC    logLik\n  43.64714 54.06782 -17.82357\n\nCorrelation Structure: corPagel\n Formula: ~species \n Parameter estimate(s):\n lambda \n1.01521 \n\nCoefficients:\n                 Value  Std.Error    t-value p-value\n(Intercept)  0.1170472 0.12862370   0.909997  0.3651\nawesomeness -0.9248858 0.03870928 -23.893129  0.0000\n\n Correlation: \n            (Intr)\nawesomeness -0.062\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.46625592 -0.74557818 -0.06456682  0.54645141  2.02371257 \n\nResidual standard error: 0.4317018 \nDegrees of freedom: 100 total; 98 residual\n\npglsModelOU = gls(hostility ~ awesomeness,\n                   correlation = corMartins(1, phy = tempTree, form =~ species),\n                   data = anolisData)\nsummary(pglsModelOU)\n\nGeneralized least squares fit by REML\n  Model: hostility ~ awesomeness \n  Data: anolisData \n      AIC      BIC    logLik\n  50.7625 61.10237 -21.38125\n\nCorrelation Structure: corMartins\n Formula: ~species \n Parameter estimate(s):\n      alpha \n0.003194918 \n\nCoefficients:\n                 Value Std.Error    t-value p-value\n(Intercept)  0.1179388 0.4300640   0.274236  0.7845\nawesomeness -0.9148437 0.0384949 -23.765320  0.0000\n\n Correlation: \n            (Intr)\nawesomeness -0.02 \n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.11558553 -0.54574106 -0.05696661  0.40461428  1.48285458 \n\nResidual standard error: 0.5740297 \nDegrees of freedom: 100 total; 98 residual\n\n\nOther example: http://schmitzlab.info/pgls.html.\nFor fitting PGLS with various models, you should also consider the caper package.\n\n\n\n\n\n\nExcercise\n\n\n\nThe following exercise uses data from a study by Corboda et al., 2017, which examined the influence of environmental factors on the evolution of song in an group of Asian bird species called “barbets.” The following code cleans the raw data available from the EcoData package:\n\nlibrary(ape)\nlibrary(EcoData)\n\ndat = barbetData\ntree = barbetTree\n\ndat$species = row.names(dat)\nplot(tree)\n\n\n\n# dropping species in the phylogeny for which we don't have data\nobj<-geiger::name.check(tree,dat)\nreducedTree<-drop.tip(tree, obj$tree_not_data)\ngeiger::name.check(reducedTree,dat)\n\n[1] \"OK\"\n\n\nTask: Check if there is a relationship between altitude at which a species is found and the length of the note in its song, which uses the variables Lnote~Lnalt\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(Lnote~Lnalt, data = dat)\n\n\n\nfit <- lm(Lnote~ scale(Lnalt), data = dat)\nsummary(fit)\n\n\nCall:\nlm(formula = Lnote ~ scale(Lnalt), data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.11597 -0.06798 -0.03097  0.01019  0.34676 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.11652    0.01994   5.844 1.91e-06 ***\nscale(Lnalt)  0.02870    0.02025   1.418    0.166    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1145 on 31 degrees of freedom\nMultiple R-squared:  0.06087,   Adjusted R-squared:  0.03058 \nF-statistic: 2.009 on 1 and 31 DF,  p-value: 0.1663\n\nlibrary(effects)\n\nLoading required package: carData\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nplot(allEffects(fit,partial.residuals = T))\n\nWarning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\npredictor scale(Lnalt) is a one-column matrix that was converted to a vector\n\n\n\n\n\nBit of a misfit, to get a good fit, after playing around, I logged the response and added a quadratic and a cubic effect - you can probably also find other solutions.\n\nfit <- lm(log(Lnote) ~ scale(Lnalt) + I(scale(Lnalt)^2) + I(scale(Lnalt)^3), data = dat)\nsummary(fit)\n\n\nCall:\nlm(formula = log(Lnote) ~ scale(Lnalt) + I(scale(Lnalt)^2) + \n    I(scale(Lnalt)^3), data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.54583 -0.34238 -0.05857  0.41449  1.81049 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       -3.04435    0.19067 -15.967 6.64e-16 ***\nscale(Lnalt)      -0.02132    0.22516  -0.095  0.92520    \nI(scale(Lnalt)^2)  0.76725    0.21638   3.546  0.00135 ** \nI(scale(Lnalt)^3)  0.27479    0.10296   2.669  0.01233 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7305 on 29 degrees of freedom\nMultiple R-squared:  0.3376,    Adjusted R-squared:  0.2691 \nF-statistic: 4.928 on 3 and 29 DF,  p-value: 0.006911\n\nplot(allEffects(fit,partial.residuals = T))\n\nWarning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\npredictors scale(Lnalt), I(scale(Lnalt)^2), I(scale(Lnalt)^3) are one-column\nmatrices that were converted to vectors\n\nWarning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\npredictors scale(Lnalt), I(scale(Lnalt)^2), I(scale(Lnalt)^3) are one-column\nmatrices that were converted to vectors\n\nWarning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\npredictors scale(Lnalt), I(scale(Lnalt)^2), I(scale(Lnalt)^3) are one-column\nmatrices that were converted to vectors\n\n\n\n\n\nNow, with a more complex polynomial for Lnalt, how to we see if there is an overall effect of Lnalt? Easiest option is to do a LRT:\n\nfit0 = lm(log(Lnote)~ 1, data = dat)\nanova(fit0, fit)\n\nAnalysis of Variance Table\n\nModel 1: log(Lnote) ~ 1\nModel 2: log(Lnote) ~ scale(Lnalt) + I(scale(Lnalt)^2) + I(scale(Lnalt)^3)\n  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   \n1     32 23.366                                \n2     29 15.477  3    7.8893 4.9276 0.006911 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCheck residuals for phylogenetic correlation\n\nw = 1/cophenetic(reducedTree)\ndiag(w) = 0\nMoran.I(residuals(fit), w)\n\n$observed\n[1] -0.06427062\n\n$expected\n[1] -0.03125\n\n$sd\n[1] 0.03427974\n\n$p.value\n[1] 0.3354125\n\n\nNothing! So we could leave the model as it is. Just for completeness, fit the same comparison with a PGLS, effect remains significant, but p-value a bit larger.\n\nfit <- gls(Lnote~ scale(Lnalt) * I(scale(Lnalt)^2), \n           correlation = corBrownian(phy = reducedTree, \n                                     form =~ species), data = dat, \n           method = \"ML\")\nsummary(fit)\n\nGeneralized least squares fit by maximum likelihood\n  Model: Lnote ~ scale(Lnalt) * I(scale(Lnalt)^2) \n  Data: dat \n        AIC       BIC   logLik\n  -76.95977 -69.47723 43.47988\n\nCorrelation Structure: corBrownian\n Formula: ~species \n Parameter estimate(s):\nnumeric(0)\n\nCoefficients:\n                                    Value  Std.Error   t-value p-value\n(Intercept)                    0.14433555 0.07391392 1.9527520  0.0606\nscale(Lnalt)                   0.02481932 0.02700222 0.9191586  0.3656\nI(scale(Lnalt)^2)              0.03434214 0.02114728 1.6239510  0.1152\nscale(Lnalt):I(scale(Lnalt)^2) 0.01160067 0.01076432 1.0776963  0.2901\n\n Correlation: \n                               (Intr) scl(L) I((L)^\nscale(Lnalt)                    0.224              \nI(scale(Lnalt)^2)              -0.286 -0.565       \nscale(Lnalt):I(scale(Lnalt)^2) -0.230 -0.791  0.861\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-1.5858745 -0.8709001 -0.7491330 -0.3869937  2.0469866 \n\nResidual standard error: 0.1188854 \nDegrees of freedom: 33 total; 29 residual\n\n\nBtw, did you notice that neither Lnalt has a significant effect - so is there no dependence on Lnalt? Because we have the linear and quadaratic effect, the best way to answer this is to run an ANOVA:\n\nfit0 <- gls(Lnote~ 1, \n           correlation = corBrownian(phy = reducedTree, \n                                     form =~ species), data = dat, \n           method = \"ML\")\n\nanova(fit0, fit)\n\n     Model df       AIC       BIC   logLik   Test  L.Ratio p-value\nfit0     1  2 -73.47388 -70.48087 38.73694                        \nfit      2  5 -76.95977 -69.47723 43.47988 1 vs 2 9.485883  0.0235\n\n\nwhich tells us that Lnalt has a significant effect, even though we can’t reject that either the linear or the quadrart term is zero.\nAddition: what would happen if we do the same with a misspecified model? Have a look at the p-values of the fitted models. Can you explain what’s going on here?\n\nfit <- lm(Lnote~ scale(Lnalt), data = dat)\nsummary(fit)\n\n\nCall:\nlm(formula = Lnote ~ scale(Lnalt), data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.11597 -0.06798 -0.03097  0.01019  0.34676 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.11652    0.01994   5.844 1.91e-06 ***\nscale(Lnalt)  0.02870    0.02025   1.418    0.166    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1145 on 31 degrees of freedom\nMultiple R-squared:  0.06087,   Adjusted R-squared:  0.03058 \nF-statistic: 2.009 on 1 and 31 DF,  p-value: 0.1663\n\nplot(allEffects(fit, partial.residuals = T))\n\nWarning in Analyze.model(focal.predictors, mod, xlevels, default.levels, : the\npredictor scale(Lnalt) is a one-column matrix that was converted to a vector\n\n\n\n\nw = 1/cophenetic(reducedTree)\ndiag(w) = 0\nMoran.I(residuals(fit), w)\n\n$observed\n[1] -0.05831654\n\n$expected\n[1] -0.03125\n\n$sd\n[1] 0.0334855\n\n$p.value\n[1] 0.4189142\n\nfit <- gls(Lnote~ scale(Lnalt), \n           correlation = corBrownian(phy = reducedTree, \n                                     form =~ species), data = dat, \n           method = \"ML\")\nsummary(fit)\n\nGeneralized least squares fit by maximum likelihood\n  Model: Lnote ~ scale(Lnalt) \n  Data: dat \n        AIC      BIC   logLik\n  -77.67763 -73.1881 41.83881\n\nCorrelation Structure: corBrownian\n Formula: ~species \n Parameter estimate(s):\nnumeric(0)\n\nCoefficients:\n                  Value  Std.Error  t-value p-value\n(Intercept)  0.18020988 0.07194706 2.504757  0.0177\nscale(Lnalt) 0.03941982 0.01556802 2.532103  0.0166\n\n Correlation: \n             (Intr)\nscale(Lnalt) 0.15  \n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-1.5260202 -1.0620664 -0.7067039 -0.4679653  2.1556803 \n\nResidual standard error: 0.1249469 \nDegrees of freedom: 33 total; 31 residual\n\n\nThe observation is that the PGLS effect estimate is significant while normal lm is not. The reason is probably that the PGLS is re-weighting residuals, and it seems that in this case, the re-weighting is changing the slope. What we learn by this example is that a PGLS can increase significance, and in this case I would argue wrongly so, as we have no indication that there is a phylogenetic signal. I would therefore NOT recommend to blindly fit PGLS, but rather test first if a PGLS is needed, and only then apply."
  },
  {
    "objectID": "4C-CorrelationStructures.html#multivariate-glms",
    "href": "4C-CorrelationStructures.html#multivariate-glms",
    "title": "10  Correlation structures",
    "section": "10.5 Multivariate GLMs",
    "text": "10.5 Multivariate GLMs\nIn the recent years, multivariate GLMs, in particular the multivariate probit model, and latent variable versions thereof, have become popular for the analysis of community data. The keyword here is “joint species distribution models” (jSDMs).\nBriefly, what we want a jSDM to do is to fit a vector of responses (could be abundance or presence / absence data) as a function of environmental predictors and a covariance between the responses. The model is thus\n\\[\ny \\sim f(x) + \\Sigma\n\\] where y is a response vector, f(x) is a matrix with effects, and \\(\\Sigma\\) is a covariance matrix that we want to estimate.\nYou can fit these models directly in lme4 or glmmTMB via implementing an RE per species\n\nlme4(abundance ~ 0 + species + env:species + (0+species|site)\nglmmTMB(abundance ~ 0 + species + env:species + (0+species|site)\n\nhowever, these so-called full-rank models have a lot of degrees of freedom and are slow to compute if the number of species gets large. It has therefore become common to fit rank-reduced versions of the these models using a latent-variable reparameterization of the model above. A latent-variable version of this model can be fit in glmmTMB via this syntax\n\nglmmTMB(abundance ~ 0 + species + env:species + rr(Species + 0|id, d = 2))\n\nThe parameter d controlls the number of latent variables.\nOf course, there are many more specialized packages for fitting latent-variable jSDMs in R right now, including hmsc, gllvm or sjSDM, but I find it nice to set this models in the general topic of correlations, and realize that we can fit them with the same methods and packages as for all other correlation structures."
  },
  {
    "objectID": "5A-Summary.html",
    "href": "5A-Summary.html",
    "title": "11  Summary and concluding thoughts",
    "section": "",
    "text": "Things to note:\n\nFor an lm, the link function is the identity function.\nFixed effects \\(\\operatorname{f}(x)\\) can be either a polynomial \\(\\left( a \\cdot x = b \\right)\\) = linear regression, a nonlinear function = nonlinear regression, or a smooth spline = generalized additive model (GAM).\nRandom effects assume normal distribution for groups.\nRandom effects can also act on fixed effects (random slope).\nFor an lm with correlation structure, C is integrated in Dist. For all other GLMMs, there is another distribution, plus the additional multivariate normal on the linear predictor.\n\nStrategy for analysis:\n\nDefine formula via scientific questions + confounders.\nDefine type of GLM (lm, logistic, Poisson).\nBlocks in data -> Random effects, start with random intercept.\n\nFit this base model, then do residual checks for\n\nWrong functional form -> Change fitted function.\nWrong distribution-> Transformation or GLM adjustment.\n(Over)dispersion -> Variable dispersion GLM.\nHeteroskedasticity -> Model dispersion.\nZero-inflation -> Add ZIP term.\nCorrelation -> Add correlation structure.\n\nAnd adjust the model accordingly.\nPackages to fit thes models (see Apppedix: syntax cheat sheet):\n\nbaseR: lm, glm.\nlme4: mixed models, lmer, glmer.\nmgcv: GAM.\nnlme: Variance and correlations structure modelling for linear (mixed) models, using gls + lme.\nglmmTMB: Generalized linear mixed models with variance / correlation modelling and zip term.\n\nAlternatively, you could move to Bayesian estimates with brms, which allows all that, easier inclusion of shrinkage, and p-value / df problems less visible"
  },
  {
    "objectID": "5A-Summary.html#thoughts-about-the-analysis-pipeline",
    "href": "5A-Summary.html#thoughts-about-the-analysis-pipeline",
    "title": "12  Summary and concluding thoughts",
    "section": "12.2 Thoughts About the Analysis Pipeline",
    "text": "12.2 Thoughts About the Analysis Pipeline\nIn statistics, we rarely use a simple analysis. We often use an entire pipeline, consisting, for example, of the protocol that I sketched in chapter @ref(protocol). What we should constantly ask ourselves: Is our pipeline good? By “good”, we typically mean: If 1000 analyses are run in that way:\n\nWhat is the typical error of the estimate?\nWhat is the Type I error (false positives)?\nAre the confidence intervals correctly calculated?\n…\n\nThe way to check this is to run simulations. For example, the following function creates data that follows the assumptions of a linear regression with slope 0.5, then fits a linear regression, and returns the estimate\n\ngetEstimate = function(n = 100){\n  x = runif(n)\n  y = 0.5 * x + rnorm(n)\n  fit = lm(y ~ x)\n  x = summary(fit)\n  return(x$coefficients[2, 1])  # Get fitted x weight (should be ~0.5).\n}\n\nThe replicate function allows us to execute this 1000 times:\n\nset.seed(543210)\n\nout = replicate(1000, getEstimate())\n\nPlotting the result, we can check whether the linear regression is an unbiased estimator for the slope.\n\nhist(out, breaks = 50)\nabline(v = 0.5, col = \"red\")\n\n\n\n\n“Unbiased” means that, while each single estimate will have some error, the mean of many estimates will spread around the true value.\nExplicitly calculating these values\nBias\n\nmean(out) - 0.5 # Should be ~0.\n\n[1] -0.001826401\n\n\nVariance / standard deviation of the estimator\n\nsd(out)\n\n[1] 0.3587717\n\n\nTo check p-values, we could run:\n\nset.seed(12345)\n\ngetEstimate = function(n = 100){  # Mind: Function has changed!\n  x = runif(n)\n  y = rnorm(n)  # No dependence of x! Identical: y = 0 * x + rnorm(100).\n  fit = lm(y ~ x)\n  x = summary(fit)\n  return(x$coefficients[2, 4])  # P-value for H0: Weight of x = 0.\n}\n\nout = replicate(2000, getEstimate())\n\nhist(out) # Expected: Uniformly distributed p-values. -&gt; Check.\n\n\n\nmean(out &lt; 0.05) # Expected: ~0.05. But this is NO p-value... Check H0/H1!\n\n[1] 0.0515\n\n# Explanation of syntax: Logical vectors are interpreted as vectors of 0s and 1s.\n\nTo check the properties of other, possibly more complicated pipelines, statisticians will typically use the same technique. I recommend doing this! For example, you could modify the function above to have a non-normal error. How much difference does that make? Simulating often beats recommendations in the books!"
  },
  {
    "objectID": "5B-References.html",
    "href": "5B-References.html",
    "title": "Appendix A — References",
    "section": "",
    "text": "Bates, Douglas. 2011. “Computational Methods for Mixed\nModels.” Vignette for Lme4.\n\n\nBhaskaran, Krishnan, and Liam Smeeth. 2014. “What Is the\nDifference Between Missing Completely at Random and Missing at\nRandom?” International Journal of Epidemiology 43 (4):\n1336–39.\n\n\nHector, Andy, Stefanie Von Felten, and Bernhard Schmid. 2010.\n“Analysis of Variance with Unbalanced Data: An Update for Ecology\n& Evolution.” Journal of Animal Ecology 79 (2):\n308–16.\n\n\nVerbyla, Arunas Petras. 2019. “A Note on Model Selection Using\nInformation Criteria for General Linear Models Estimated Using\nREML.” Australian & New Zealand Journal of\nStatistics 61 (1): 39–50. https://doi.org/10.1111/anzs.12254."
  },
  {
    "objectID": "6A-RCrashCourse.html",
    "href": "6A-RCrashCourse.html",
    "title": "Appendix A — A crashcourse in R",
    "section": "",
    "text": "This Appendix reminds you about basic R data types and how to operate on them. Also suitable for self-study prior to the course!"
  },
  {
    "objectID": "6A-RCrashCourse.html#representing-data-in-r",
    "href": "6A-RCrashCourse.html#representing-data-in-r",
    "title": "Appendix B — A crashcourse in R",
    "section": "B.1 Representing Data in R",
    "text": "B.1 Representing Data in R\n\nB.1.1 Exploring Data Structures\nA fundamental requirement for working with data is representing it in a computer. In R, we can either read in data (e.g. with functions such as read.table()), or we can assign variables certain values. For example, if I type\n\nx &lt;- 1\n\nthe variable x now contains some data, namely the value 1, and I can use x in as a placeholder for the data it contains in further calculations.\nAlternatively to the &lt;- operator, you can also use = (in all circumstances that you are likely to encounter, it’s the same).\n\nx = 1\n\nIf you have worked with R previously, this should all be familiar to you, and you should also know that the commands\n\nclass(x)\ndim(x)\nstr(x)\n\nallow you to explore the structure of variables and the data they contain. Ask yourself, or discuss with your partner(s):\n\n\n\n\n\n\nExcercise\n\n\n\nWhat is the meaning of the three functions, and what is the structure / properties of the following data types in R:\n\nAtomic types (which atomic types exist),\nlist,\nvector,\ndata.frame,\nmatrix,\narray.\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAtomic types: e.g. numeric, factor, boolean …; List can have severa tyes, vector not! data.frame is list of vectors. matrix is 2-dim array, array can have any dim, only one type.\n\n\n\n\n\n\n\n\n\nExcercise\n\n\n\nWhat is the data type of the iris data set, which is built-in in R under the name\n\niris\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIris, like most simple datasets, is of type data.frame\n\n\n\n\n\nB.1.2 Dynamic Typing\nR is a dynamically typed language, which means that the type of variables is determined automatically depending on what values you supply. Try this:\n\nx = 1\nclass(x)\nx = \"dog\"\nclass(x)\n\nThis also works if a data set already exists, i.e. if you assign a different value, the type will automatically be changed. Look at what happens when we assign a character value to a previously numeric column in a data.frame:\n\niris$Sepal.Length[2] = \"dog\"\nstr(iris)\n\nNote that all numeric values are changed to characters as well. You can try to force back the values to numeric by:\n\niris$Sepal.Length = as.numeric(iris$Sepal.Length)\n\nHave a look at what this does to the values in iris$Sepal.Length.\nNote: The actions above operate on a local copy of the iris data set. You don’t overwrite the base data and can use it again in a new R session or reset it with data(iris)."
  },
  {
    "objectID": "6A-RCrashCourse.html#data-selection-slicing-and-subsetting",
    "href": "6A-RCrashCourse.html#data-selection-slicing-and-subsetting",
    "title": "Appendix B — A crashcourse in R",
    "section": "B.2 Data Selection, Slicing and Subsetting",
    "text": "B.2 Data Selection, Slicing and Subsetting\n\nB.2.1 Subsetting and Slicing for Single Data Types\nWe often want to select only a subset of our data. You can generally subset from data structures using indices and TRUE/FALSE (or T/F). Here for a vector:\n\nvector = 1:6\nvector[1] # First element.\nvector[1:3] # Elements 1, 2, 3.\nvector[c(1, 5, 6)]  # Elements 1, 5, 6.\nvector[c(T, T, F, F, T)]  # Elements 1, 2, 5.\n\nCareful, special behavior of R: If you specify fewer values than needed, the input vector will be repeated. This is called “recycling”.\n\nvector[c(T, F)] # Does NOT work!\n\nFor a list, it’s basically the same, except the following points:\n\nElements in lists usually have a name, so you can also access those via list$name.\nLists accessed with [] return a list. If you want to select a single element, you have to access it via [[]], as in list[[2]].\n\n\nmyList = list(a = 1, b = \"dog\", c = TRUE)\nmyList[1]\n\n$a\n[1] 1\n\nmyList[[1]]\n\n[1] 1\n\nmyList$a\n\n[1] 1\n\n\nFor data.frames and other objects with dimension &gt; 2, the same is true, except that you have several indices.\n\nmatrix = matrix(1:16, nrow = 4)\nmatrix[1, 2]  # Element in first row, second column.\nmatrix[1:2,]  # First two rows, all columns.\nmatrix[, c(T, F ,T)]  # All rows, 1st and 3rd column.\n\nThe syntax matrix[1,] is also called slicing, for obvious reasons.\nData.frames are the same as matrices, except that, like with lists of vectors, you can also access columns via names as in data.frame$column. This is because a data.frame ist a list of vectors.\n\n\nB.2.2 Logic and Slicing\nSlicing is very powerful if you combine it with logical operators, such as “&” (logical and), “|” (logical or), “==” (equal), “!=” (not equal), “&lt;=”, “&gt;”, etc. Here are a few examples:\n\nhead(iris[iris$Species == \"virginica\", ])  \n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n101          6.3         3.3          6.0         2.5 virginica\n102          5.8         2.7          5.1         1.9 virginica\n103          7.1         3.0          5.9         2.1 virginica\n104          6.3         2.9          5.6         1.8 virginica\n105          6.5         3.0          5.8         2.2 virginica\n106          7.6         3.0          6.6         2.1 virginica\n\n\nNote that this is identical to using the subset command:\n\nhead(subset(iris, Species == \"virginica\"))  \n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n101          6.3         3.3          6.0         2.5 virginica\n102          5.8         2.7          5.1         1.9 virginica\n103          7.1         3.0          5.9         2.1 virginica\n104          6.3         2.9          5.6         1.8 virginica\n105          6.5         3.0          5.8         2.2 virginica\n106          7.6         3.0          6.6         2.1 virginica\n\n\nYou can also combine several logical commands:\n\niris[iris$Species == \"virginica\" & iris$Sepal.Length &gt; 7, ]\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n103          7.1         3.0          5.9         2.1 virginica\n106          7.6         3.0          6.6         2.1 virginica\n108          7.3         2.9          6.3         1.8 virginica\n110          7.2         3.6          6.1         2.5 virginica\n118          7.7         3.8          6.7         2.2 virginica\n119          7.7         2.6          6.9         2.3 virginica\n123          7.7         2.8          6.7         2.0 virginica\n126          7.2         3.2          6.0         1.8 virginica\n130          7.2         3.0          5.8         1.6 virginica\n131          7.4         2.8          6.1         1.9 virginica\n132          7.9         3.8          6.4         2.0 virginica\n136          7.7         3.0          6.1         2.3 virginica"
  },
  {
    "objectID": "6A-RCrashCourse.html#applying-functions-and-aggregates-across-a-data-set",
    "href": "6A-RCrashCourse.html#applying-functions-and-aggregates-across-a-data-set",
    "title": "Appendix B — A crashcourse in R",
    "section": "B.3 Applying Functions and Aggregates Across a Data Set",
    "text": "B.3 Applying Functions and Aggregates Across a Data Set\nApart from selecting data, you will often combine or calculate statistics on data.\n\nB.3.1 Functions\nMaybe this is a good time to remind you about functions. The two basic options we use in R are:\n\nVariables / data structures.\nFunctions.\n\nWe have already used variables / data structures. Variables have a name and if you type this name in R, you get the values that are inside the respective data structure.\nFunctions are algorithms that are called like:\n\nfunction(variable)\n\nFor example, you can do:\n\nsummary(iris)\n\nIf you want to know what the summary function does, type ?summary, or put your mouse on the function and press “F1”.\nTo be able to work properly with data, you have to know how to define your own functions. This works like the following:\n\nsquareValue = function(x){\n  temp = x * x \n  return(temp)\n}\n\n  \n  Tasks\n\nTry what happens if you type in squareValue(2).\nWrite a function for multiplying 2 values. Hint: This should start with function(x1, x2).\nChange the first line of the squareValue function to function(x = 3) and try out the following commands: squareValue(2), squareValue(). What is the sense of this syntax?\n\n  \n    \n      Solution\n    \n    \n1\n\nmultiply = function(x1, x2){\n  return(x1 * x2)\n}\n\n2\n\nsquareValue(2)\n\n[1] 4\n\n\n3\n\nsquareValue = function(x = 3){\n  temp = x * x \n  return(temp)\n}\n\nsquareValue(2)\n\n[1] 4\n\nsquareValue()\n\n[1] 9\n\n\nThe given value (3 in the example above) is the default value. This value is used automatically, if no value is supplied for the respective variable. Default values can be specified for all variables, but you should put them to the end of the function definition. Hint: In R, it is always useful to name the parameters when using functions.\nLook at the following example:\n\ntestFunction = function(a = 1, b, c = 3){\n  return(a * b + c)\n}\n\ntestFunction()\n\nError in testFunction(): argument \"b\" is missing, with no default\n\ntestFunction(10)\n\nError in testFunction(10): argument \"b\" is missing, with no default\n\ntestFunction(10, 20)\n\n[1] 203\n\ntestFunction(10, 20, 30)\n\n[1] 230\n\ntestFunction(b = 10, c = 20, a = 30)\n\n[1] 320\n\n\n    \n  \n  \n\n\nB.3.2 The apply() Function\nNow that we know functions, we can introduce functions that use functions. One of the most important is the apply function. The apply function applies a function of a data structure, typically a matrix or data.frame.\nTry the following:\n\napply(iris[,1:4], 2, mean)\n\n  \n  Tasks\n\nCheck the help of apply to understand what this does.\nWhy is the first result of apply(iris[,1:4], 2, mean) NA? Check the help of mean to understand this.\nTry apply(iris[,1:4], 1, mean). Think about what has changed here.\nWhat would happen if you use iris instead of iris[,1:4]?\n\n  \n    \n      Solution\n    \n    \n1\n\n?apply\n\n2\nRemember, what we have done above (if you run this part separately, execute the following lines again):\n\niris$Sepal.Length[2] = \"Hund\"\niris$Sepal.Length = as.numeric(iris$Sepal.Length)\n\nWarning: NAs introduced by coercion\n\n\n\napply(iris[,1:4], 2, mean)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n          NA     3.057333     3.758000     1.199333 \n\n\nTaking the mean of a character sequence is not possible, so the result is NA (Not Available, missing value(s)).\nBut you can skip missing values with the option na.rm = TRUE of the mean function. To use it with the apply function, pass the argument(s) after.\n\napply(iris[,1:4], 2, mean, na.rm = T)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.849664     3.057333     3.758000     1.199333 \n\n\n3\n\napply(iris[,1:4], 1, mean)\n\n  [1] 2.550    NA 2.350 2.350 2.550 2.850 2.425 2.525 2.225 2.400 2.700 2.500\n [13] 2.325 2.125 2.800 3.000 2.750 2.575 2.875 2.675 2.675 2.675 2.350 2.650\n [25] 2.575 2.450 2.600 2.600 2.550 2.425 2.425 2.675 2.725 2.825 2.425 2.400\n [37] 2.625 2.500 2.225 2.550 2.525 2.100 2.275 2.675 2.800 2.375 2.675 2.350\n [49] 2.675 2.475 4.075 3.900 4.100 3.275 3.850 3.575 3.975 2.900 3.850 3.300\n [61] 2.875 3.650 3.300 3.775 3.350 3.900 3.650 3.400 3.600 3.275 3.925 3.550\n [73] 3.800 3.700 3.725 3.850 3.950 4.100 3.725 3.200 3.200 3.150 3.400 3.850\n [85] 3.600 3.875 4.000 3.575 3.500 3.325 3.425 3.775 3.400 2.900 3.450 3.525\n [97] 3.525 3.675 2.925 3.475 4.525 3.875 4.525 4.150 4.375 4.825 3.400 4.575\n[109] 4.200 4.850 4.200 4.075 4.350 3.800 4.025 4.300 4.200 5.100 4.875 3.675\n[121] 4.525 3.825 4.800 3.925 4.450 4.550 3.900 3.950 4.225 4.400 4.550 5.025\n[133] 4.250 3.925 3.925 4.775 4.425 4.200 3.900 4.375 4.450 4.350 3.875 4.550\n[145] 4.550 4.300 3.925 4.175 4.325 3.950\n\n\nArrays (and thus matrices, data.frame(s), etc.) have several dimensions. For a simple 2D array (or matrix), the first dimension is the rows and the second dimension is the columns. The second parameter of the “apply” function specifies the dimension of which the mean should be computed. If you use 1, you demand the row means (150), if you use 2, you request the column means (5, resp. 4).\n4\n\napply(iris, 2, mean)\n\nWarning in mean.default(newX[, i], ...): argument is not numeric or logical:\nreturning NA\n\nWarning in mean.default(newX[, i], ...): argument is not numeric or logical:\nreturning NA\n\nWarning in mean.default(newX[, i], ...): argument is not numeric or logical:\nreturning NA\n\nWarning in mean.default(newX[, i], ...): argument is not numeric or logical:\nreturning NA\n\nWarning in mean.default(newX[, i], ...): argument is not numeric or logical:\nreturning NA\n\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n          NA           NA           NA           NA           NA \n\n\nThe 5th column is “Species”. These values are not numeric. So the whole data.frame is taken as a data.frame full of characters.\n\napply(iris[,1:4], 2, str)\n\n num [1:150] 5.1 NA 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n\n\nNULL\n\napply(iris, 2, str)\n\n chr [1:150] \"5.1\" NA \"4.7\" \"4.6\" \"5.0\" \"5.4\" \"4.6\" \"5.0\" \"4.4\" \"4.9\" \"5.4\" ...\n chr [1:150] \"3.5\" \"3.0\" \"3.2\" \"3.1\" \"3.6\" \"3.9\" \"3.4\" \"3.4\" \"2.9\" \"3.1\" ...\n chr [1:150] \"1.4\" \"1.4\" \"1.3\" \"1.5\" \"1.4\" \"1.7\" \"1.4\" \"1.5\" \"1.4\" \"1.5\" ...\n chr [1:150] \"0.2\" \"0.2\" \"0.2\" \"0.2\" \"0.2\" \"0.4\" \"0.3\" \"0.2\" \"0.2\" \"0.1\" ...\n chr [1:150] \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" \"setosa\" ...\n\n\nNULL\n\n\nRemark: The “NULL” statement is the return value of apply. str returns nothing (but prints something out), so the returned vector (or array, list, …) is empty, just like:\n\nc()\n\nNULL\n\n\n    \n  \n  \n\n\nB.3.3 The aggregate() Function\naggregate() calculates a function per grouping variable. Try out this example:\n\naggregate(. ~ Species, data = iris, FUN = max)\n\n     Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     setosa          5.8         4.4          1.9         0.6\n2 versicolor          7.0         3.4          5.1         1.8\n3  virginica          7.9         3.8          6.9         2.5\n\n\nNote that max` is the function to get the maximum value, and has nothing to do with your lecturer, who should be spelled Max.\nThe dot is general R syntax and usually refers to “use all columns in the data set”.\n\n\nB.3.4 For loops\nApply and aggregate are convenience function for a far more general concept that exists in all programming language, which is the for loop. In R, a for loop look like this:\n\nfor (i in 1:10){\n  #doSomething\n}\n\nand if it is executed, it will excecute 10 times the main block in the curly brackes, while counting the index variable i from 1:10. To demonstrate this, let’s execute a shorter for lool, going from 1:3, and printing i\n\nfor (i in 1:3){\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n\n\nFor loops are very useful when you want to execute the same task many times. This can be for plotting, but also for data manipulation. For example, if I would like to re-programm the apply function with a for loop, it would look like that:\n\napply(iris[,1:4], 2, mean, na.rm = T)\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.849664     3.057333     3.758000     1.199333 \n\nout = rep(NA, 4)\nfor (i in 1:4){\n  out[i] = mean(iris[,i])\n}\nout\n\n[1]       NA 3.057333 3.758000 1.199333"
  },
  {
    "objectID": "6A-RCrashCourse.html#plotting",
    "href": "6A-RCrashCourse.html#plotting",
    "title": "Appendix B — A crashcourse in R",
    "section": "B.4 Plotting",
    "text": "B.4 Plotting\nI assume that you have already made plots with R. Else here is a super-quick 5-min introduction video. In this course, we will not be using a lot graphics, but it will be useful for you to know the basic plot commands. Note in particular that the following two commands are identical:\n\nplot(iris$Sepal.Length, iris$Sepal.Width)\nplot(Sepal.Width ~ Sepal.Length, data = iris)\n\nThe second option is preferable, because it allows you to subset data easier and can be directly copied to regression functions.\n\nplot(Sepal.Width ~ Sepal.Length, data = iris[iris$Species == \"versicolor\", ])\n\n\n\n\nThe plot command will use the standard plot depending on the type of variable supplied. For example, if the x axis is a factor, a boxplot will be produced.\n\nplot(Sepal.Width ~ Species, data = iris)\n\n\n\n\nYou can change color, size, shape etc. and this is often useful for visualization.\n\nplot(iris$Sepal.Length, iris$Sepal.Width, col = iris$Species,\n     cex = iris$Petal.Length)\n\n\n\n\nFor more help on plotting, I recommend:\n\nRead “Fundamentals of Data Visualization” by Claus O. Wilke (explains all standard plots and why / when to use them)\nData to Viz provides a decision tree for visualizations and links to the R graph gallery"
  },
  {
    "objectID": "6B-CheatSheet.html",
    "href": "6B-CheatSheet.html",
    "title": "Appendix B — Cheat sheet",
    "section": "",
    "text": "Formula\nInterpretation\n\n\n\n\ny ~ x\nIntercept + slope\n\n\ny ~ 1\nintercept only\n\n\ny ~ x - 1 or x + 0\nonly slope, for categorical\n\n\nlog(x) ~ sqrt(x)\npredictor and response variable can be transformed\n\n\ny ~ x1 + x2\nmultiple predictors can be added\n\n\ny ~ x1 * x2\ninteraction and main effects\n\n\ny ~ x1:x2\nonly interaction\n\n\ny ~ (x1 + x2)^2\nall 2nd order interactions, works also with 3rd order ^3 etc\n\n\ny ~ I(x^2)\nI() forces mathematical interpretation of whatever is in the parenthesis\n\n\ny ~ s(x)\nspline (works only for mgcv)\n\n\ny ~ te(x1, x2)\ntesnsor spline (works only for mgcv)\n\n\ny ~.\ninclude all variables as main effects\n\n\ny~.^2\ninclude all variables as main effects and their 2nd order interactions"
  },
  {
    "objectID": "6B-CheatSheet.html#random-effect-syntax",
    "href": "6B-CheatSheet.html#random-effect-syntax",
    "title": "Appendix B — Cheat sheet",
    "section": "B.2 Random effect syntax",
    "text": "B.2 Random effect syntax\nThe following formulas are used by many R packages (e.g. glmmTMB and lme4). For a detailed overview, see Ben Bolkers glmmFAQ\n\n\n\n\n\n\n\nFormula\nInterpretation\n\n\n\n\n(1|group)\nRandom intercept within group\n\n\n(x|group)\nRandom intercept within group and random slope for x within group; correlation between random intercept and random slope\n\n\n(0+x|group) or (x-1|group)\nRandom slope for x within group without random intercept for group\n\n\n(0+x|group) + (1|group) or (x||group)\nRandom intercept and slope but uncorrelated\n\n\n(1|group:site)\nRandom intercept for sites within group (nested random effect)\n\n\n(1|group) + (1|site)\nRandom intercept within groups and within sites (crossed random effects)\n\n\n\n\nB.2.1 Correlation structures:\n\nB.2.1.1 glmmTMB\nglmmTMB supports several correlation structures but we will show here only two common representatives. See also the glmmTMB vignette about correlation structures.\n\n\n\n\n\n\n\n\nType\nFormula\nInterpretation\n\n\n\n\nSpatial\ncoords = numFactor(cbind(Latitude, Longitude))\ny~x + exp(0+coords|group)\nUse glmmTMB::numFactor() function to represent your coordinates as a factor.\nConditional autoregressive random intercepts within group based on the coords (sites).\n\n\nTemporal\nyearFactor = factor(years)\ny~x + ar1(0+yearFactor|group)\nChange your time variable to factor.\nConditional autoregressive random intercepts within group based on years.\n\n\n\n\n\nB.2.1.2 lme4\nlme4 does not support correlation structures\n\n\nB.2.1.3 nlme::gls\n\n\n\n\n\n\n\n\nType\nFormula\nInterpretation\n\n\n\n\nSpatial\ngls(y~.., correlation = corExp( ~ Lat + Long|group)\nSpatial correlation structure (no random effects) between sites within group (group is optional)\n\n\nTemporal\ngls(y~.., correlation = corAR1( ~ years | group)\nTemoral correlation structure (no random effects) between years within group (group is optional)\n\n\nPhylogeny\nlibrary(ape)gls(y~.., correlation = corBrownian(phy = pyhTree, form ~ species | group)\nPyhlogenetic correlation structure (no random effects) between species within group (group is optional)"
  },
  {
    "objectID": "6C-CaseStudies.html",
    "href": "6C-CaseStudies.html",
    "title": "Appendix C — Case studies",
    "section": "",
    "text": "General strategy for analysis:\nFit this base model, then do residual checks for\nAnd adjust the model accordingly."
  },
  {
    "objectID": "6C-CaseStudies.html#hurricanes",
    "href": "6C-CaseStudies.html#hurricanes",
    "title": "Appendix C — Case studies",
    "section": "C.1 Hurricanes",
    "text": "C.1 Hurricanes\nIn https://www.pnas.org/content/111/24/8782, Jung et al. claim that “Female hurricanes are deadlier than male hurricanes”.\nSpecifically, they analyze the number of hurricane fatalities, and claim that there is an effect of the femininity of the name on the number of fatalities, correcting for several possible confounders. They interpret the result as causal (including mediators), claiming that giving only male names to hurricanes would considerably reduce death toll.\nThe data is available in DHARMa.\n\nlibrary(DHARMa)\nlibrary(mgcv)\n?hurricanes\nstr(hurricanes)\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   92 obs. of  14 variables:\n $ Year                    : num  1950 1950 1952 1953 1953 ...\n $ Name                    : chr  \"Easy\" \"King\" \"Able\" \"Barbara\" ...\n $ MasFem                  : num  6.78 1.39 3.83 9.83 8.33 ...\n $ MinPressure_before      : num  958 955 985 987 985 960 954 938 962 987 ...\n $ Minpressure_Updated_2014: num  960 955 985 987 985 960 954 938 962 987 ...\n $ Gender_MF               : num  1 0 0 1 1 1 1 1 1 1 ...\n $ Category                : num  3 3 1 1 1 3 3 4 3 1 ...\n $ alldeaths               : num  2 4 3 1 0 60 20 20 0 200 ...\n $ NDAM                    : num  1590 5350 150 58 15 ...\n $ Elapsed_Yrs             : num  63 63 61 60 60 59 59 59 58 58 ...\n $ Source                  : chr  \"MWR\" \"MWR\" \"MWR\" \"MWR\" ...\n $ ZMasFem                 : num  -0.000935 -1.670758 -0.913313 0.945871 0.481075 ...\n $ ZMinPressure_A          : num  -0.356 -0.511 1.038 1.141 1.038 ...\n $ ZNDAM                   : num  -0.439 -0.148 -0.55 -0.558 -0.561 ...\n\n\nSome plots:\n\nplot(hurricanes$MasFem, hurricanes$NDAM, cex = 0.5, pch = 5)\npoints(hurricanes$MasFem, hurricanes$NDAM, cex = hurricanes$alldeaths/20,\n       pch = 4, col= \"red\")\n\n\n\n\nThe original model from the paper fits a negative binomial, using mgcv.{R}. I suppose the reason is mainly that glmmTMB was not available at the time, and implementations of the negative binomial, in particular mass::glm.nb and lme4::glmer.nb often had convergence problems.\n\noriginalModelGAM = gam(alldeaths ~ MasFem * (Minpressure_Updated_2014 + NDAM),\n    data = hurricanes, family = nb, na.action = \"na.fail\")\nsummary(originalModelGAM)\n\n\nFamily: Negative Binomial(0.736) \nLink function: log \n\nFormula:\nalldeaths ~ MasFem * (Minpressure_Updated_2014 + NDAM)\n\nParametric coefficients:\n                                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                      7.014e+01  2.003e+01   3.502 0.000462 ***\nMasFem                          -5.986e+00  2.529e+00  -2.367 0.017927 *  \nMinpressure_Updated_2014        -7.008e-02  2.060e-02  -3.402 0.000669 ***\nNDAM                            -3.845e-05  2.945e-05  -1.305 0.191735    \nMasFem:Minpressure_Updated_2014  6.124e-03  2.603e-03   2.352 0.018656 *  \nMasFem:NDAM                      1.593e-05  3.756e-06   4.242 2.21e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  -3.61e+03   Deviance explained = 57.4%\n-REML = 357.56  Scale est. = 1         n = 92\n\n\n\n\n\n\n\n\nCaution\n\n\n\nTasks:\n\nConfirm that you get the same results as in the paper. It makes sense to translate their model to glmmTMB. Note that the nb parameterization of mgcv corresponds to nbinom2 in glmmTMB. You will get different results when choosing nbinom1\nInspect the fitted model for potential problems, in particular perform a residual analysis of the model, including residuals against all predictors, and improve the model if you find problems.\nForget what they did. Go back to start, do a causal analysis like we did, and do your own model, diagnosing all residual problems that we discussed. Do you think there is an effect of femininity?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is the model fit by Jung et al., fit with glmmTMB\n\nlibrary(DHARMa)\nlibrary(glmmTMB)\n\nWarning in checkMatrixPackageVersion(): Package version inconsistency detected.\nTMB was built with Matrix version 1.4.1\nCurrent Matrix version is 1.5.4.1\nPlease re-install 'TMB' from source using install.packages('TMB', type = 'source') or ask CRAN for a binary version of 'TMB' matching CRAN's 'Matrix' package\n\n\nWarning in checkDepPackageVersion(dep_pkg = \"TMB\"): Package version inconsistency detected.\nglmmTMB was built with TMB version 1.9.3\nCurrent TMB version is 1.9.1\nPlease re-install glmmTMB from source or restore original 'TMB' package (see '?reinstalling' for more information)\n\nm1 = glmmTMB(alldeaths ~ MasFem*\n                             (Minpressure_Updated_2014 + scale(NDAM)),\n                           data = hurricanes, family = nbinom2)\nsummary(m1)\n\n Family: nbinom2  ( log )\nFormula:          alldeaths ~ MasFem * (Minpressure_Updated_2014 + scale(NDAM))\nData: hurricanes\n\n     AIC      BIC   logLik deviance df.resid \n   660.7    678.4   -323.4    646.7       85 \n\n\nDispersion parameter for nbinom2 family (): 0.787 \n\nConditional model:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                     69.661590  23.425598   2.974 0.002942 ** \nMasFem                          -5.855078   2.716589  -2.155 0.031138 *  \nMinpressure_Updated_2014        -0.069870   0.024251  -2.881 0.003964 ** \nscale(NDAM)                     -0.494094   0.455968  -1.084 0.278536    \nMasFem:Minpressure_Updated_2014  0.006108   0.002813   2.171 0.029901 *  \nMasFem:scale(NDAM)               0.205418   0.061956   3.316 0.000915 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that in the code that I gave you not all predictors were scaled (and they don’t say if they scaled in the paper), but as we for looking for main effects in the presence of interactions, we should definitely scale to improve the interpretability\n\nm2 = glmmTMB(alldeaths ~ scale(MasFem) *\n                             (scale(Minpressure_Updated_2014) + scale(NDAM)),\n                           data = hurricanes, family = nbinom2)\nsummary(m2)\n\n Family: nbinom2  ( log )\nFormula:          \nalldeaths ~ scale(MasFem) * (scale(Minpressure_Updated_2014) +  \n    scale(NDAM))\nData: hurricanes\n\n     AIC      BIC   logLik deviance df.resid \n   660.7    678.4   -323.4    646.7       85 \n\n\nDispersion parameter for nbinom2 family (): 0.787 \n\nConditional model:\n                                              Estimate Std. Error z value\n(Intercept)                                     2.5034     0.1231  20.341\nscale(MasFem)                                   0.1237     0.1210   1.022\nscale(Minpressure_Updated_2014)                -0.5425     0.1603  -3.384\nscale(NDAM)                                     0.8988     0.2190   4.105\nscale(MasFem):scale(Minpressure_Updated_2014)   0.3758     0.1731   2.171\nscale(MasFem):scale(NDAM)                       0.6629     0.1999   3.316\n                                              Pr(&gt;|z|)    \n(Intercept)                                    &lt; 2e-16 ***\nscale(MasFem)                                 0.306923    \nscale(Minpressure_Updated_2014)               0.000715 ***\nscale(NDAM)                                   4.05e-05 ***\nscale(MasFem):scale(Minpressure_Updated_2014) 0.029901 *  \nscale(MasFem):scale(NDAM)                     0.000915 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nnow main effect is n.s.; it’s a bit dodgy, but if you read in the main paper, they do not claim a significant main effect, they mainly argue via ANOVA and significance at high values of NDAM, so let’s run an ANOVA:\n\ncar::Anova(m2)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: alldeaths\n                                                Chisq Df Pr(&gt;Chisq)    \nscale(MasFem)                                  1.9495  1  0.1626364    \nscale(Minpressure_Updated_2014)                7.1285  1  0.0075868 ** \nscale(NDAM)                                   14.6100  1  0.0001322 ***\nscale(MasFem):scale(Minpressure_Updated_2014)  4.7150  1  0.0299011 *  \nscale(MasFem):scale(NDAM)                     10.9929  1  0.0009146 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the ANOVA we see that MasFem still n.s. but interactions, and if you would calculate effect of MasFem at high NDAM, it is significant. Something like that is argued in the paper. We can emulate this by changing NDAM centering to high NDAM, which gives us a p-value for the main effect of MasFem at high values of NDAM\n\nhurricanes$highcenteredNDAM = hurricanes$NDAM - max(hurricanes$NDAM)\n\nm3 = glmmTMB(alldeaths ~ scale(MasFem) *\n                             (scale(Minpressure_Updated_2014) + highcenteredNDAM),\n                           data = hurricanes, family = nbinom2)\nsummary(m3)\n\n Family: nbinom2  ( log )\nFormula:          \nalldeaths ~ scale(MasFem) * (scale(Minpressure_Updated_2014) +  \n    highcenteredNDAM)\nData: hurricanes\n\n     AIC      BIC   logLik deviance df.resid \n   660.7    678.4   -323.4    646.7       85 \n\n\nDispersion parameter for nbinom2 family (): 0.787 \n\nConditional model:\n                                                Estimate Std. Error z value\n(Intercept)                                    7.210e+00  1.149e+00   6.275\nscale(MasFem)                                  3.595e+00  1.041e+00   3.455\nscale(Minpressure_Updated_2014)               -5.425e-01  1.603e-01  -3.384\nhighcenteredNDAM                               6.949e-05  1.693e-05   4.105\nscale(MasFem):scale(Minpressure_Updated_2014)  3.758e-01  1.731e-01   2.171\nscale(MasFem):highcenteredNDAM                 5.125e-05  1.546e-05   3.316\n                                              Pr(&gt;|z|)    \n(Intercept)                                   3.50e-10 ***\nscale(MasFem)                                 0.000551 ***\nscale(Minpressure_Updated_2014)               0.000715 ***\nhighcenteredNDAM                              4.05e-05 ***\nscale(MasFem):scale(Minpressure_Updated_2014) 0.029904 *  \nscale(MasFem):highcenteredNDAM                0.000915 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow we see the significant main effect that they report. Note, hwoever, that the signficant differences is only there for high NDAM, i.e. what we do here is to project the effect of the interaction on the main effect. An alternative to do the same thing would be an effects plot, or to specifically use predict() to calculate differences and CIs at high NDAM values.\n\nlibrary(effects)\n\nLoading required package: carData\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nplot(allEffects(m3, partial.residuals = T))\n\nWarning in Effect.glmmTMB(predictors, mod, vcov. = vcov., ...): overriding\nvariance function for effects: computed variances may be incorrect\n\n\nWarning in Analyze.model(focal.predictors, mod, xlevels, default.levels, :\nthe predictors scale(MasFem), scale(Minpressure_Updated_2014) are one-column\nmatrices that were converted to vectors\n\n\nWarning in Effect.glmmTMB(predictors, mod, vcov. = vcov., ...): overriding\nvariance function for effects: computed variances may be incorrect\n\n\nWarning in Analyze.model(focal.predictors, mod, xlevels, default.levels, :\nthe predictors scale(MasFem), scale(Minpressure_Updated_2014) are one-column\nmatrices that were converted to vectors\n\n\n\n\n\nOK, this means we can replicate the results of the paper, even if concentrating the entire analysis exclusive on high NDAM seems a bit cherry-picking. Another way to phrase the result is that we don’t find a main effect of MasFem. However, to be fair: the current results to say that there is a significant difference at high NDAM, and such a difference, if it existed, would be importat.\nBut we haven’t done residual checks yet. Let’s do that:\n\nres &lt;- simulateResiduals(originalModelGAM)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nRegistered S3 method overwritten by 'mgcViz':\n  method from  \n  +.gg   GGally\n\nplot(res)\n\n\n\nplotResiduals(res, hurricanes$NDAM)\n\n\n\nplotResiduals(res, hurricanes$MasFem)\n\n\n\nplotResiduals(res, hurricanes$Minpressure_Updated_2014)\n\n\n\n\nNo significant deviation in the general DHARMa plot, but residuals ~ NDAM looks funny, which was also pointed out by Bob O’Hara in a blog post after publication of the paper. Let’s try to correct this - scaling with ^0.2 does a great job:\n\ncorrectedModel = glmmTMB(alldeaths ~ scale(MasFem) *\n                             (scale(Minpressure_Updated_2014) + scale(NDAM^0.2)),\n                          data = hurricanes, family = nbinom2)\n\nres &lt;- simulateResiduals(correctedModel, plot = T)\n\n\n\nplotResiduals(res, hurricanes$NDAM)\n\n\n\nsummary(correctedModel)\n\n Family: nbinom2  ( log )\nFormula:          \nalldeaths ~ scale(MasFem) * (scale(Minpressure_Updated_2014) +  \n    scale(NDAM^0.2))\nData: hurricanes\n\n     AIC      BIC   logLik deviance df.resid \n   630.8    648.5   -308.4    616.8       85 \n\n\nDispersion parameter for nbinom2 family (): 1.11 \n\nConditional model:\n                                              Estimate Std. Error z value\n(Intercept)                                    2.26430    0.10912  20.751\nscale(MasFem)                                  0.05156    0.10695   0.482\nscale(Minpressure_Updated_2014)               -0.03162    0.18141  -0.174\nscale(NDAM^0.2)                                1.28961    0.18992   6.790\nscale(MasFem):scale(Minpressure_Updated_2014) -0.02410    0.20343  -0.118\nscale(MasFem):scale(NDAM^0.2)                  0.16045    0.20350   0.788\n                                              Pr(&gt;|z|)    \n(Intercept)                                    &lt; 2e-16 ***\nscale(MasFem)                                    0.630    \nscale(Minpressure_Updated_2014)                  0.862    \nscale(NDAM^0.2)                               1.12e-11 ***\nscale(MasFem):scale(Minpressure_Updated_2014)    0.906    \nscale(MasFem):scale(NDAM^0.2)                    0.430    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(correctedModel)\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: alldeaths\n                                                Chisq Df Pr(&gt;Chisq)    \nscale(MasFem)                                  0.5732  1     0.4490    \nscale(Minpressure_Updated_2014)                0.1255  1     0.7232    \nscale(NDAM^0.2)                               73.5010  1     &lt;2e-16 ***\nscale(MasFem):scale(Minpressure_Updated_2014)  0.0140  1     0.9057    \nscale(MasFem):scale(NDAM^0.2)                  0.6216  1     0.4304    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAll gone, only damage is doing the effect. This wouldn’t change with re-scaling probably, as interactions are n.s.\nWhat if we would have fit our own model? First of all, note that if hurricane names were given randomly, we wouldn’t have to worry about confounders. However, this is not the case, hurricanes were only named randomly after 1978 or so.\n\nplot(MasFem ~ Year, data = hurricanes)\n\n\n\n\nSo, we could either take the earlier data out, which would remove half of our data, or we have to worry about confounding with variables that change over time. The most obvious thing would be to take time itself (Year) in the model, to correct for temporal confounding.\nDo we need other variables that are not confounders? There is two reasons to add them:\n\nthey have strong effects on the response - not adding them could lead to residual problems and increase residual variance, which increases uncertainties and cost power\nwe want to fit interacts.\n\nI added NDAM to the model, because we saw earlier that it has a strong effect. I think it’s not unreasonable to check for an interaction as well.\nAs we have several observations per year, a conservative approach would be to add a RE on year. Note that we use year both as a fixed effect (to remove temporal trends) and a random intercept, which is perfectly fine, however.\n\nnewModel = glmmTMB(alldeaths ~ scale(MasFem) * scale(NDAM^0.2) + Year + (1|Year),\n                           data = hurricanes, family = nbinom2)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\nsummary(newModel)\n\n Family: nbinom2  ( log )\nFormula:          \nalldeaths ~ scale(MasFem) * scale(NDAM^0.2) + Year + (1 | Year)\nData: hurricanes\n\n     AIC      BIC   logLik deviance df.resid \n   630.8    648.4   -308.4    616.8       85 \n\nRandom effects:\n\nConditional model:\n Groups Name        Variance  Std.Dev. \n Year   (Intercept) 2.571e-07 0.0005071\nNumber of obs: 92, groups:  Year, 49\n\nDispersion parameter for nbinom2 family (): 1.11 \n\nConditional model:\n                               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                   -2.542287  12.730846  -0.200    0.842    \nscale(MasFem)                  0.073207   0.119273   0.614    0.539    \nscale(NDAM^0.2)                1.309624   0.118106  11.089   &lt;2e-16 ***\nYear                           0.002426   0.006423   0.378    0.706    \nscale(MasFem):scale(NDAM^0.2)  0.179874   0.117191   1.535    0.125    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncar::Anova(newModel) # nothing regarding MasFem\n\nAnalysis of Deviance Table (Type II Wald chisquare tests)\n\nResponse: alldeaths\n                                 Chisq Df Pr(&gt;Chisq)    \nscale(MasFem)                   0.5563  1     0.4558    \nscale(NDAM^0.2)               121.9237  1     &lt;2e-16 ***\nYear                            0.1426  1     0.7057    \nscale(MasFem):scale(NDAM^0.2)   2.3559  1     0.1248    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results remain that there is no effect of MasFem!"
  },
  {
    "objectID": "6C-CaseStudies.html#researchers-degrees-of-freedom-skin-color-and-red-cards",
    "href": "6C-CaseStudies.html#researchers-degrees-of-freedom-skin-color-and-red-cards",
    "title": "Appendix C — Case studies",
    "section": "C.2 Researchers Degrees of Freedom — Skin Color and Red Cards",
    "text": "C.2 Researchers Degrees of Freedom — Skin Color and Red Cards\nIn 2018 Silberzahn et al. published a “meta analysis” in Advances in Methods and Practices in Psychological Science, where they had provided 29 teams with the same data set to answer one research question: “[W]hether soccer players with dark skin tone are more likely than those with light skin tone to receive red cards from referees”.\nSpoiler: They found that the “[a]nalytic approaches varied widely across the teams, and the estimated effect sizes ranged from 0.89 to 2.93 (Mdn = 1.31) in odds-ratio units”, highlighting that different approaches in data analysis can yield significant variation in the results.\nYou can find the paper “Many Analysts, One Data Set: Making Transparent How Variations in Analytic Choices Affect Results” at: https://journals.sagepub.com/doi/10.1177/2515245917747646.\nThe data is in\n\nlibrary(EcoData)\n?redCards\n\nTask: Do a re-analysis of the data as if you were the 30th team to contribute the results to the meta analysis. You can find the data in the ecodata package, dataset redCards.\n\nResponse variable: ‘redCards’ (+‘yellowReds’?).\nprimary predictors: ‘rater1’, ‘rater2’\nMultiple variables, potentially accounting for confounding, offsetting, grouping, … are included in the data.\n\nThe rater variable contains ratings of “two independent raters blind to the research question who, based on their profile photo, categorized players on a 5-point scale ranging from (1) very light skin to (5) very dark skin. Make sure that ‘rater1’ and ‘rater2’ are rescaled to the range 0 … 1 as described in the paper (”This variable was rescaled to be bounded by 0 (very light skin) and 1 (very dark skin) prior to the final analysis, to ensure consistency of effect sizes across the teams of analysts. The raw ratings were rescaled to 0, .25, .50, .75, and 1 to create this new scale.”)\nWhen you’re done, have a look at the other modelling teams. Do you understand the models they fit? Note that the results are displayed in terms of odd ratios. Are your results within the range of estimates from the 29 teams in Silberzahn et al. (2018)?\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "6C-CaseStudies.html#scouting-ants",
    "href": "6C-CaseStudies.html#scouting-ants",
    "title": "Appendix C — Case studies",
    "section": "C.3 Scouting Ants",
    "text": "C.3 Scouting Ants\nLook at the dataset EcoData::scoutingAnts, and find out if there are really scouting Ants in Lasius Niger.\nA base model should be:\n\nlibrary(EcoData)\ndat = scoutingAnts[scoutingAnts$first.visit == 0,]\ndat$ant_group = as.factor(dat$ant_group)\ndat$ant_group_main = as.factor(dat$ant_group_main)\n\nfit &lt;- glm(went.phero ~ ant_group_main, data = dat)\nsummary(fit)\n\n\nCall:\nglm(formula = went.phero ~ ant_group_main, data = dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.6986  -0.6624   0.3014   0.3014   0.3376  \n\nCoefficients:\n                                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                             0.66239    0.02139  30.974   &lt;2e-16 ***\nant_group_mainSecondvisit_1st_to_phero  0.03626    0.02469   1.469    0.142    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.2140339)\n\n    Null deviance: 401.35  on 1874  degrees of freedom\nResidual deviance: 400.89  on 1873  degrees of freedom\nAIC: 2434.5\n\nNumber of Fisher Scoring iterations: 2\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor me, it made sense to change the contrasts of the possible confounders into something more easily interpretable:\n\nDid the side in which the pheromone was stay constant or not (testing for a directional persistence of the ants)\nWas the pheromone in the left or the right arm (testing for a directional preference)\n\n\ndat$directionConst = ifelse(dat$Treatment %in% c(\"LL\", \"RR\"), T, F)\ndat$directionPhero = as.factor(ifelse(dat$Treatment %in% c(\"LL\", \"RL\"), \"left\", \"right\"))\n\nTogether with the orientation of the Maze, this makes 3 possible directional confounders, and the main predictor (if the Ant went to the pheromone in the first visit).\nAdding an RE on colony is logical, and then let’s run the model:\n\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'lme4'\n\n\nThe following object is masked from 'package:nlme':\n\n    lmList\n\nfit1&lt;-glmer(went.phero ~ ant_group_main\n             + directionConst\n             + directionPhero\n             + Orientation\n             + (1|Colony),family=\"binomial\", \n             data=dat)\nsummary(fit1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: went.phero ~ ant_group_main + directionConst + directionPhero +  \n    Orientation + (1 | Colony)\n   Data: dat\n\n     AIC      BIC   logLik deviance df.resid \n  2192.5   2225.7  -1090.2   2180.5     1869 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.6870 -1.0451  0.4836  0.6876  1.0318 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n Colony (Intercept) 0.9284   0.9636  \nNumber of obs: 1875, groups:  Colony, 15\n\nFixed effects:\n                                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                              1.9505     0.3330   5.857 4.71e-09 ***\nant_group_mainSecondvisit_1st_to_phero   0.1311     0.1248   1.050  0.29351    \ndirectionConstTRUE                      -1.1465     0.2409  -4.760 1.94e-06 ***\ndirectionPheroright                     -0.5680     0.1984  -2.863  0.00420 ** \nOrientationright                        -0.3493     0.1222  -2.859  0.00425 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) a__S_1 dCTRUE drctnP\nant_g_S_1__ -0.252                     \ndrctnCnTRUE -0.413 -0.062              \ndrctnPhrrgh -0.441 -0.001  0.306       \nOrinttnrght -0.182 -0.093 -0.074  0.085\n\n\nSurprisingly, we find large effects of the other variables. Because of these large effects, testing for interactions with the experimental treatment as well\n\nfit2&lt;-glmer(went.phero ~ ant_group_main * (\n             + directionConst\n             + directionPhero\n             + Orientation)\n             + (1|Colony),family=\"binomial\", \n             data=dat)\nsummary(fit2)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: went.phero ~ ant_group_main * (+directionConst + directionPhero +  \n    Orientation) + (1 | Colony)\n   Data: dat\n\n     AIC      BIC   logLik deviance df.resid \n  2131.2   2181.0  -1056.6   2113.2     1866 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-6.7899 -0.8936  0.4857  0.5984  1.9020 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n Colony (Intercept) 0.9497   0.9745  \nNumber of obs: 1875, groups:  Colony, 15\n\nFixed effects:\n                                                           Estimate Std. Error\n(Intercept)                                                  2.7384     0.3914\nant_group_mainSecondvisit_1st_to_phero                      -0.7658     0.2763\ndirectionConstTRUE                                          -1.9420     0.3065\ndirectionPheroright                                         -1.8133     0.3008\nOrientationright                                             0.2322     0.2345\nant_group_mainSecondvisit_1st_to_phero:directionConstTRUE    1.0467     0.2638\nant_group_mainSecondvisit_1st_to_phero:directionPheroright   1.4752     0.2664\nant_group_mainSecondvisit_1st_to_phero:Orientationright     -0.8261     0.2668\n                                                           z value Pr(&gt;|z|)    \n(Intercept)                                                  6.997 2.61e-12 ***\nant_group_mainSecondvisit_1st_to_phero                      -2.771  0.00558 ** \ndirectionConstTRUE                                          -6.337 2.34e-10 ***\ndirectionPheroright                                         -6.029 1.65e-09 ***\nOrientationright                                             0.990  0.32199    \nant_group_mainSecondvisit_1st_to_phero:directionConstTRUE    3.967 7.28e-05 ***\nant_group_mainSecondvisit_1st_to_phero:directionPheroright   5.537 3.08e-08 ***\nant_group_mainSecondvisit_1st_to_phero:Orientationright     -3.097  0.00196 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) an__S_1__ dCTRUE drctnP Ornttn a__S_1__:C a__S_1__:P\nant_g_S_1__ -0.532                                                     \ndrctnCnTRUE -0.433  0.241                                              \ndrctnPhrrgh -0.512  0.369     0.186                                    \nOrinttnrght -0.282  0.428    -0.100  0.008                             \na__S_1__:CT  0.217 -0.443    -0.592  0.043  0.015                      \nan__S_1__:P  0.343 -0.529     0.007 -0.733 -0.049 -0.072               \nan__S_1__:O  0.225 -0.526     0.089  0.034 -0.845 -0.021      0.046    \n\n\nHere we find now that ther is an interaction with the main predictor, and there could be effects. We can also look at this visually.\n\nplot(allEffects(fit2))\n\n\n\n\nThe results are difficult to interpret. I would think that there was some bias in the experiment, which led to an effect of the Maze direction, which then create a spill-over to the other (and in particular the main) predictors.\nFor our education, we can also look at the residual plots. I will use m1, because there was a misfit:\n\nres &lt;- simulateResiduals(m1, plot = T)\n\n\n\n\nAs we would significant interactions, we would probably see something if we plot residuals against predictors or their interactions, but I want to show you something else:\nWe will not see dispersion problems in a 0/1 binomial, but actually, this is a k/n binomial, just that the data are not prepared as such.\nEither way, in DHARMa, you can aggregate residuals by a grouping variable.\n\nres2 &lt;- recalculateResiduals(res, group = dat$Colony)\n\nNow, we essentially check k/n data, and we see that there is overdispersion, which is caused by the misfit.\n\nplot(res2)\n\nWarning in smooth.construct.tp.smooth.spec(object, dk$data, dk$knots): basis dimension, k, increased to minimum possible\n\n\nUnable to calculate quantile regression for quantile 0.25. Possibly to few (unique) data points / predictions. Will be ommited in plots and significance calculations.\n\n\nWarning in smooth.construct.tp.smooth.spec(object, dk$data, dk$knots): basis dimension, k, increased to minimum possible\n\n\nUnable to calculate quantile regression for quantile 0.5. Possibly to few (unique) data points / predictions. Will be ommited in plots and significance calculations.\n\n\nWarning in smooth.construct.tp.smooth.spec(object, dk$data, dk$knots): basis dimension, k, increased to minimum possible\n\n\nUnable to calculate quantile regression for quantile 0.75. Possibly to few (unique) data points / predictions. Will be ommited in plots and significance calculations.\n\n\n\n\ntestDispersion(res2)\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.47405, p-value = 0.624\nalternative hypothesis: two.sided\n\n\nLet’s do the same for model 2, which included the interactions.\n\nres &lt;- simulateResiduals(m2, plot = T)\n\n\n\nres2 &lt;- recalculateResiduals(res, group = dat$Colony)\n\nplot(res2)\n\nWarning in smooth.construct.tp.smooth.spec(object, dk$data, dk$knots): basis dimension, k, increased to minimum possible\n\n\nUnable to calculate quantile regression for quantile 0.25. Possibly to few (unique) data points / predictions. Will be ommited in plots and significance calculations.\n\n\nWarning in smooth.construct.tp.smooth.spec(object, dk$data, dk$knots): basis dimension, k, increased to minimum possible\n\n\nUnable to calculate quantile regression for quantile 0.5. Possibly to few (unique) data points / predictions. Will be ommited in plots and significance calculations.\n\n\nWarning in smooth.construct.tp.smooth.spec(object, dk$data, dk$knots): basis dimension, k, increased to minimum possible\n\n\nUnable to calculate quantile regression for quantile 0.75. Possibly to few (unique) data points / predictions. Will be ommited in plots and significance calculations.\n\n\n\n\ntestDispersion(res2)\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.47405, p-value = 0.624\nalternative hypothesis: two.sided\n\n\nWhich largely removes the problem!"
  },
  {
    "objectID": "6C-CaseStudies.html#owls",
    "href": "6C-CaseStudies.html#owls",
    "title": "Appendix C — Case studies",
    "section": "C.4 Owls",
    "text": "C.4 Owls\nLook at the Owl data set in the glmmTMB package. The initial hypothesis is\n\nlibrary(glmmTMB)\n\nm1 = glm(SiblingNegotiation ~ FoodTreatment*SexParent + offset(log(BroodSize)),\n         data = Owls , family = poisson)\nres = simulateResiduals(m1)\nplot(res)\n\n\n\n\n\n\n\n\n\n\nOffset\n\n\n\nThe offset is a special command that can be used in all regression models. It means that we include an effect with effect size 1.\nThe offset has a special importance in models with a log link function, because with these models, we have y = exp(x …), so if you do y = exp(x + log(BroodSize) ) and use exp rules, this is y = exp(x) * exp(log(BroodSize)) = y = exp(x) * BroodSize, so this makes the response proportional to BroodSize. This trick is often used in log link GLMs to make the response proportional to Area, Sampling effort, etc.\n\n\nTask: try to improve the model with everything we have discussed so far.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nm1 = glmmTMB::glmmTMB(SiblingNegotiation ~ FoodTreatment * SexParent\n  + (1|Nest) + offset(log(BroodSize)), data = Owls , family = nbinom1,\n  dispformula = ~ FoodTreatment + SexParent,\n  ziformula = ~ FoodTreatment + SexParent)\nsummary(m1)\n\n Family: nbinom1  ( log )\nFormula:          \nSiblingNegotiation ~ FoodTreatment * SexParent + (1 | Nest) +  \n    offset(log(BroodSize))\nZero inflation:                      ~FoodTreatment + SexParent\nDispersion:                          ~FoodTreatment + SexParent\nData: Owls\n\n     AIC      BIC   logLik deviance df.resid \n  3354.6   3402.9  -1666.3   3332.6      588 \n\nRandom effects:\n\nConditional model:\n Groups Name        Variance Std.Dev.\n Nest   (Intercept) 0.0876   0.296   \nNumber of obs: 599, groups:  Nest, 27\n\nConditional model:\n                                    Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                          0.80028    0.09736   8.220  &lt; 2e-16 ***\nFoodTreatmentSatiated               -0.46893    0.16760  -2.798  0.00514 ** \nSexParentMale                       -0.09127    0.09247  -0.987  0.32363    \nFoodTreatmentSatiated:SexParentMale  0.13087    0.19028   0.688  0.49158    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nZero-inflation model:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)            -1.9132     0.3269  -5.853 4.84e-09 ***\nFoodTreatmentSatiated   1.0564     0.4072   2.594  0.00948 ** \nSexParentMale          -0.4688     0.3659  -1.281  0.20012    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDispersion model:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             1.2122     0.2214   5.475 4.37e-08 ***\nFoodTreatmentSatiated   0.7978     0.2732   2.920   0.0035 ** \nSexParentMale          -0.1540     0.2399  -0.642   0.5209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nres = simulateResiduals(m1, plot = T)\n\n\n\ntestDispersion(m1)\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.78311, p-value = 0.104\nalternative hypothesis: two.sided\n\ntestZeroInflation(m1)\n\n\n\n\n\n    DHARMa zero-inflation test via comparison to expected zeros with\n    simulation under H0 = fitted model\n\ndata:  simulationOutput\nratioObsSim = 1.0465, p-value = 0.608\nalternative hypothesis: two.sided\n\n\n\n\n\n\n\n\n\n\n\nSolution using brms\n\n\n\n\n\nThis is not adding dispersion and zero-inflation yet, just to show how such a model could be fit with brms\n\nlibrary(brms)\nm2 = brms::brm(SiblingNegotiation ~ FoodTreatment * SexParent\n  + (1|Nest) + offset(log(BroodSize)), \n  data = Owls , \n  family = negbinomial)\n\n\nSAMPLING FOR MODEL 'd0815fc5dccdb228591681bc60d04517' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000199 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.99 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.84772 seconds (Warm-up)\nChain 1:                1.45125 seconds (Sampling)\nChain 1:                3.29897 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'd0815fc5dccdb228591681bc60d04517' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000117 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.17 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.70341 seconds (Warm-up)\nChain 2:                1.44988 seconds (Sampling)\nChain 2:                3.15329 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'd0815fc5dccdb228591681bc60d04517' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 9.6e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.96 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.86753 seconds (Warm-up)\nChain 3:                1.39721 seconds (Sampling)\nChain 3:                3.26475 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'd0815fc5dccdb228591681bc60d04517' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.000112 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.12 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.82473 seconds (Warm-up)\nChain 4:                1.43496 seconds (Sampling)\nChain 4:                3.25969 seconds (Total)\nChain 4: \n\nsummary(m2)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: SiblingNegotiation ~ FoodTreatment * SexParent + (1 | Nest) + offset(log(BroodSize)) \n   Data: Owls (Number of observations: 599) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~Nest (Number of levels: 27) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.40      0.10     0.22     0.63 1.01     1153     1551\n\nPopulation-Level Effects: \n                                    Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept                               0.72      0.14     0.44     0.99 1.00\nFoodTreatmentSatiated                  -0.78      0.17    -1.11    -0.46 1.00\nSexParentMale                          -0.03      0.15    -0.31     0.25 1.00\nFoodTreatmentSatiated:SexParentMale     0.16      0.21    -0.24     0.54 1.00\n                                    Bulk_ESS Tail_ESS\nIntercept                               2658     3238\nFoodTreatmentSatiated                   3274     3273\nSexParentMale                           3600     3265\nFoodTreatmentSatiated:SexParentMale     3684     3379\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     0.84      0.07     0.71     0.98 1.00     5749     2912\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(m2, ask = FALSE)"
  },
  {
    "objectID": "6C-CaseStudies.html#snails",
    "href": "6C-CaseStudies.html#snails",
    "title": "Appendix C — Case studies",
    "section": "C.5 Snails",
    "text": "C.5 Snails\n\nlibrary(EcoData)\nlibrary(glmmTMB)\nlibrary(lme4)\nlibrary(DHARMa)\nlibrary(tidyverse)\n?EcoData::snails\n\nLook at the Snails data set in the EcoData package, and find out which environmental and/or seasonal predictors i) explain the total abundance and ii) the infection rate of the three species.\nThe snails data set in the EcoData package includes observations on the distribution of freshwater snails and their infection rates ( schistosomiasis (a parasit)).\nThe first scientific question is that their adbundance depends on the water conditions. The second scientific question is that their infection rate depends on the water conditions and seasonsal factors\nThe data also contains data on other environmental (and seasonal factors). You should consider if it is useful to add them to the analysis.\nSpecies: BP_tot, BF_tot, BT_tot\nNumber of infected individuals: BP_pos_tot, BF_pos_tot, BT_pos_tot\nTotal abundances of BP species: Bulinus_tot\nTotal number of infected in BP species: Bulinus_pos_tot\nTasks:\n\nModel the summed total abundance of the three species (Bulinus_tot)\nModel the infection rate of all three species (Bulinuts_pos_tot) (k/n binomial)\nOptional: Model the species individually (BP_tot, BF_tot, BT_tot)\nOptional: Fit a multivariate (joint) species distribution model\n\n\n\n\n\n\n\nSolution for total Abundance of Bulinus (glmm)\n\n\n\n\n\nPrepare+scale data:\n\nlibrary(lme4)\nlibrary(glmmTMB)\nlibrary(DHARMa)\ndata = EcoData::snails\ndata$sTemp_Water = scale(data$Temp_Water)\ndata$spH = scale(data$pH)\ndata$swater_speed_ms = scale(data$water_speed_ms)\ndata$swater_depth = scale(data$water_depth)\ndata$sCond = scale(data$Cond)\ndata$swmo_prec = scale(data$wmo_prec)\ndata$syear = scale(data$year)\ndata$sLat = scale(data$Latitude)\ndata$sLon = scale(data$Longitude)\ndata$sTemp_Air = scale(data$Temp_Air)\n\n# Let's remove NAs beforehand:\nrows = rownames(model.matrix(Bulinus_tot~sTemp_Water + spH + sLat + sLon + sCond + seas_wmo+ swmo_prec + swater_speed_ms + swater_depth +sTemp_Air+ syear + duration + locality + site_irn + coll_date, data = data))\ndata = data[rows, ]\n\nOur hypothesis is that the abundance of Bulinus species depends on the water characteristics, e.g. site_type, Temp_water, pH, Cond, swmo_prec, water_speed_ms, and water_depth. We will set the length of the collection duration as an offset.\n\nmodel1 = glm(Bulinus_tot~\n                 offset(log(duration)) + site_type + sTemp_Water + spH +\n                 sCond + swmo_prec + swater_speed_ms + swater_depth,\n              data = data,  family = poisson)\nsummary(model1)\n\n\nCall:\nglm(formula = Bulinus_tot ~ offset(log(duration)) + site_type + \n    sTemp_Water + spH + sCond + swmo_prec + swater_speed_ms + \n    swater_depth, family = poisson, data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-11.181   -5.895   -3.382    0.729   46.751  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        0.328925   0.006417  51.258  &lt; 2e-16 ***\nsite_typecanal.3  -0.161047   0.010287 -15.656  &lt; 2e-16 ***\nsite_typepond     -0.837273   0.022624 -37.009  &lt; 2e-16 ***\nsite_typerice.p   -1.378799   0.027252 -50.595  &lt; 2e-16 ***\nsite_typeriver    -1.730850   0.032842 -52.703  &lt; 2e-16 ***\nsite_typerivulet  -1.757255   0.041545 -42.298  &lt; 2e-16 ***\nsite_typespillway -1.679141   0.048544 -34.590  &lt; 2e-16 ***\nsTemp_Water       -0.089050   0.004435 -20.080  &lt; 2e-16 ***\nspH                0.036653   0.004501   8.144 3.82e-16 ***\nsCond              0.072787   0.004979  14.620  &lt; 2e-16 ***\nswmo_prec         -0.098717   0.006337 -15.577  &lt; 2e-16 ***\nswater_speed_ms   -0.181606   0.007695 -23.600  &lt; 2e-16 ***\nswater_depth      -0.113600   0.005998 -18.940  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 131410  on 2071  degrees of freedom\nResidual deviance: 116126  on 2059  degrees of freedom\nAIC: 122311\n\nNumber of Fisher Scoring iterations: 6\n\n\nAs the sites are nested within localities, we will set a nested random intercept on site_irn within locality. Also potential confounders are collection date (coll_date), the season (wet or dry months, seas_wmo), year, and maybe other environmental factors such as the air temperature?.\n\nmodel2 = glmer(Bulinus_tot~\n                 offset(log(duration)) + site_type + sTemp_Water + spH +\n                 sCond + swmo_prec + swater_speed_ms + swater_depth +\n                 sTemp_Air + seas_wmo + (1|year) + (1|locality/site_irn) +  (swater_depth|coll_date),\n              data = data,  family = poisson)\nsummary(model2)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: poisson  ( log )\nFormula: Bulinus_tot ~ offset(log(duration)) + site_type + sTemp_Water +  \n    spH + sCond + swmo_prec + swater_speed_ms + swater_depth +  \n    sTemp_Air + seas_wmo + (1 | year) + (1 | locality/site_irn) +  \n    (swater_depth | coll_date)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n 48873.9  48992.3 -24415.9  48831.9     2051 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-11.997  -2.448  -0.963   1.285  41.809 \n\nRandom effects:\n Groups            Name         Variance Std.Dev. Corr\n coll_date         (Intercept)  1.8752   1.3694       \n                   swater_depth 1.2494   1.1178   0.29\n site_irn:locality (Intercept)  0.7207   0.8489       \n locality          (Intercept)  0.8319   0.9121       \n year              (Intercept)  0.1613   0.4016       \nNumber of obs: 2072, groups:  \ncoll_date, 191; site_irn:locality, 89; locality, 20; year, 5\n\nFixed effects:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -0.971234   0.362760  -2.677 0.007421 ** \nsite_typecanal.3  -0.078434   0.238250  -0.329 0.741997    \nsite_typepond      0.291206   0.412076   0.707 0.479766    \nsite_typerice.p   -0.774651   0.343155  -2.257 0.023981 *  \nsite_typeriver    -0.531948   0.423089  -1.257 0.208646    \nsite_typerivulet  -0.449273   0.555608  -0.809 0.418737    \nsite_typespillway -0.422274   0.558556  -0.756 0.449644    \nsTemp_Water       -0.055653   0.018103  -3.074 0.002110 ** \nspH               -0.029208   0.008983  -3.251 0.001148 ** \nsCond             -0.015815   0.009465  -1.671 0.094733 .  \nswmo_prec         -0.212255   0.099804  -2.127 0.033444 *  \nswater_speed_ms   -0.128273   0.009414 -13.626  &lt; 2e-16 ***\nswater_depth      -0.290808   0.084218  -3.453 0.000554 ***\nsTemp_Air         -0.102229   0.013609  -7.512 5.83e-14 ***\nseas_wmowet       -0.057509   0.205743  -0.280 0.779848    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nCorrelation matrix not shown by default, as p = 15 &gt; 12.\nUse print(x, correlation=TRUE)  or\n    vcov(x)        if you need it\n\n\nCheck residuals:\n\nres = simulateResiduals(model2, plot = TRUE, re.form = NULL)\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\n\nDoes not look great -&gt; dispersion problems -&gt; switch to -&gt; negative binomial distribution:\n\nmodel3 = glmmTMB(Bulinus_tot~\n                 offset(log(duration)) + site_type + sTemp_Water + spH +\n                 sCond + swmo_prec + swater_speed_ms + swater_depth +\n                 sTemp_Air + seas_wmo + (1|year) + (1|locality/site_irn) +  (swater_depth|coll_date),\n              data = data,  family = nbinom2)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\nsummary(model3)\n\n Family: nbinom2  ( log )\nFormula:          \nBulinus_tot ~ offset(log(duration)) + site_type + sTemp_Water +  \n    spH + sCond + swmo_prec + swater_speed_ms + swater_depth +  \n    sTemp_Air + seas_wmo + (1 | year) + (1 | locality/site_irn) +  \n    (swater_depth | coll_date)\nData: data\n\n     AIC      BIC   logLik deviance df.resid \n 14137.0  14261.0  -7046.5  14093.0     2050 \n\nRandom effects:\n\nConditional model:\n Groups            Name         Variance Std.Dev. Corr \n year              (Intercept)  0.32980  0.5743        \n site_irn:locality (Intercept)  0.59947  0.7743        \n locality          (Intercept)  0.54314  0.7370        \n coll_date         (Intercept)  0.66067  0.8128        \n                   swater_depth 0.02441  0.1563   0.16 \nNumber of obs: 2072, groups:  \nyear, 5; site_irn:locality, 89; locality, 20; coll_date, 191\n\nDispersion parameter for nbinom2 family (): 0.433 \n\nConditional model:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -0.48794    0.37886  -1.288 0.197773    \nsite_typecanal.3  -0.14448    0.24459  -0.591 0.554728    \nsite_typepond      0.09447    0.43016   0.220 0.826172    \nsite_typerice.p   -0.92636    0.36321  -2.550 0.010757 *  \nsite_typeriver    -0.82008    0.42509  -1.929 0.053709 .  \nsite_typerivulet  -0.98675    0.54309  -1.817 0.069231 .  \nsite_typespillway -0.41259    0.56965  -0.724 0.468888    \nsTemp_Water       -0.10038    0.09361  -1.072 0.283532    \nspH               -0.07251    0.05265  -1.377 0.168487    \nsCond              0.12188    0.05953   2.047 0.040613 *  \nswmo_prec         -0.10279    0.07948  -1.293 0.195908    \nswater_speed_ms   -0.12390    0.03418  -3.625 0.000289 ***\nswater_depth      -0.18908    0.05497  -3.440 0.000583 ***\nsTemp_Air         -0.01520    0.08007  -0.190 0.849444    \nseas_wmowet       -0.03143    0.17994  -0.175 0.861355    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCheck residuals:\n\nres = simulateResiduals(model3, plot = TRUE)\n\n\n\n\nResiduals look better but there is still a dispersion problem.\nLet’s use the dispformula:\n\nmodel4 = glmmTMB(Bulinus_tot~\n                 offset(log(duration)) + site_type + sTemp_Water + spH +\n                 sCond + swmo_prec + swater_speed_ms + swater_depth +\n                 sTemp_Air + seas_wmo + (1|year) + (1|locality/site_irn) +  (swater_depth|coll_date),\n                 dispformula = ~swater_speed_ms + swater_depth+sCond,\n              data = data,  family = nbinom2)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\nsummary(model4)\n\n Family: nbinom2  ( log )\nFormula:          \nBulinus_tot ~ offset(log(duration)) + site_type + sTemp_Water +  \n    spH + sCond + swmo_prec + swater_speed_ms + swater_depth +  \n    sTemp_Air + seas_wmo + (1 | year) + (1 | locality/site_irn) +  \n    (swater_depth | coll_date)\nDispersion:                   ~swater_speed_ms + swater_depth + sCond\nData: data\n\n     AIC      BIC   logLik deviance df.resid \n 14138.2  14279.1  -7044.1  14088.2     2047 \n\nRandom effects:\n\nConditional model:\n Groups            Name         Variance Std.Dev. Corr \n year              (Intercept)  0.32157  0.5671        \n site_irn:locality (Intercept)  0.60194  0.7758        \n locality          (Intercept)  0.53655  0.7325        \n coll_date         (Intercept)  0.65362  0.8085        \n                   swater_depth 0.01958  0.1399   0.14 \nNumber of obs: 2072, groups:  \nyear, 5; site_irn:locality, 89; locality, 20; coll_date, 191\n\nConditional model:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -0.46934    0.37644  -1.247 0.212485    \nsite_typecanal.3  -0.15293    0.24497  -0.624 0.532444    \nsite_typepond      0.05419    0.43145   0.126 0.900045    \nsite_typerice.p   -0.95968    0.36331  -2.641 0.008254 ** \nsite_typeriver    -0.84089    0.42561  -1.976 0.048184 *  \nsite_typerivulet  -1.01483    0.54260  -1.870 0.061439 .  \nsite_typespillway -0.45053    0.57035  -0.790 0.429575    \nsTemp_Water       -0.09780    0.09342  -1.047 0.295146    \nspH               -0.07259    0.05246  -1.384 0.166427    \nsCond              0.12399    0.06344   1.954 0.050647 .  \nswmo_prec         -0.10189    0.07895  -1.290 0.196899    \nswater_speed_ms   -0.16899    0.04665  -3.623 0.000292 ***\nswater_depth      -0.17509    0.05627  -3.112 0.001861 ** \nsTemp_Air         -0.01550    0.08014  -0.193 0.846589    \nseas_wmowet       -0.04867    0.17952  -0.271 0.786290    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDispersion model:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.838268   0.038472 -21.789   &lt;2e-16 ***\nswater_speed_ms -0.095345   0.044883  -2.124   0.0336 *  \nswater_depth    -0.003945   0.042073  -0.094   0.9253    \nsCond           -0.013696   0.037138  -0.369   0.7123    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nDispformula is significant. Residuals:\n\nres = simulateResiduals(model4, plot = TRUE)\n\n\n\n\nDispersion tests are n.s.:\n\ntestDispersion(res)\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.065593, p-value = 0.08\nalternative hypothesis: two.sided\n\ntestZeroInflation(res)\n\n\n\n\n\n    DHARMa zero-inflation test via comparison to expected zeros with\n    simulation under H0 = fitted model\n\ndata:  simulationOutput\nratioObsSim = 1.052, p-value = 0.656\nalternative hypothesis: two.sided\n\n\nWe detrended space there could be spatial autocorrelation, let’s check for it:\n\n## Spatial\nres2 = recalculateResiduals(res, group = c(data$site_irn))\ngroupLocations = aggregate(cbind(data$sLat, data$sLon ), list( data$site_irn), mean)\ntestSpatialAutocorrelation(res2, x = groupLocations$V1, y = groupLocations$V2)\n\n\n\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res2\nobserved = 0.295785, expected = -0.011364, sd = 0.066951, p-value =\n4.482e-06\nalternative hypothesis: Distance-based autocorrelation\n\n\nSignificant! Let’s add a spatial correlation structure:\n\nnumFac = numFactor(data$sLat, data$sLon)\ngroup = factor(rep(1, nrow(data)))\ndata$fmonth = as.factor(data$month)\n\nmodel5 = glmmTMB(Bulinus_tot~\n                 offset(log(duration)) + site_type + sTemp_Water + spH +\n                 sCond + swmo_prec + swater_speed_ms + swater_depth +\n                 sTemp_Air + seas_wmo + (1|year) + (1|locality/site_irn) +  (swater_depth|coll_date) + exp(0+numFac|group),\n                 dispformula = ~swater_speed_ms + swater_depth+sCond,\n              data = data,  family = nbinom2)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\n\n\nres = simulateResiduals(model5, plot = TRUE)\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\n\nglmmTMB does not support conditional simulations but we can create conditional simulations on our own:\n\npred = predict(model5, re.form = NULL, type = \"response\")\npred_dispersion = predict(model5, re.form = NULL, type = \"disp\")\nsimulations = sapply(1:1000, function(i) rnbinom(length(pred),size = pred_dispersion, mu =  pred))\nres = createDHARMa(simulations, model.frame(model5)[,1], pred)\nplot(res)\n\n\n\n\nResiduals do not look perfect but I would say that we can stop here now.\n\n\n\n\n\n\n\n\n\nSolution for prevalence of Bulinus (glmm)\n\n\n\n\n\nPrepare+scale data:\n\nlibrary(lme4)\nlibrary(glmmTMB)\nlibrary(DHARMa)\ndata = EcoData::snails\ndata$sTemp_Water = scale(data$Temp_Water)\ndata$spH = scale(data$pH)\ndata$swater_speed_ms = scale(data$water_speed_ms)\ndata$swater_depth = scale(data$water_depth)\ndata$sCond = scale(data$Cond)\ndata$swmo_prec = scale(data$wmo_prec)\ndata$syear = scale(data$year)\ndata$sLat = scale(data$Latitude)\ndata$sLon = scale(data$Longitude)\ndata$sTemp_Air = scale(data$Temp_Air)\n\n# Let's remove NAs beforehand:\nrows = rownames(model.matrix(Bulinus_tot~sTemp_Water + spH + sLat + sLon + sCond + seas_wmo+ swmo_prec + swater_speed_ms + swater_depth +sTemp_Air+ syear + duration + locality + site_irn + coll_date, data = data))\ndata = data[rows, ]\n\nLet’s start directly with all potential confounders (see previous solution):\n\nmodel1 = glmmTMB(cbind(Bulinus_pos_tot, Bulinus_tot - Bulinus_pos_tot )~\n                 offset(log(duration)) + site_type + sTemp_Water + spH +\n                 sCond + swmo_prec + swater_speed_ms + swater_depth +\n                 sTemp_Air + seas_wmo + (1|year) + (1|locality/site_irn) +  (swater_depth|coll_date),\n              data = data,  family = binomial)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\nsummary(model1)\n\n Family: binomial  ( logit )\nFormula:          \ncbind(Bulinus_pos_tot, Bulinus_tot - Bulinus_pos_tot) ~ offset(log(duration)) +  \n    site_type + sTemp_Water + spH + sCond + swmo_prec + swater_speed_ms +  \n    swater_depth + sTemp_Air + seas_wmo + (1 | year) + (1 | locality/site_irn) +  \n    (swater_depth | coll_date)\nData: data\n\n     AIC      BIC   logLik deviance df.resid \n  1323.3   1441.7   -640.7   1281.3     2051 \n\nRandom effects:\n\nConditional model:\n Groups            Name         Variance Std.Dev. Corr \n year              (Intercept)  0.04167  0.2041        \n site_irn:locality (Intercept)  1.79468  1.3397        \n locality          (Intercept)  0.91215  0.9551        \n coll_date         (Intercept)  1.40999  1.1874        \n                   swater_depth 0.46082  0.6788   0.76 \nNumber of obs: 2072, groups:  \nyear, 5; site_irn:locality, 89; locality, 20; coll_date, 191\n\nConditional model:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -9.71202    0.52321 -18.562  &lt; 2e-16 ***\nsite_typecanal.3  -0.20119    0.52740  -0.381  0.70285    \nsite_typepond      2.30894    0.81865   2.820  0.00480 ** \nsite_typerice.p   -0.21964    0.84882  -0.259  0.79582    \nsite_typeriver    -0.05417    0.99933  -0.054  0.95677    \nsite_typerivulet   1.09537    1.05508   1.038  0.29918    \nsite_typespillway  0.84839    1.37766   0.616  0.53802    \nsTemp_Water        0.02948    0.16178   0.182  0.85543    \nspH               -0.08128    0.10741  -0.757  0.44919    \nsCond             -0.32655    0.10527  -3.102  0.00192 ** \nswmo_prec         -0.59112    0.32596  -1.813  0.06976 .  \nswater_speed_ms    0.17271    0.08010   2.156  0.03107 *  \nswater_depth      -0.27776    0.19217  -1.445  0.14835    \nsTemp_Air         -0.23545    0.14630  -1.609  0.10753    \nseas_wmowet       -0.36256    0.32166  -1.127  0.25969    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nres = simulateResiduals(model1, plot=TRUE)\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\n\nWe have dispersion problems, but we cannot model the dispersion for binomial models.\nCheck for spatial autocorrelation:\n\n## Spatial\nres2 = recalculateResiduals(res, group = c(data$site_irn))\ngroupLocations = aggregate(cbind(data$sLat, data$sLon ), list( data$site_irn), mean)\ntestSpatialAutocorrelation(res2, x = groupLocations$V1, y = groupLocations$V2)\n\n\n\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res2\nobserved = 0.159982, expected = -0.011364, sd = 0.067031, p-value =\n0.01058\nalternative hypothesis: Distance-based autocorrelation\n\n\nSignificant! Let’s correct for spatial autocorrelation with a correlation structure:\n\nnumFac = numFactor(data$sLat, data$sLon)\ngroup = factor(rep(1, nrow(data)))\ndata$fmonth = as.factor(data$month)\nmodel2 = glmmTMB(cbind(Bulinus_pos_tot, Bulinus_tot - Bulinus_pos_tot )~\n                 offset(log(duration)) + site_type + sTemp_Water + spH +\n                 sCond + swmo_prec + swater_speed_ms + swater_depth +\n                 sTemp_Air + seas_wmo + (1|year) + (1|locality/site_irn) +  (swater_depth|coll_date) + exp(0+numFac|group),\n              data = data,  family = binomial)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\n\n\nres = simulateResiduals(model2, plot = TRUE)\n\n\n\n\nThey look good now!\n\n\n\n\n\n\n\n\n\nSolution of multivariate (joint) species distribution model\n\n\n\n\n\nThe species models are connected by their response to latent variable (unobserved environment). For that, we will transform our dataset with respect to species from wide (sp1, sp2, sp3) to long format (species abundances in one column and a second column telling us the group (species)). In the model then, we will separate the species and their responses by using ~0+Species + Species:(predictors).\nThe latent variable structure is set by the rr(…) object in the formula:\n\nlibrary(lme4)\nlibrary(glmmTMB)\nlibrary(DHARMa)\nlibrary(tidyverse)\ndata = EcoData::snails\ndata$sTemp_Water = scale(data$Temp_Water)\ndata$spH = scale(data$pH)\ndata$swater_speed_ms = scale(data$water_speed_ms)\ndata$swater_depth = scale(data$water_depth)\ndata$sCond = scale(data$Cond)\ndata$swmo_prec = scale(data$wmo_prec)\ndata$syear = scale(data$year)\ndata$sLat = scale(data$Latitude)\ndata$sLon = scale(data$Longitude)\ndata$sTemp_Air = scale(data$Temp_Air)\n\n# Let's remove NAs beforehand:\nrows = rownames(model.matrix(cbind(Bulinus_pos_tot, Bulinus_tot-Bulinus_pos_tot)~sTemp_Water + spH + sLat + sLon + sCond + seas_wmo+ swmo_prec + swater_speed_ms + swater_depth +sTemp_Air+ syear + duration + locality + site_irn + coll_date, data = data))\ndata = data[rows, ]\n\n\ndata =\n  data %&gt;% pivot_longer(cols = c(\"BP_tot\", \"BF_tot\", \"BT_tot\"),\n                      names_to = \"Species\",\n                      values_to = \"Abundance\" )\n\nnumFac = numFactor(data$sLat, data$sLon)\ngroup = factor(rep(1, nrow(data)))\n\nnumFac = numFactor(data$sLat, data$sLon)\ngroup = factor(rep(1, nrow(data)))\ndata$fmonth = as.factor(data$month)\nmodelJoint = glmmTMB(Abundance~ 0 +\n                 offset(log(duration)) + Species + Species:(site_type + \n                  sTemp_Water + spH + sCond + swmo_prec + swater_speed_ms + swater_depth +\n                 sTemp_Air + seas_wmo) + (1|year) + (1|locality/site_irn) +  (swater_depth|coll_date:Species) + exp(0+numFac|group) + rr(Species + 0|locality:site_irn, d = 2),\n                 dispformula = ~0+Species+Species:(swater_speed_ms + swater_depth+sCond),\n              data = data,  family = nbinom2)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\n\nUnconditional residuals:\n\nplot(simulateResiduals(modelJoint))\n\n\n\n\nConditional residuals:\n\npred = predict(modelJoint, re.form = NULL, type = \"response\")\npred_dispersion = predict(modelJoint, re.form = NULL, type = \"disp\")\nsimulations = sapply(1:1000, function(i) rnbinom(length(pred),size = pred_dispersion, mu =  pred))\nres = createDHARMa(simulations, model.frame(modelJoint)[,1], pred)\nplot(res)"
  },
  {
    "objectID": "6C-CaseStudies.html#seed-bank",
    "href": "6C-CaseStudies.html#seed-bank",
    "title": "Appendix C — Case studies",
    "section": "C.6 Seed bank",
    "text": "C.6 Seed bank\n\nlibrary(EcoData)\nlibrary(glmmTMB)\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(DHARMa)\nlibrary(tidyverse)\n?EcoData::seedBank\n\nThe seedBank data set in the EcoData package includes observation on seed bank presence and size in different vegetaition plots.\nThe scientific question is if the ability of plants to build a seed bank depends on their seed traits (see help).\nThe data also contains data on environmental factors and plant traits. You should consider if it is useful to add them to the analysis.\nTasks:\n\nFit a lm/lmm with SBDensity as response\nBonus: Add phylogenetic correlation structure\nFit a glm/glmm with SBPA as response (Bonus: add phylogenetic correlation structure)\n\n\n\n\n\n\n\nSolution for SBDensity (lmm)\n\n\n\nPrepare+scale data:\n\ndata = as.data.frame(EcoData::seedBank)\ndata$sAltitude = scale(data$Altitude)\ndata$sSeedMass = scale(data$SeedMass)\ndata$sSeedShape = scale(data$SeedShape)\ndata$sSeedN = scale(data$SeedN)\ndata$sSeedPr = scale(data$SeedPr)\ndata$sDormRank = scale(data$DormRank)\ndata$sTemp = scale(data$Temp)\ndata$sHum = scale(data$Humidity)\ndata$sNitro = scale(data$Nitrogen)\ndata$sGrazing = scale(data$Grazing)\ndata$sMGT = scale(data$MGT)\ndata$sJwidth = scale(data$Jwidth)\ndata$sEpiStein = scale(data$EpiStein)\ndata$sMGR = scale(data$MGR)\ndata$sT95 = scale(data$T95)\n\n# Let's remove NAs beforehand:\nrows = rownames(model.matrix(SBDensity~sAltitude + sSeedMass + sSeedShape + sSeedN +\n                               sSeedPr + sDormRank + sTemp + sHum + sNitro + sMGT + \n                               sMGR + sEpiStein + sT95 +\n                               sJwidth + sGrazing + Site + Species, data = data))\ndata = data[rows, ]\n\nThe response is highly skewed, so it makes sense to log-transform:\n\nhist(data$SBDensity)\n\n\n\ndata$logSBDensity = log(data$SBDensity + 1)\n\nLet’s fit a base model with with our hypothesis that logSBDensity ~ sSeedMass (Seed Mass) + sSeedShape + sSeedN (Number of Seeds).\nWe set random intercepts on species and sites because we assume that there are species and site specific variations:\n\nmodel1 = lmer(logSBDensity~\n                sSeedMass + sSeedShape + sSeedN + \n                (1|Site) + (1|Species),\n              data = data)\n\nsummary(model1)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logSBDensity ~ sSeedMass + sSeedShape + sSeedN + (1 | Site) +  \n    (1 | Species)\n   Data: data\n\nREML criterion at convergence: 7531.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6261 -0.4945 -0.0823  0.4542  3.1525 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Species  (Intercept) 3.0724   1.7528  \n Site     (Intercept) 0.4705   0.6859  \n Residual             3.6790   1.9181  \nNumber of obs: 1729, groups:  Species, 152; Site, 17\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)   1.5314     0.2271  42.4393   6.743 3.23e-08 ***\nsSeedMass    -0.3316     0.1273 175.6509  -2.604  0.00999 ** \nsSeedShape   -0.3175     0.1546 155.2402  -2.054  0.04168 *  \nsSeedN       -0.1818     0.1289 165.3141  -1.410  0.16028    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) sSdMss sSdShp\nsSeedMass  -0.037              \nsSeedShape -0.034  0.168       \nsSeedN     -0.031  0.039  0.077\n\n\nEnvironmental factors can be potential confounders, let’s add them to the model:\n\nmodel2 = lmer(logSBDensity~\n                sSeedMass + sSeedShape + sSeedN +\n                 sAltitude + sHum + \n                (1|Site) + (1|Species),\n              data = data)\n\nsummary(model2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logSBDensity ~ sSeedMass + sSeedShape + sSeedN + sAltitude +  \n    sHum + (1 | Site) + (1 | Species)\n   Data: data\n\nREML criterion at convergence: 7510.7\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.6468 -0.5089 -0.0720  0.4388  3.2140 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Species  (Intercept) 3.03405  1.7419  \n Site     (Intercept) 0.06929  0.2632  \n Residual             3.68099  1.9186  \nNumber of obs: 1729, groups:  Species, 152; Site, 17\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)   1.65385    0.16664 113.38609   9.925  &lt; 2e-16 ***\nsSeedMass    -0.32886    0.12662 176.33088  -2.597   0.0102 *  \nsSeedShape   -0.32165    0.15372 155.68190  -2.092   0.0380 *  \nsSeedN       -0.18051    0.12816 165.92065  -1.408   0.1609    \nsAltitude    -0.56452    0.10125  17.10538  -5.575 3.27e-05 ***\nsHum          0.14280    0.09738  14.04198   1.466   0.1646    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) sSdMss sSdShp sSeedN sAlttd\nsSeedMass  -0.052                            \nsSeedShape -0.044  0.168                     \nsSeedN     -0.042  0.039  0.077              \nsAltitude  -0.058  0.007 -0.012 -0.006       \nsHum       -0.004 -0.008 -0.003  0.004  0.512\n\n\nsSeedShape and sSeedN are now statistically significant!\nQuestion:\nWhat about the germination temperatures, T50/T95?\nAnswer:\nThey could be mediators, Seed Shape -&gt; T95 -&gt; SBDensity, so it is up to if you want to include them or not!\nResidual checks:\nCheck for missing random slopes:\n\nplot(model2, resid(., rescale=TRUE) ~ fitted(.) | Species, abline = 1)\n\n\n\n\nThere seems to be a pattern!\n\nplot(model2, resid(., rescale=TRUE) ~ sAltitude | Species, abline = 1)\n\n\n\n\nThe pattern seems to be caused by sAltitude (check plot(model2, resid(., rescale=TRUE) ~ sSeedMass | Species, abline = 1) )\nRandom slope model\nLet’s add a random slope on sAlitude:\n\nmodel3 = lmer(logSBDensity~\n                sSeedMass + sSeedShape + sSeedN +\n                sAltitude + sHum + \n                (1|Site) + (sAltitude|Species),\n              data = data)\n\nsummary(model3)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: logSBDensity ~ sSeedMass + sSeedShape + sSeedN + sAltitude +  \n    sHum + (1 | Site) + (sAltitude | Species)\n   Data: data\n\nREML criterion at convergence: 7216.1\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.77162 -0.39884 -0.08256  0.22365  3.06745 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n Species  (Intercept) 2.61864  1.6182        \n          sAltitude   1.31560  1.1470   -0.42\n Site     (Intercept) 0.09023  0.3004        \n Residual             2.79664  1.6723        \nNumber of obs: 1729, groups:  Species, 152; Site, 17\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)   1.52436    0.16898 106.79022   9.021 8.45e-15 ***\nsSeedMass    -0.29923    0.13843 181.27154  -2.162 0.031960 *  \nsSeedShape   -0.25730    0.14094 145.11888  -1.826 0.069969 .  \nsSeedN       -0.15018    0.11145 147.11514  -1.347 0.179918    \nsAltitude    -0.59612    0.15103  54.56729  -3.947 0.000228 ***\nsHum          0.09296    0.10330  13.65567   0.900 0.383740    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) sSdMss sSdShp sSeedN sAlttd\nsSeedMass  -0.042                            \nsSeedShape -0.052  0.139                     \nsSeedN     -0.045  0.037  0.086              \nsAltitude  -0.286  0.040 -0.021 -0.003       \nsHum        0.015 -0.009 -0.009  0.004  0.367\n\n\nResidual checks:\n\nplot(simulateResiduals(model3, re.form=NULL))\n\n\n\n\nThere seems to be dispersion problem.\nBonus: Modeling variance with glmmTMB\n\nmodel4 = glmmTMB(logSBDensity~\n                sSeedMass + sSeedShape + sSeedN +\n                sAltitude + sHum + \n                (1|Site) + (sAltitude|Species),\n              dispformula = ~sAltitude + sHum,\n              data = data)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\nsummary(model4)\n\n Family: gaussian  ( identity )\nFormula:          logSBDensity ~ sSeedMass + sSeedShape + sSeedN + sAltitude +  \n    sHum + (1 | Site) + (sAltitude | Species)\nDispersion:                    ~sAltitude + sHum\nData: data\n\n     AIC      BIC   logLik deviance df.resid \n  7196.3   7267.2  -3585.2   7170.3     1716 \n\nRandom effects:\n\nConditional model:\n Groups   Name        Variance Std.Dev. Corr  \n Site     (Intercept) 0.07148  0.2674         \n Species  (Intercept) 2.53436  1.5920         \n          sAltitude   1.30196  1.1410   -0.44 \n Residual                  NA      NA         \nNumber of obs: 1729, groups:  Site, 17; Species, 152\n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.51414    0.16437   9.212  &lt; 2e-16 ***\nsSeedMass   -0.28916    0.13762  -2.101   0.0356 *  \nsSeedShape  -0.24086    0.13832  -1.741   0.0816 .  \nsSeedN      -0.14540    0.10827  -1.343   0.1793    \nsAltitude   -0.59733    0.14562  -4.102 4.09e-05 ***\nsHum         0.09070    0.09475   0.957   0.3384    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDispersion model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.02897    0.03713  27.713  &lt; 2e-16 ***\nsAltitude   -0.24685    0.04795  -5.148 2.63e-07 ***\nsHum        -0.03151    0.05259  -0.599    0.549    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\n\n\nBonus: Solution for SBDensity (lmm) with phylogenetic correlation structure\n\n\n\n\n\nPrepare phylogeny:\n\nlibrary(ape)\nlibrary(geiger)\n\n\nAttaching package: 'geiger'\n\n\nThe following object is masked from 'package:brms':\n\n    bf\n\nspecies = unique(data$Species)\nspecies_df = data.frame(Species = species)\nrownames(species_df) = species\nobj = name.check(plantPhylo, species_df)\n\n# drop rest of the species\nphyl.upd = drop.tip(plantPhylo, obj$tree_not_data)\nsummary(phyl.upd)\n\n\nPhylogenetic tree: phyl.upd \n\n  Number of tips: 152 \n  Number of nodes: 140 \n  Branch lengths:\n    mean: 22.21203 \n    variance: 624.9334 \n    distribution summary:\n      Min.    1st Qu.     Median    3rd Qu.       Max. \n  0.200000   5.425002  12.800003  30.402499 123.000001 \n  Root edge: 1 \n  First ten tip labels: Tofieldia_pusilla \n                        Tofieldia_calyculata\n                        Veratrum_album\n                        Maianthemum_bifolium\n                        Polygonatum_verticillatum\n                        Juncus_monanthos\n                        Luzula_glabrata\n                        Luzula_sylvatica\n                        Luzula_multiflora\n                        Carex_sempervirens\n  First ten node labels: N398 \n                         N401\n                         Tofieldiaceae\n                         N573\n                         N636\n                         N1019\n                         N1054\n                         N1063\n                         Juncaceae\n                         Luzula\n\n# check the names in the tree and in the data set\nname.check(phyl.upd, species_df)\n\n[1] \"OK\"\n\nphyl.upd2 = multi2di(phyl.upd)\n\nnlme:\n\nlibrary(nlme)\nmodel4 = gls(logSBDensity ~\n              sSeedMass + sSeedShape + sSeedN +\n                 sAltitude + sHum,\n             correlation = corBrownian(phy = phyl.upd2, form =~ Species),\n             data = data)\nsummary(model4)\n\nGeneralized least squares fit by REML\n  Model: logSBDensity ~ sSeedMass + sSeedShape + sSeedN + sAltitude +      sHum \n  Data: data \n        AIC     BIC   logLik\n  -300576.2 -300538 150295.1\n\nCorrelation Structure: corBrownian\n Formula: ~Species \n Parameter estimate(s):\nnumeric(0)\n\nCoefficients:\n                 Value Std.Error   t-value p-value\n(Intercept)  1.8940733 0.7879503  2.403798  0.0163\nsSeedMass   -0.3233717 0.0930947 -3.473578  0.0005\nsSeedShape  -0.1898476 0.1184347 -1.602973  0.1091\nsSeedN      -0.0678053 0.1180418 -0.574418  0.5658\nsAltitude   -0.6393685 0.1998323 -3.199526  0.0014\nsHum         0.0454568 0.1819503  0.249831  0.8027\n\n Correlation: \n           (Intr) sSdMss sSdShp sSeedN sAlttd\nsSeedMass  -0.117                            \nsSeedShape  0.041  0.048                     \nsSeedN     -0.007  0.017  0.037              \nsAltitude   0.167 -0.012 -0.008  0.007       \nsHum        0.083  0.013  0.000 -0.007  0.621\n\nStandardized residuals:\n       Min         Q1        Med         Q3        Max \n-1.4951026 -0.8482715 -0.5644143  1.1540499  4.1762125 \n\nResidual standard error: 2.144554 \nDegrees of freedom: 1729 total; 1723 residual\n\n\nglmmTMB (not perfect because lack of support of specific phylogenetic correlation structures):\n\ndist_phylo = ape::cophenetic.phylo(phyl.upd2) # create distance matrix\ncorrelation_matrix = vcv(phyl.upd2)[unique(data$Species), unique(data$Species)]\n\n###\n#the following code was taken from https://github.com/glmmTMB/glmmTMB/blob/master/misc/fixcorr.rmd\nas.theta.vcov &lt;- function(Sigma,corrs.only=FALSE) {\n    logsd &lt;- log(diag(Sigma))/2\n    cr &lt;- cov2cor(Sigma)\n    cc &lt;- chol(cr)\n    cc &lt;- cc %*% diag(1 / diag(cc))\n    corrs &lt;- cc[upper.tri(cc)]\n    if (corrs.only) return(corrs)\n    ret &lt;- c(logsd,corrs)\n    return(ret)\n}\ncorrs = as.theta.vcov(correlation_matrix, corrs.only=TRUE)\n#####\n\ndata$dummy = factor(rep(0, nrow(data)))\nnsp = length(unique(data$Species))\nmodel5 = glmmTMB(logSBDensity~\n                sSeedMass + sSeedShape + sSeedN + \n                sAltitude + sHum +\n                (1|Site) + (sAltitude|Species) +\n                (1+Species|dummy),\n              dispformula = ~sAltitude + sHum,\n              map=list(theta=factor(c(rep(0, 4), rep(1,nsp),rep(NA,length(corrs))) )),\n              start=list(theta=c(rep(0, 4), rep(0,nsp),corrs)),\n              data = data)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\nsummary(model5)\n\n Family: gaussian  ( identity )\nFormula:          logSBDensity ~ sSeedMass + sSeedShape + sSeedN + sAltitude +  \n    sHum + (1 | Site) + (sAltitude | Species) + (1 + Species |      dummy)\nDispersion:                    ~sAltitude + sHum\nData: data\n\n     AIC      BIC   logLik deviance df.resid \n  7264.6   7324.6  -3621.3   7242.6     1718 \n\nRandom effects:\n\nConditional model:\n Groups   Name                                Variance Std.Dev. Corr          \n Site     (Intercept)                         1.022    1.011                  \n Species  (Intercept)                         1.022    1.011                  \n          sAltitude                           1.022    1.011    0.01          \n dummy    (Intercept)                         2.395    1.547                  \n          SpeciesAchillea_clavennae           2.395    1.547    0.00          \n          SpeciesAchillea_millefolium         2.395    1.547    0.20 0.00     \n          SpeciesAcinos_alpinus               2.395    1.547    0.28 0.00 0.20\n          SpeciesAconitum_tauricum            2.395    1.547    0.31 0.00 0.20\n          SpeciesAdenostyles_alliariae        2.395    1.547    0.00 0.93 0.00\n          SpeciesAgrostis_alpina              2.395    1.547    0.20 0.00 0.34\n          SpeciesAgrostis_capillaris          2.395    1.547    0.31 0.00 0.20\n          SpeciesAjuga_reptans                2.395    1.547    0.31 0.00 0.20\n          SpeciesAlchemilla_vulgaris          2.395    1.547    0.20 0.00 0.26\n          SpeciesAndrosace_chamaejasme        2.395    1.547    0.25 0.00 0.20\n          SpeciesAnemone_nemorosa             2.395    1.547    0.00 0.93 0.00\n          SpeciesAntennaria_dioica            2.395    1.547    0.31 0.00 0.20\n          SpeciesAnthoxanthum_alpinum         2.395    1.547    0.31 0.00 0.20\n          SpeciesAnthoxanthum_odoratum        2.395    1.547    0.00 0.45 0.00\n          SpeciesAnthyllis_vulneraria         2.395    1.547    0.00 0.45 0.00\n          SpeciesAposeris_foetida             2.395    1.547    0.00 0.45 0.00\n          SpeciesArctostaphylos_alpinus       2.395    1.547    0.00 0.45 0.00\n          SpeciesAster_bellidiastrum          2.395    1.547    0.00 0.45 0.00\n          SpeciesBartsia_alpina               2.395    1.547    0.00 0.45 0.00\n          SpeciesBellis_perennis              2.395    1.547    0.00 0.45 0.00\n          SpeciesBetonica_alopecuros          2.395    1.547    0.00 0.45 0.00\n          SpeciesBiscutella_laevigata         2.395    1.547    0.00 0.45 0.00\n          SpeciesBistorta_vivipara            2.395    1.547    0.31 0.00 0.20\n          SpeciesBriza_media                  2.395    1.547    0.31 0.00 0.20\n          SpeciesBuphthalmum_salicifolium     2.395    1.547    0.31 0.00 0.20\n          SpeciesCalamagrostis_varia          2.395    1.547    0.00 0.88 0.00\n          SpeciesCampanula_scheuchzeri        2.395    1.547    0.00 0.88 0.00\n          SpeciesCarduus_defloratus           2.395    1.547    0.20 0.00 0.51\n          SpeciesCarex_caryophyllea           2.395    1.547    0.20 0.00 0.30\n          SpeciesCarex_firma                  2.395    1.547    0.55 0.00 0.20\n          SpeciesCarex_flacca                 2.395    1.547    0.00 0.88 0.00\n          SpeciesCarex_flava                  2.395    1.547    0.40 0.00 0.20\n          SpeciesCarex_ornithopoda            2.395    1.547    0.40 0.00 0.20\n          SpeciesCarex_ornithopodioides       2.395    1.547    0.20 0.00 0.26\n          SpeciesCarex_pallescens             2.395    1.547    0.20 0.00 0.34\n          SpeciesCarex_panicea                2.395    1.547    0.31 0.00 0.20\n          SpeciesCarex_sempervirens           2.395    1.547    0.20 0.00 0.30\n          SpeciesCarex_sylvatica              2.395    1.547    0.31 0.00 0.20\n          SpeciesCarlina_acaulis              2.395    1.547    0.31 0.00 0.20\n          SpeciesCentaurea_jacea              2.395    1.547    0.20 0.00 0.30\n          SpeciesChaerophyllum_hirsutum       2.395    1.547    0.20 0.00 0.34\n          SpeciesClinopodium_vulgare          2.395    1.547    0.00 0.17 0.00\n          SpeciesCrepis_alpestris             2.395    1.547    0.85 0.00 0.20\n          SpeciesCrepis_aurea                 2.395    1.547    0.31 0.00 0.20\n          SpeciesCrepis_biennis               2.395    1.547    0.55 0.00 0.20\n          SpeciesCrepis_terglouensis          2.395    1.547    0.55 0.00 0.20\n          SpeciesCruciata_laevipes            2.395    1.547    0.55 0.00 0.20\n          SpeciesCynosurus_cristatus          2.395    1.547    0.55 0.00 0.20\n          SpeciesDactylis_glomerata           2.395    1.547    0.00 0.88 0.00\n          SpeciesDeschampsia_cespitosa        2.395    1.547    0.00 0.88 0.00\n          SpeciesDryas_octopetala             2.395    1.547    0.00 0.88 0.00\n          SpeciesErica_carnea                 2.395    1.547    0.20 0.00 0.34\n          SpeciesEuphorbia_cyparissias        2.395    1.547    0.20 0.00 0.34\n          SpeciesEuphrasia_montana            2.395    1.547    0.00 0.17 0.00\n          SpeciesEuphrasia_picta              2.395    1.547    0.20 0.00 0.78\n          SpeciesFestuca_alpina               2.395    1.547    0.28 0.00 0.20\n          SpeciesFestuca_pratensis            2.395    1.547    0.28 0.00 0.20\n          SpeciesFestuca_quadriflora          2.395    1.547    0.69 0.00 0.20\n          SpeciesFestuca_rubra                2.395    1.547    0.09 0.00 0.09\n          SpeciesFragaria_vesca               2.395    1.547    0.09 0.00 0.09\n          SpeciesGalium_anisophyllon          2.395    1.547    0.09 0.00 0.09\n          SpeciesGentiana_asclepiadea         2.395    1.547    0.31 0.00 0.20\n          SpeciesGentiana_bavarica            2.395    1.547    0.00 0.88 0.00\n          SpeciesGentiana_pannonica           2.395    1.547    0.00 0.11 0.00\n          SpeciesGentiana_verna               2.395    1.547    0.20 0.00 0.34\n          SpeciesGentianella_aspera           2.395    1.547    0.20 0.00 0.34\n          SpeciesGeranium_sylvaticum          2.395    1.547    0.31 0.00 0.20\n          SpeciesGypsophila_repens            2.395    1.547    0.55 0.00 0.20\n          SpeciesHedysarum_hedysaroides       2.395    1.547    0.55 0.00 0.20\n          SpeciesHelianthemum_nummularium     2.395    1.547    0.31 0.00 0.20\n          SpeciesHelictotrichon_pubescens     2.395    1.547    0.00 0.94 0.00\n          SpeciesHeracleum_austriacum         2.395    1.547    0.00 0.45 0.00\n          SpeciesHieracium_villosum           2.395    1.547    0.31 0.00 0.20\n          SpeciesHippocrepis_comosa           2.395    1.547    0.00 0.88 0.00\n          SpeciesHolcus_lanatus               2.395    1.547    0.40 0.00 0.20\n          SpeciesHomogyne_alpina              2.395    1.547    0.20 0.00 0.26\n          SpeciesHypericum_maculatum          2.395    1.547    0.00 0.45 0.00\n          SpeciesJuncus_monanthos             2.395    1.547    0.00 0.45 0.00\n          SpeciesKnautia_dipsacifolia         2.395    1.547    0.00 0.45 0.00\n          SpeciesLeontodon_hispidus           2.395    1.547    0.25 0.00 0.20\n          SpeciesLeucanthemum_ircutianum      2.395    1.547    0.20 0.00 0.30\n          SpeciesLigusticum_mutellina         2.395    1.547    0.55 0.00 0.20\n          SpeciesLinum_catharticum            2.395    1.547    0.31 0.00 0.20\n          SpeciesLotus_corniculatus           2.395    1.547    0.20 0.00 0.78\n          SpeciesLuzula_glabrata              2.395    1.547    0.09 0.00 0.09\n          SpeciesLuzula_multiflora            2.395    1.547    0.28 0.00 0.20\n          SpeciesLuzula_sylvatica             2.395    1.547    0.28 0.00 0.20\n          SpeciesLysimachia_nemorum           2.395    1.547    0.09 0.00 0.09\n          SpeciesMaianthemum_bifolium         2.395    1.547    0.28 0.00 0.20\n          SpeciesMentha_longifolia            2.395    1.547    0.00 0.17 0.00\n          SpeciesMinuartia_sedoides           2.395    1.547    0.20 0.00 0.30\n          SpeciesMoehringia_ciliata           2.395    1.547    0.31 0.00 0.20\n          SpeciesMyosotis_alpestris           2.395    1.547    0.31 0.00 0.20\n          SpeciesMyosotis_sylvatica           2.395    1.547    0.55 0.00 0.20\n          SpeciesNardus_stricta               2.395    1.547    0.00 0.88 0.00\n          SpeciesOriganum_vulgare             2.395    1.547    0.40 0.00 0.20\n          SpeciesParnassia_palustris          2.395    1.547    0.28 0.00 0.20\n          SpeciesPedicularis_rostratocapitata 2.395    1.547    0.09 0.00 0.09\n          SpeciesPhleum_pratense              2.395    1.547    0.20 0.00 0.21\n          SpeciesPhyteuma_orbiculare          2.395    1.547    0.25 0.00 0.20\n          SpeciesPimpinella_major             2.395    1.547    0.28 0.00 0.20\n          SpeciesPinguicula_alpina            2.395    1.547    0.55 0.00 0.20\n          SpeciesPlantago_lanceolata          2.395    1.547    0.20 0.00 0.34\n          SpeciesPlantago_major               2.395    1.547    0.31 0.00 0.20\n          SpeciesPlantago_media               2.395    1.547    0.31 0.00 0.20\n          SpeciesPoa_alpina                   2.395    1.547    0.31 0.00 0.20\n          SpeciesPoa_pratensis                2.395    1.547    0.00 0.88 0.00\n          SpeciesPoa_trivialis                2.395    1.547    0.00 0.88 0.00\n          SpeciesPolygala_amara               2.395    1.547    0.31 0.00 0.20\n          SpeciesPolygala_chamaebuxus         2.395    1.547    0.31 0.00 0.20\n          SpeciesPolygonatum_verticillatum    2.395    1.547    0.28 0.00 0.20\n          SpeciesPotentilla_aurea             2.395    1.547    0.87 0.00 0.20\n          SpeciesPotentilla_erecta            2.395    1.547    0.36 0.00 0.20\n          SpeciesPrimula_auricula             2.395    1.547    0.00 0.88 0.00\n          SpeciesPrimula_elatior              2.395    1.547    0.25 0.00 0.20\n          SpeciesPrimula_farinosa             2.395    1.547    0.25 0.00 0.20\n          SpeciesPrimula_minima               2.395    1.547    0.85 0.00 0.20\n          SpeciesPrunella_vulgaris            2.395    1.547    0.31 0.00 0.20\n          SpeciesRanunculus_acris             2.395    1.547    0.00 0.97 0.00\n          SpeciesRanunculus_alpestris         2.395    1.547    0.65 0.00 0.20\n          SpeciesRanunculus_montanus          2.395    1.547    0.28 0.00 0.20\n          SpeciesRanunculus_nemorosus         2.395    1.547    0.20 0.00 0.79\n          SpeciesRanunculus_repens            2.395    1.547    0.40 0.00 0.20\n          SpeciesRhinanthus_glacialis         2.395    1.547    0.31 0.00 0.20\n          SpeciesRhodothamnus_chamaecistus    2.395    1.547    0.31 0.00 0.20\n          SpeciesRumex_acetosa                2.395    1.547    0.36 0.00 0.20\n          SpeciesSalix_retusa                 2.395    1.547    0.31 0.00 0.20\n          SpeciesSaxifraga_caesia             2.395    1.547    0.22 0.00 0.20\n          SpeciesScabiosa_lucida              2.395    1.547    0.40 0.00 0.20\n          SpeciesSenecio_abrotanifolius       2.395    1.547    0.25 0.00 0.20\n          SpeciesSesleria_albicans            2.395    1.547    0.25 0.00 0.20\n          SpeciesSilene_acaulis               2.395    1.547    0.20 0.00 0.30\n          SpeciesSoldanella_alpina            2.395    1.547    0.09 0.00 0.09\n          SpeciesSolidago_virgaurea           2.395    1.547    0.00 0.93 0.00\n          SpeciesStellaria_graminea           2.395    1.547    0.00 0.45 0.00\n          SpeciesThesium_alpinum              2.395    1.547    0.00 0.69 0.00\n          SpeciesThymus_pulegioides           2.395    1.547    0.28 0.00 0.20\n          SpeciesTofieldia_calyculata         2.395    1.547    0.00 0.88 0.00\n          SpeciesTofieldia_pusilla            2.395    1.547    0.55 0.00 0.20\n          SpeciesTrifolium_pratense           2.395    1.547    0.31 0.00 0.20\n          SpeciesTrifolium_repens             2.395    1.547    0.00 0.88 0.00\n          SpeciesTrollius_europaeus           2.395    1.547    0.63 0.00 0.20\n          SpeciesVaccinium_myrtillus          2.395    1.547    0.31 0.00 0.20\n          SpeciesVaccinium_vitis-idaea        2.395    1.547    0.31 0.00 0.20\n          SpeciesValeriana_saxatilis          2.395    1.547    0.28 0.00 0.20\n          SpeciesVeratrum_album               2.395    1.547    0.31 0.00 0.20\n          SpeciesVeronica_aphylla             2.395    1.547    0.20 0.00 0.34\n          SpeciesVeronica_chamaedrys          2.395    1.547    0.00 0.11 0.00\n          SpeciesVicia_sepium                 2.395    1.547    0.90 0.00 0.20\n          SpeciesViola_biflora                2.395    1.547    0.40 0.00 0.20\n          SpeciesWillemetia_stipitata         2.395    1.547    0.09 0.00 0.09\n Residual                                        NA       NA                  \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n 0.28                                                                      \n 0.00 0.00                                                                 \n 0.20 0.20 0.00                                                            \n 0.28 0.81 0.00 0.20                                                       \n 0.28 0.85 0.00 0.20 0.81                                                  \n 0.20 0.20 0.00 0.26 0.20 0.20                                             \n 0.25 0.25 0.00 0.20 0.25 0.25 0.20                                        \n 0.00 0.00 0.93 0.00 0.00 0.00 0.00 0.00                                   \n 0.28 0.48 0.00 0.20 0.48 0.48 0.20 0.25 0.00                              \n 0.28 0.77 0.00 0.20 0.77 0.77 0.20 0.25 0.00 0.48                         \n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00                    \n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.91               \n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.92 0.91          \n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.92 0.91 0.92     \n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.92 0.91 0.92 0.92\n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.92 0.91 0.92 0.93\n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.92 0.91 0.92 0.92\n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.91 0.94 0.91 0.91\n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.92 0.91 0.92 0.96\n 0.28 0.77 0.00 0.20 0.77 0.77 0.20 0.25 0.00 0.48 0.83 0.00 0.00 0.00 0.00\n 0.28 0.77 0.00 0.20 0.77 0.77 0.20 0.25 0.00 0.48 0.90 0.00 0.00 0.00 0.00\n 0.28 0.81 0.00 0.20 0.87 0.81 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.45 0.45 0.45 0.45\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.45 0.45 0.45 0.45\n 0.20 0.20 0.00 0.34 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.30 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.45 0.45 0.45 0.45\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.26 0.20 0.20 0.38 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.84 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.28 0.83 0.00 0.20 0.81 0.83 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.30 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.28 0.81 0.00 0.20 0.87 0.81 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.28 0.37 0.00 0.20 0.37 0.37 0.20 0.25 0.00 0.37 0.37 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.30 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.84 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.17 0.17 0.17 0.17\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.37 0.00 0.20 0.37 0.37 0.20 0.25 0.00 0.37 0.37 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.45 0.45 0.45 0.45\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.45 0.45 0.45 0.45\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.45 0.45 0.45 0.45\n 0.20 0.20 0.00 0.43 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.43 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.17 0.17 0.17 0.17\n 0.20 0.20 0.00 0.34 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.72 0.28 0.00 0.20 0.28 0.28 0.20 0.25 0.00 0.28 0.28 0.00 0.00 0.00 0.00\n 0.72 0.28 0.00 0.20 0.28 0.28 0.20 0.25 0.00 0.28 0.28 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.00 0.00 0.00 0.00\n 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.00 0.00 0.00 0.00\n 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.00 0.00 0.00 0.00\n 0.28 0.37 0.00 0.20 0.37 0.37 0.20 0.25 0.00 0.37 0.37 0.00 0.00 0.00 0.00\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.45 0.45 0.45 0.45\n 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00 0.11 0.11 0.11 0.11\n 0.20 0.20 0.00 0.66 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.66 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.28 0.37 0.00 0.20 0.37 0.37 0.20 0.25 0.00 0.37 0.37 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.84 0.00 0.20 0.81 0.84 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.00 0.00 0.93 0.00 0.00 0.00 0.00 0.00 0.93 0.00 0.00 0.45 0.45 0.45 0.45\n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.92 0.91 0.92 0.92\n 0.28 0.37 0.00 0.20 0.37 0.37 0.20 0.25 0.00 0.37 0.37 0.00 0.00 0.00 0.00\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.45 0.45 0.45 0.45\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.26 0.20 0.20 0.27 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.50 0.50 0.50 0.50\n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.50 0.50 0.50 0.50\n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.50 0.50 0.50 0.50\n 0.25 0.25 0.00 0.20 0.25 0.25 0.20 0.43 0.00 0.25 0.25 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.30 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.48 0.00 0.20 0.48 0.48 0.20 0.25 0.00 0.90 0.48 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.34 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.00 0.00 0.00 0.00\n 0.34 0.28 0.00 0.20 0.28 0.28 0.20 0.25 0.00 0.28 0.28 0.00 0.00 0.00 0.00\n 0.72 0.28 0.00 0.20 0.28 0.28 0.20 0.25 0.00 0.28 0.28 0.00 0.00 0.00 0.00\n 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.00 0.00 0.00 0.00\n 0.34 0.28 0.00 0.20 0.28 0.28 0.20 0.25 0.00 0.28 0.28 0.00 0.00 0.00 0.00\n 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.17 0.17 0.17 0.17\n 0.20 0.20 0.00 0.30 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.28 0.85 0.00 0.20 0.81 0.86 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.28 0.85 0.00 0.20 0.81 0.86 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.45 0.45 0.45 0.45\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.72 0.28 0.00 0.20 0.28 0.28 0.20 0.25 0.00 0.28 0.28 0.00 0.00 0.00 0.00\n 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.21 0.20 0.20 0.21 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.25 0.25 0.00 0.20 0.25 0.25 0.20 0.43 0.00 0.25 0.25 0.00 0.00 0.00 0.00\n 0.34 0.28 0.00 0.20 0.28 0.28 0.20 0.25 0.00 0.28 0.28 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.66 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.28 0.85 0.00 0.20 0.81 0.86 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.28 0.83 0.00 0.20 0.81 0.83 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.28 0.85 0.00 0.20 0.81 0.95 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.45 0.45 0.45 0.45\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.45 0.45 0.45 0.45\n 0.28 0.37 0.00 0.20 0.37 0.37 0.20 0.25 0.00 0.37 0.37 0.00 0.00 0.00 0.00\n 0.28 0.85 0.00 0.20 0.81 0.86 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.68 0.28 0.00 0.20 0.28 0.28 0.20 0.25 0.00 0.28 0.28 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.45 0.45 0.45 0.45\n 0.25 0.25 0.00 0.20 0.25 0.25 0.20 0.77 0.00 0.25 0.25 0.00 0.00 0.00 0.00\n 0.25 0.25 0.00 0.20 0.25 0.25 0.20 0.43 0.00 0.25 0.25 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.81 0.00 0.20 0.87 0.81 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.00 0.00 0.93 0.00 0.00 0.00 0.00 0.00 0.93 0.00 0.00 0.45 0.45 0.45 0.45\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.34 0.28 0.00 0.20 0.28 0.28 0.20 0.25 0.00 0.28 0.28 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.34 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.81 0.00 0.20 0.86 0.81 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.28 0.37 0.00 0.20 0.37 0.37 0.20 0.25 0.00 0.37 0.37 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.85 0.00 0.20 0.81 0.91 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.22 0.22 0.00 0.20 0.22 0.22 0.20 0.22 0.00 0.22 0.22 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.25 0.25 0.00 0.20 0.25 0.25 0.20 0.43 0.00 0.25 0.25 0.00 0.00 0.00 0.00\n 0.25 0.25 0.00 0.20 0.25 0.25 0.20 0.43 0.00 0.25 0.25 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.30 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.00 0.00 0.00 0.00\n 0.00 0.00 0.98 0.00 0.00 0.00 0.00 0.00 0.93 0.00 0.00 0.45 0.45 0.45 0.45\n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.50 0.50 0.50 0.50\n 0.00 0.00 0.69 0.00 0.00 0.00 0.00 0.00 0.69 0.00 0.00 0.45 0.45 0.45 0.45\n 0.72 0.28 0.00 0.20 0.28 0.28 0.20 0.25 0.00 0.28 0.28 0.00 0.00 0.00 0.00\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.45 0.45 0.45 0.45\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.81 0.00 0.20 0.87 0.81 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.45 0.45 0.45 0.45\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.83 0.00 0.20 0.81 0.83 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.28 0.81 0.00 0.20 0.87 0.81 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.34 0.28 0.00 0.20 0.28 0.28 0.20 0.25 0.00 0.28 0.28 0.00 0.00 0.00 0.00\n 0.28 0.81 0.00 0.20 0.87 0.81 0.20 0.25 0.00 0.48 0.77 0.00 0.00 0.00 0.00\n 0.20 0.20 0.00 0.66 0.20 0.20 0.26 0.20 0.00 0.20 0.20 0.00 0.00 0.00 0.00\n 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00 0.11 0.11 0.11 0.11\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.28 0.31 0.00 0.20 0.31 0.31 0.20 0.25 0.00 0.31 0.31 0.00 0.00 0.00 0.00\n 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.00 0.00 0.00 0.00\n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n 0.92                                                                      \n 0.92 0.92                                                                 \n 0.91 0.91 0.91                                                            \n 0.92 0.93 0.92 0.91                                                       \n 0.00 0.00 0.00 0.00 0.00                                                  \n 0.00 0.00 0.00 0.00 0.00 0.83                                             \n 0.00 0.00 0.00 0.00 0.00 0.77 0.77                                        \n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00                                   \n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.93                              \n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00                         \n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.30                    \n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20               \n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.91 0.91 0.00 0.00 0.00          \n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.40 0.00     \n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.40 0.00 0.53\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.26 0.26 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.34 0.30 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.81 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.30 0.34 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.88 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.37 0.37 0.37 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.30 0.34 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.34 0.30 0.20 0.00 0.20\n 0.17 0.17 0.17 0.17 0.17 0.00 0.00 0.00 0.17 0.17 0.00 0.00 0.00 0.17 0.00\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.55 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.37 0.37 0.37 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.55 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.55 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.55 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.55 0.00 0.40\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.90 0.90 0.00 0.00 0.00 0.90 0.00\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.90 0.90 0.00 0.00 0.00 0.90 0.00\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.90 0.90 0.00 0.00 0.00 0.90 0.00\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.34 0.30 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.34 0.30 0.20 0.00 0.20\n 0.17 0.17 0.17 0.17 0.17 0.00 0.00 0.00 0.17 0.17 0.00 0.00 0.00 0.17 0.00\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.51 0.30 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.28 0.28 0.28 0.00 0.00 0.20 0.20 0.28 0.00 0.28\n 0.00 0.00 0.00 0.00 0.00 0.28 0.28 0.28 0.00 0.00 0.20 0.20 0.28 0.00 0.28\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.55 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.09 0.09 0.09 0.00 0.00 0.09 0.09 0.09 0.00 0.09\n 0.00 0.00 0.00 0.00 0.00 0.09 0.09 0.09 0.00 0.00 0.09 0.09 0.09 0.00 0.09\n 0.00 0.00 0.00 0.00 0.00 0.09 0.09 0.09 0.00 0.00 0.09 0.09 0.09 0.00 0.09\n 0.00 0.00 0.00 0.00 0.00 0.37 0.37 0.37 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.91 0.91 0.00 0.00 0.00 0.91 0.00\n 0.11 0.11 0.11 0.11 0.11 0.00 0.00 0.00 0.11 0.11 0.00 0.00 0.00 0.11 0.00\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.34 0.30 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.34 0.30 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.37 0.37 0.37 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.55 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.85 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.81 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.88 0.88 0.00 0.00 0.00 0.88 0.00\n 0.99 0.92 0.92 0.91 0.92 0.00 0.00 0.00 0.45 0.45 0.00 0.00 0.00 0.45 0.00\n 0.00 0.00 0.00 0.00 0.00 0.37 0.37 0.37 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.91 0.91 0.00 0.00 0.00 0.91 0.00\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.40 0.00 0.53\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.26 0.26 0.20 0.00 0.20\n 0.50 0.50 0.50 0.50 0.50 0.00 0.00 0.00 0.45 0.45 0.00 0.00 0.00 0.45 0.00\n 0.50 0.50 0.50 0.50 0.50 0.00 0.00 0.00 0.45 0.45 0.00 0.00 0.00 0.45 0.00\n 0.50 0.50 0.50 0.50 0.50 0.00 0.00 0.00 0.45 0.45 0.00 0.00 0.00 0.45 0.00\n 0.00 0.00 0.00 0.00 0.00 0.25 0.25 0.25 0.00 0.00 0.20 0.20 0.25 0.00 0.25\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.30 0.31 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.77 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.48 0.48 0.48 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.51 0.30 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.09 0.09 0.09 0.00 0.00 0.09 0.09 0.09 0.00 0.09\n 0.00 0.00 0.00 0.00 0.00 0.28 0.28 0.28 0.00 0.00 0.20 0.20 0.28 0.00 0.28\n 0.00 0.00 0.00 0.00 0.00 0.28 0.28 0.28 0.00 0.00 0.20 0.20 0.28 0.00 0.28\n 0.00 0.00 0.00 0.00 0.00 0.09 0.09 0.09 0.00 0.00 0.09 0.09 0.09 0.00 0.09\n 0.00 0.00 0.00 0.00 0.00 0.28 0.28 0.28 0.00 0.00 0.20 0.20 0.28 0.00 0.28\n 0.17 0.17 0.17 0.17 0.17 0.00 0.00 0.00 0.17 0.17 0.00 0.00 0.00 0.17 0.00\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.30 0.34 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.81 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.81 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.98 0.00 0.40\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.91 0.91 0.00 0.00 0.00 0.91 0.00\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.40 0.00 0.53\n 0.00 0.00 0.00 0.00 0.00 0.28 0.28 0.28 0.00 0.00 0.20 0.20 0.28 0.00 0.28\n 0.00 0.00 0.00 0.00 0.00 0.09 0.09 0.09 0.00 0.00 0.09 0.09 0.09 0.00 0.09\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.21 0.21 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.25 0.25 0.25 0.00 0.00 0.20 0.20 0.25 0.00 0.25\n 0.00 0.00 0.00 0.00 0.00 0.28 0.28 0.28 0.00 0.00 0.20 0.20 0.28 0.00 0.28\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.55 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.34 0.30 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.81 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.81 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.81 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.91 0.91 0.00 0.00 0.00 0.91 0.00\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.88 0.88 0.00 0.00 0.00 0.88 0.00\n 0.00 0.00 0.00 0.00 0.00 0.37 0.37 0.37 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.81 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.28 0.28 0.28 0.00 0.00 0.20 0.20 0.28 0.00 0.28\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.55 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.36 0.00 0.36\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.90 0.90 0.00 0.00 0.00 0.90 0.00\n 0.00 0.00 0.00 0.00 0.00 0.25 0.25 0.25 0.00 0.00 0.20 0.20 0.25 0.00 0.25\n 0.00 0.00 0.00 0.00 0.00 0.25 0.25 0.25 0.00 0.00 0.20 0.20 0.25 0.00 0.25\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.55 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.88 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.88 0.88 0.00 0.00 0.00 0.88 0.00\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.55 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.28 0.28 0.28 0.00 0.00 0.20 0.20 0.28 0.00 0.28\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.51 0.30 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.40 0.00 0.53\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.86 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.37 0.37 0.37 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.36 0.00 0.36\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.81 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.22 0.22 0.22 0.00 0.00 0.20 0.20 0.22 0.00 0.22\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.40 0.00 0.53\n 0.00 0.00 0.00 0.00 0.00 0.25 0.25 0.25 0.00 0.00 0.20 0.20 0.25 0.00 0.25\n 0.00 0.00 0.00 0.00 0.00 0.25 0.25 0.25 0.00 0.00 0.20 0.20 0.25 0.00 0.25\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.30 0.34 0.20 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.09 0.09 0.09 0.00 0.00 0.09 0.09 0.09 0.00 0.09\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.88 0.88 0.00 0.00 0.00 0.88 0.00\n 0.50 0.50 0.50 0.50 0.50 0.00 0.00 0.00 0.45 0.45 0.00 0.00 0.00 0.45 0.00\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.69 0.69 0.00 0.00 0.00 0.69 0.00\n 0.00 0.00 0.00 0.00 0.00 0.28 0.28 0.28 0.00 0.00 0.20 0.20 0.28 0.00 0.28\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.91 0.91 0.00 0.00 0.00 0.91 0.00\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.83 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.97 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.45 0.45 0.45 0.45 0.45 0.00 0.00 0.00 0.91 0.91 0.00 0.00 0.00 0.91 0.00\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.55 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.81 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.98 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.28 0.28 0.28 0.00 0.00 0.20 0.20 0.28 0.00 0.28\n 0.00 0.00 0.00 0.00 0.00 0.77 0.77 0.94 0.00 0.00 0.20 0.20 0.31 0.00 0.31\n 0.00 0.00 0.00 0.00 0.00 0.20 0.20 0.20 0.00 0.00 0.34 0.30 0.20 0.00 0.20\n 0.11 0.11 0.11 0.11 0.11 0.00 0.00 0.00 0.11 0.11 0.00 0.00 0.00 0.11 0.00\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.55 0.00 0.40\n 0.00 0.00 0.00 0.00 0.00 0.31 0.31 0.31 0.00 0.00 0.20 0.20 0.40 0.00 0.92\n 0.00 0.00 0.00 0.00 0.00 0.09 0.09 0.09 0.00 0.00 0.09 0.09 0.09 0.00 0.09\n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n 0.20                                                                      \n 0.20 0.26                                                                 \n 0.31 0.20 0.20                                                            \n 0.20 0.26 0.30 0.20                                                       \n 0.31 0.20 0.20 0.81 0.20                                                  \n 0.31 0.20 0.20 0.37 0.20 0.37                                             \n 0.20 0.26 0.30 0.20 0.34 0.20 0.20                                        \n 0.20 0.26 0.85 0.20 0.30 0.20 0.20 0.30                                   \n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00                              \n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00                         \n 0.31 0.20 0.20 0.37 0.20 0.37 0.70 0.20 0.20 0.00 0.31                    \n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.55 0.31               \n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.55 0.31 0.55          \n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.55 0.31 0.55 0.88     \n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.55 0.31 0.55 0.88 0.93\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.20 0.26 0.43 0.20 0.30 0.20 0.20 0.30 0.43 0.00 0.20 0.20 0.20 0.20 0.20\n 0.20 0.26 0.43 0.20 0.30 0.20 0.20 0.30 0.43 0.00 0.20 0.20 0.20 0.20 0.20\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.94 0.00 0.00 0.00 0.00 0.00\n 0.20 0.26 0.34 0.20 0.30 0.20 0.20 0.30 0.34 0.00 0.20 0.20 0.20 0.20 0.20\n 0.28 0.20 0.20 0.28 0.20 0.28 0.28 0.20 0.20 0.00 0.28 0.28 0.28 0.28 0.28\n 0.28 0.20 0.20 0.28 0.20 0.28 0.28 0.20 0.20 0.00 0.28 0.28 0.28 0.28 0.28\n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.69 0.31 0.55 0.55 0.55\n 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09\n 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09\n 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09\n 0.31 0.20 0.20 0.37 0.20 0.37 0.38 0.20 0.20 0.00 0.31 0.38 0.31 0.31 0.31\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00\n 0.20 0.26 0.66 0.20 0.30 0.20 0.20 0.30 0.66 0.00 0.20 0.20 0.20 0.20 0.20\n 0.20 0.26 0.66 0.20 0.30 0.20 0.20 0.30 0.66 0.00 0.20 0.20 0.20 0.20 0.20\n 0.31 0.20 0.20 0.37 0.20 0.37 0.38 0.20 0.20 0.00 0.31 0.38 0.31 0.31 0.31\n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.55 0.31 0.55 0.76 0.76\n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.55 0.31 0.55 0.55 0.55\n 0.31 0.20 0.20 0.83 0.20 0.81 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.31 0.20 0.20 0.37 0.20 0.37 0.70 0.20 0.20 0.00 0.31 0.70 0.31 0.31 0.31\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.86 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.40 0.31 0.40 0.40 0.40\n 0.20 0.27 0.26 0.20 0.26 0.20 0.20 0.26 0.26 0.00 0.20 0.20 0.20 0.20 0.20\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.25 0.20 0.20 0.25 0.20 0.25 0.25 0.20 0.20 0.00 0.25 0.25 0.25 0.25 0.25\n 0.20 0.26 0.30 0.20 0.31 0.20 0.20 0.31 0.30 0.00 0.20 0.20 0.20 0.20 0.20\n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.55 0.31 0.55 0.55 0.55\n 0.31 0.20 0.20 0.48 0.20 0.48 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.20 0.26 0.34 0.20 0.30 0.20 0.20 0.30 0.34 0.00 0.20 0.20 0.20 0.20 0.20\n 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09\n 0.28 0.20 0.20 0.28 0.20 0.28 0.28 0.20 0.20 0.00 0.28 0.28 0.28 0.28 0.28\n 0.28 0.20 0.20 0.28 0.20 0.28 0.28 0.20 0.20 0.00 0.28 0.28 0.28 0.28 0.28\n 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09\n 0.28 0.20 0.20 0.28 0.20 0.28 0.28 0.20 0.20 0.00 0.28 0.28 0.28 0.28 0.28\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.20 0.26 0.30 0.20 0.34 0.20 0.20 0.34 0.30 0.00 0.20 0.20 0.20 0.20 0.20\n 0.31 0.20 0.20 0.83 0.20 0.81 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.31 0.20 0.20 0.83 0.20 0.81 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.55 0.31 0.55 0.55 0.55\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.78 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.40 0.31 0.40 0.40 0.40\n 0.28 0.20 0.20 0.28 0.20 0.28 0.28 0.20 0.20 0.00 0.28 0.28 0.28 0.28 0.28\n 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09\n 0.20 0.21 0.21 0.20 0.21 0.20 0.20 0.21 0.21 0.00 0.20 0.20 0.20 0.20 0.20\n 0.25 0.20 0.20 0.25 0.20 0.25 0.25 0.20 0.20 0.00 0.25 0.25 0.25 0.25 0.25\n 0.28 0.20 0.20 0.28 0.20 0.28 0.28 0.20 0.20 0.00 0.28 0.28 0.28 0.28 0.28\n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.55 0.31 0.55 0.76 0.76\n 0.20 0.26 0.66 0.20 0.30 0.20 0.20 0.30 0.66 0.00 0.20 0.20 0.20 0.20 0.20\n 0.31 0.20 0.20 0.83 0.20 0.81 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.31 0.20 0.20 0.91 0.20 0.81 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.31 0.20 0.20 0.83 0.20 0.81 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.31 0.20 0.20 0.37 0.20 0.37 0.70 0.20 0.20 0.00 0.31 0.76 0.31 0.31 0.31\n 0.31 0.20 0.20 0.83 0.20 0.81 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.28 0.20 0.20 0.28 0.20 0.28 0.28 0.20 0.20 0.00 0.28 0.28 0.28 0.28 0.28\n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.85 0.31 0.55 0.55 0.55\n 0.36 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.36 0.31 0.36 0.36 0.36\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.25 0.20 0.20 0.25 0.20 0.25 0.25 0.20 0.20 0.00 0.25 0.25 0.25 0.25 0.25\n 0.25 0.20 0.20 0.25 0.20 0.25 0.25 0.20 0.20 0.00 0.25 0.25 0.25 0.25 0.25\n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.95 0.31 0.55 0.55 0.55\n 0.31 0.20 0.20 0.81 0.20 0.88 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.65 0.31 0.55 0.55 0.55\n 0.28 0.20 0.20 0.28 0.20 0.28 0.28 0.20 0.20 0.00 0.28 0.28 0.28 0.28 0.28\n 0.20 0.26 0.34 0.20 0.30 0.20 0.20 0.30 0.34 0.00 0.20 0.20 0.20 0.20 0.20\n 0.89 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.40 0.31 0.40 0.40 0.40\n 0.31 0.20 0.20 0.81 0.20 0.86 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.31 0.20 0.20 0.37 0.20 0.37 0.38 0.20 0.20 0.00 0.31 0.38 0.31 0.31 0.31\n 0.36 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.36 0.31 0.36 0.36 0.36\n 0.31 0.20 0.20 0.83 0.20 0.81 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.22 0.20 0.20 0.22 0.20 0.22 0.22 0.20 0.20 0.00 0.22 0.22 0.22 0.22 0.22\n 0.86 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.40 0.31 0.40 0.40 0.40\n 0.25 0.20 0.20 0.25 0.20 0.25 0.25 0.20 0.20 0.00 0.25 0.25 0.25 0.25 0.25\n 0.25 0.20 0.20 0.25 0.20 0.25 0.25 0.20 0.20 0.00 0.25 0.25 0.25 0.25 0.25\n 0.20 0.26 0.30 0.20 0.34 0.20 0.20 0.34 0.30 0.00 0.20 0.20 0.20 0.20 0.20\n 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.28 0.20 0.20 0.28 0.20 0.28 0.28 0.20 0.20 0.00 0.28 0.28 0.28 0.28 0.28\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.55 0.31 0.55 0.55 0.55\n 0.31 0.20 0.20 0.81 0.20 0.88 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00\n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.63 0.31 0.55 0.55 0.55\n 0.31 0.20 0.20 0.91 0.20 0.81 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.31 0.20 0.20 0.81 0.20 0.88 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.28 0.20 0.20 0.28 0.20 0.28 0.28 0.20 0.20 0.00 0.28 0.28 0.28 0.28 0.28\n 0.31 0.20 0.20 0.81 0.20 0.88 0.37 0.20 0.20 0.00 0.31 0.37 0.31 0.31 0.31\n 0.20 0.26 0.66 0.20 0.30 0.20 0.20 0.30 0.66 0.00 0.20 0.20 0.20 0.20 0.20\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00\n 0.40 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.85 0.31 0.55 0.55 0.55\n 0.53 0.20 0.20 0.31 0.20 0.31 0.31 0.20 0.20 0.00 0.40 0.31 0.40 0.40 0.40\n 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09\n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n 0.00                                                                      \n 0.00 0.93                                                                 \n 0.00 0.93 0.95                                                            \n 0.20 0.00 0.00 0.00                                                       \n 0.20 0.00 0.00 0.00 0.72                                                  \n 0.00 0.17 0.17 0.17 0.00 0.00                                             \n 0.20 0.00 0.00 0.00 0.34 0.34 0.00                                        \n 0.28 0.00 0.00 0.00 0.20 0.20 0.00 0.20                                   \n 0.28 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.84                              \n 0.55 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28                         \n 0.09 0.00 0.00 0.00 0.09 0.09 0.00 0.09 0.09 0.09 0.09                    \n 0.09 0.00 0.00 0.00 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.92               \n 0.09 0.00 0.00 0.00 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.92 0.92          \n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09     \n 0.00 0.90 0.90 0.90 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.00 0.11 0.11 0.11 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.20 0.00 0.00 0.00 0.43 0.43 0.00 0.34 0.20 0.20 0.20 0.09 0.09 0.09 0.20\n 0.20 0.00 0.00 0.00 0.43 0.43 0.00 0.34 0.20 0.20 0.20 0.09 0.09 0.09 0.20\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.59\n 0.76 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.55 0.09 0.09 0.09 0.31\n 0.55 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.55 0.09 0.09 0.09 0.31\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.00 0.88 0.88 0.88 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.00 0.45 0.45 0.45 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.38\n 0.00 0.90 0.90 0.90 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.40 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.40 0.09 0.09 0.09 0.31\n 0.20 0.00 0.00 0.00 0.26 0.26 0.00 0.26 0.20 0.20 0.20 0.09 0.09 0.09 0.20\n 0.00 0.45 0.45 0.45 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.00 0.45 0.45 0.45 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.00 0.45 0.45 0.45 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.25 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.25 0.25 0.25 0.09 0.09 0.09 0.25\n 0.20 0.00 0.00 0.00 0.30 0.30 0.00 0.30 0.20 0.20 0.20 0.09 0.09 0.09 0.20\n 0.55 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.55 0.09 0.09 0.09 0.31\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.20 0.00 0.00 0.00 0.34 0.34 0.00 0.88 0.20 0.20 0.20 0.09 0.09 0.09 0.20\n 0.09 0.00 0.00 0.00 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.92 0.92 0.99 0.09\n 0.28 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.34 0.34 0.28 0.09 0.09 0.09 0.28\n 0.28 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.75 0.75 0.28 0.09 0.09 0.09 0.28\n 0.09 0.00 0.00 0.00 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.54 0.54 0.54 0.09\n 0.28 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.34 0.34 0.28 0.09 0.09 0.09 0.28\n 0.00 0.17 0.17 0.17 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.20 0.00 0.00 0.00 0.30 0.30 0.00 0.30 0.20 0.20 0.20 0.09 0.09 0.09 0.20\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.55 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.55 0.09 0.09 0.09 0.31\n 0.00 0.90 0.90 0.90 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.40 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.40 0.09 0.09 0.09 0.31\n 0.28 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.76 0.76 0.28 0.09 0.09 0.09 0.28\n 0.09 0.00 0.00 0.00 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.86 0.86 0.86 0.09\n 0.20 0.00 0.00 0.00 0.21 0.21 0.00 0.21 0.20 0.20 0.20 0.09 0.09 0.09 0.20\n 0.25 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.25 0.25 0.25 0.09 0.09 0.09 0.25\n 0.28 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.34 0.34 0.28 0.09 0.09 0.09 0.28\n 0.76 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.55 0.09 0.09 0.09 0.31\n 0.20 0.00 0.00 0.00 0.43 0.43 0.00 0.34 0.20 0.20 0.20 0.09 0.09 0.09 0.20\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.00 0.90 0.90 0.90 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.00 0.88 0.88 0.88 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.38\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.28 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.68 0.68 0.28 0.09 0.09 0.09 0.28\n 0.55 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.69 0.09 0.09 0.09 0.31\n 0.36 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.36 0.09 0.09 0.09 0.31\n 0.00 0.92 0.92 0.92 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.25 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.25 0.25 0.25 0.09 0.09 0.09 0.25\n 0.25 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.25 0.25 0.25 0.09 0.09 0.09 0.25\n 0.55 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.69 0.09 0.09 0.09 0.31\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.00 0.88 0.88 0.88 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.55 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.65 0.09 0.09 0.09 0.31\n 0.28 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.34 0.34 0.28 0.09 0.09 0.09 0.28\n 0.20 0.00 0.00 0.00 0.34 0.34 0.00 0.78 0.20 0.20 0.20 0.09 0.09 0.09 0.20\n 0.40 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.40 0.09 0.09 0.09 0.31\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.80\n 0.36 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.36 0.09 0.09 0.09 0.31\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.22 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.22 0.22 0.22 0.09 0.09 0.09 0.22\n 0.40 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.40 0.09 0.09 0.09 0.31\n 0.25 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.25 0.25 0.25 0.09 0.09 0.09 0.25\n 0.25 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.25 0.25 0.25 0.09 0.09 0.09 0.25\n 0.20 0.00 0.00 0.00 0.30 0.30 0.00 0.30 0.20 0.20 0.20 0.09 0.09 0.09 0.20\n 0.09 0.00 0.00 0.00 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.64 0.64 0.64 0.09\n 0.00 0.88 0.88 0.88 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.00 0.45 0.45 0.45 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.00 0.69 0.69 0.69 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.28 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.76 0.76 0.28 0.09 0.09 0.09 0.28\n 0.00 0.90 0.90 0.90 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.55 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.55 0.09 0.09 0.09 0.31\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.00 0.90 0.90 0.90 0.00 0.00 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.55 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.63 0.09 0.09 0.09 0.31\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.28 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.34 0.34 0.28 0.09 0.09 0.09 0.28\n 0.31 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.31 0.09 0.09 0.09 0.37\n 0.20 0.00 0.00 0.00 0.43 0.43 0.00 0.34 0.20 0.20 0.20 0.09 0.09 0.09 0.20\n 0.00 0.11 0.11 0.11 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n 0.55 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.69 0.09 0.09 0.09 0.31\n 0.40 0.00 0.00 0.00 0.20 0.20 0.00 0.20 0.28 0.28 0.40 0.09 0.09 0.09 0.31\n 0.09 0.00 0.00 0.00 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.68 0.68 0.68 0.09\n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n 0.11                                                                      \n 0.00 0.00                                                                 \n 0.00 0.00 0.84                                                            \n 0.00 0.00 0.20 0.20                                                       \n 0.00 0.00 0.20 0.20 0.31                                                  \n 0.00 0.00 0.20 0.20 0.31 0.55                                             \n 0.00 0.00 0.20 0.20 0.37 0.31 0.31                                        \n 0.88 0.11 0.00 0.00 0.00 0.00 0.00 0.00                                   \n 0.45 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.45                              \n 0.00 0.00 0.20 0.20 0.38 0.31 0.31 0.37 0.00 0.00                         \n 0.91 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.88 0.45 0.00                    \n 0.00 0.00 0.20 0.20 0.31 0.40 0.40 0.31 0.00 0.00 0.31 0.00               \n 0.00 0.00 0.26 0.26 0.20 0.20 0.20 0.20 0.00 0.00 0.20 0.00 0.20          \n 0.45 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.45 0.50 0.00 0.45 0.00 0.00     \n 0.45 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.45 0.50 0.00 0.45 0.00 0.00 0.63\n 0.45 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.45 0.50 0.00 0.45 0.00 0.00 0.63\n 0.00 0.00 0.20 0.20 0.25 0.25 0.25 0.25 0.00 0.00 0.25 0.00 0.25 0.20 0.00\n 0.00 0.00 0.30 0.30 0.20 0.20 0.20 0.20 0.00 0.00 0.20 0.00 0.20 0.26 0.00\n 0.00 0.00 0.20 0.20 0.31 0.55 0.77 0.31 0.00 0.00 0.31 0.00 0.40 0.20 0.00\n 0.00 0.00 0.20 0.20 0.37 0.31 0.31 0.48 0.00 0.00 0.37 0.00 0.31 0.20 0.00\n 0.00 0.00 0.34 0.34 0.20 0.20 0.20 0.20 0.00 0.00 0.20 0.00 0.20 0.26 0.00\n 0.00 0.00 0.09 0.09 0.09 0.09 0.09 0.09 0.00 0.00 0.09 0.00 0.09 0.09 0.00\n 0.00 0.00 0.20 0.20 0.28 0.28 0.28 0.28 0.00 0.00 0.28 0.00 0.28 0.20 0.00\n 0.00 0.00 0.20 0.20 0.28 0.28 0.28 0.28 0.00 0.00 0.28 0.00 0.28 0.20 0.00\n 0.00 0.00 0.09 0.09 0.09 0.09 0.09 0.09 0.00 0.00 0.09 0.00 0.09 0.09 0.00\n 0.00 0.00 0.20 0.20 0.28 0.28 0.28 0.28 0.00 0.00 0.28 0.00 0.28 0.20 0.00\n 0.17 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.17 0.00 0.17 0.00 0.00 0.17\n 0.00 0.00 0.30 0.30 0.20 0.20 0.20 0.20 0.00 0.00 0.20 0.00 0.20 0.26 0.00\n 0.00 0.00 0.20 0.20 0.37 0.31 0.31 0.84 0.00 0.00 0.37 0.00 0.31 0.20 0.00\n 0.00 0.00 0.20 0.20 0.37 0.31 0.31 0.84 0.00 0.00 0.37 0.00 0.31 0.20 0.00\n 0.00 0.00 0.20 0.20 0.31 0.55 0.85 0.31 0.00 0.00 0.31 0.00 0.40 0.20 0.00\n 0.91 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.88 0.45 0.00 0.91 0.00 0.00 0.45\n 0.00 0.00 0.20 0.20 0.31 0.40 0.40 0.31 0.00 0.00 0.31 0.00 0.78 0.20 0.00\n 0.00 0.00 0.20 0.20 0.28 0.28 0.28 0.28 0.00 0.00 0.28 0.00 0.28 0.20 0.00\n 0.00 0.00 0.09 0.09 0.09 0.09 0.09 0.09 0.00 0.00 0.09 0.00 0.09 0.09 0.00\n 0.00 0.00 0.21 0.21 0.20 0.20 0.20 0.20 0.00 0.00 0.20 0.00 0.20 0.21 0.00\n 0.00 0.00 0.20 0.20 0.25 0.25 0.25 0.25 0.00 0.00 0.25 0.00 0.25 0.20 0.00\n 0.00 0.00 0.20 0.20 0.28 0.28 0.28 0.28 0.00 0.00 0.28 0.00 0.28 0.20 0.00\n 0.00 0.00 0.20 0.20 0.31 0.82 0.55 0.31 0.00 0.00 0.31 0.00 0.40 0.20 0.00\n 0.00 0.00 0.80 0.80 0.20 0.20 0.20 0.20 0.00 0.00 0.20 0.00 0.20 0.26 0.00\n 0.00 0.00 0.20 0.20 0.37 0.31 0.31 0.84 0.00 0.00 0.37 0.00 0.31 0.20 0.00\n 0.00 0.00 0.20 0.20 0.37 0.31 0.31 0.83 0.00 0.00 0.37 0.00 0.31 0.20 0.00\n 0.00 0.00 0.20 0.20 0.37 0.31 0.31 0.84 0.00 0.00 0.37 0.00 0.31 0.20 0.00\n 0.91 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.88 0.45 0.00 0.91 0.00 0.00 0.45\n 0.88 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.88 0.45 0.00 0.88 0.00 0.00 0.45\n 0.00 0.00 0.20 0.20 0.38 0.31 0.31 0.37 0.00 0.00 0.70 0.00 0.31 0.20 0.00\n 0.00 0.00 0.20 0.20 0.37 0.31 0.31 0.84 0.00 0.00 0.37 0.00 0.31 0.20 0.00\n 0.00 0.00 0.20 0.20 0.28 0.28 0.28 0.28 0.00 0.00 0.28 0.00 0.28 0.20 0.00\n 0.00 0.00 0.20 0.20 0.31 0.55 0.55 0.31 0.00 0.00 0.31 0.00 0.40 0.20 0.00\n 0.00 0.00 0.20 0.20 0.31 0.36 0.36 0.31 0.00 0.00 0.31 0.00 0.36 0.20 0.00\n 0.90 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.88 0.45 0.00 0.90 0.00 0.00 0.45\n 0.00 0.00 0.20 0.20 0.25 0.25 0.25 0.25 0.00 0.00 0.25 0.00 0.25 0.20 0.00\n 0.00 0.00 0.20 0.20 0.25 0.25 0.25 0.25 0.00 0.00 0.25 0.00 0.25 0.20 0.00\n 0.00 0.00 0.20 0.20 0.31 0.55 0.55 0.31 0.00 0.00 0.31 0.00 0.40 0.20 0.00\n 0.00 0.00 0.20 0.20 0.37 0.31 0.31 0.81 0.00 0.00 0.37 0.00 0.31 0.20 0.00\n 0.88 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.94 0.45 0.00 0.88 0.00 0.00 0.45\n 0.00 0.00 0.20 0.20 0.31 0.55 0.55 0.31 0.00 0.00 0.31 0.00 0.40 0.20 0.00\n 0.00 0.00 0.20 0.20 0.28 0.28 0.28 0.28 0.00 0.00 0.28 0.00 0.28 0.20 0.00\n 0.00 0.00 0.34 0.34 0.20 0.20 0.20 0.20 0.00 0.00 0.20 0.00 0.20 0.26 0.00\n 0.00 0.00 0.20 0.20 0.31 0.40 0.40 0.31 0.00 0.00 0.31 0.00 0.86 0.20 0.00\n 0.00 0.00 0.20 0.20 0.37 0.31 0.31 0.81 0.00 0.00 0.37 0.00 0.31 0.20 0.00\n 0.00 0.00 0.20 0.20 0.59 0.31 0.31 0.37 0.00 0.00 0.38 0.00 0.31 0.20 0.00\n 0.00 0.00 0.20 0.20 0.31 0.36 0.36 0.31 0.00 0.00 0.31 0.00 0.36 0.20 0.00\n 0.00 0.00 0.20 0.20 0.37 0.31 0.31 0.84 0.00 0.00 0.37 0.00 0.31 0.20 0.00\n 0.00 0.00 0.20 0.20 0.22 0.22 0.22 0.22 0.00 0.00 0.22 0.00 0.22 0.20 0.00\n 0.00 0.00 0.20 0.20 0.31 0.40 0.40 0.31 0.00 0.00 0.31 0.00 0.95 0.20 0.00\n 0.00 0.00 0.20 0.20 0.25 0.25 0.25 0.25 0.00 0.00 0.25 0.00 0.25 0.20 0.00\n 0.00 0.00 0.20 0.20 0.25 0.25 0.25 0.25 0.00 0.00 0.25 0.00 0.25 0.20 0.00\n 0.00 0.00 0.30 0.30 0.20 0.20 0.20 0.20 0.00 0.00 0.20 0.00 0.20 0.26 0.00\n 0.00 0.00 0.09 0.09 0.09 0.09 0.09 0.09 0.00 0.00 0.09 0.00 0.09 0.09 0.00\n 0.88 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.93 0.45 0.00 0.88 0.00 0.00 0.45\n 0.45 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.45 0.50 0.00 0.45 0.00 0.00 0.63\n 0.69 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.69 0.45 0.00 0.69 0.00 0.00 0.45\n 0.00 0.00 0.20 0.20 0.28 0.28 0.28 0.28 0.00 0.00 0.28 0.00 0.28 0.20 0.00\n 0.91 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.88 0.45 0.00 0.91 0.00 0.00 0.45\n 0.00 0.00 0.20 0.20 0.31 0.55 0.83 0.31 0.00 0.00 0.31 0.00 0.40 0.20 0.00\n 0.00 0.00 0.20 0.20 0.37 0.31 0.31 0.81 0.00 0.00 0.37 0.00 0.31 0.20 0.00\n 0.91 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.88 0.45 0.00 0.91 0.00 0.00 0.45\n 0.00 0.00 0.20 0.20 0.31 0.55 0.55 0.31 0.00 0.00 0.31 0.00 0.40 0.20 0.00\n 0.00 0.00 0.20 0.20 0.37 0.31 0.31 0.83 0.00 0.00 0.37 0.00 0.31 0.20 0.00\n 0.00 0.00 0.20 0.20 0.37 0.31 0.31 0.81 0.00 0.00 0.37 0.00 0.31 0.20 0.00\n 0.00 0.00 0.20 0.20 0.28 0.28 0.28 0.28 0.00 0.00 0.28 0.00 0.28 0.20 0.00\n 0.00 0.00 0.20 0.20 0.37 0.31 0.31 0.81 0.00 0.00 0.37 0.00 0.31 0.20 0.00\n 0.00 0.00 0.74 0.74 0.20 0.20 0.20 0.20 0.00 0.00 0.20 0.00 0.20 0.26 0.00\n 0.11 0.39 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.11 0.00 0.11 0.00 0.00 0.11\n 0.00 0.00 0.20 0.20 0.31 0.55 0.55 0.31 0.00 0.00 0.31 0.00 0.40 0.20 0.00\n 0.00 0.00 0.20 0.20 0.31 0.40 0.40 0.31 0.00 0.00 0.31 0.00 0.53 0.20 0.00\n 0.00 0.00 0.09 0.09 0.09 0.09 0.09 0.09 0.00 0.00 0.09 0.00 0.09 0.09 0.00\n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n 0.75                                                                      \n 0.00 0.00                                                                 \n 0.00 0.00 0.20                                                            \n 0.00 0.00 0.25 0.20                                                       \n 0.00 0.00 0.25 0.20 0.31                                                  \n 0.00 0.00 0.20 0.30 0.20 0.20                                             \n 0.00 0.00 0.09 0.09 0.09 0.09 0.09                                        \n 0.00 0.00 0.25 0.20 0.28 0.28 0.20 0.09                                   \n 0.00 0.00 0.25 0.20 0.28 0.28 0.20 0.09 0.34                              \n 0.00 0.00 0.09 0.09 0.09 0.09 0.09 0.54 0.09 0.09                         \n 0.00 0.00 0.25 0.20 0.28 0.28 0.20 0.09 0.51 0.34 0.09                    \n 0.17 0.17 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00               \n 0.00 0.00 0.20 0.31 0.20 0.20 0.30 0.09 0.20 0.20 0.09 0.20 0.00          \n 0.00 0.00 0.25 0.20 0.31 0.48 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20     \n 0.00 0.00 0.25 0.20 0.31 0.48 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.96\n 0.00 0.00 0.25 0.20 0.77 0.31 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.31\n 0.45 0.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00\n 0.00 0.00 0.25 0.20 0.40 0.31 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.31\n 0.00 0.00 0.25 0.20 0.28 0.28 0.20 0.09 0.34 0.75 0.09 0.34 0.00 0.20 0.28\n 0.00 0.00 0.09 0.09 0.09 0.09 0.09 0.86 0.09 0.09 0.54 0.09 0.00 0.09 0.09\n 0.00 0.00 0.20 0.21 0.20 0.20 0.21 0.09 0.20 0.20 0.09 0.20 0.00 0.21 0.20\n 0.00 0.00 0.78 0.20 0.25 0.25 0.20 0.09 0.25 0.25 0.09 0.25 0.00 0.20 0.25\n 0.00 0.00 0.25 0.20 0.28 0.28 0.20 0.09 0.51 0.34 0.09 0.86 0.00 0.20 0.28\n 0.00 0.00 0.25 0.20 0.55 0.31 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.31\n 0.00 0.00 0.20 0.30 0.20 0.20 0.34 0.09 0.20 0.20 0.09 0.20 0.00 0.30 0.20\n 0.00 0.00 0.25 0.20 0.31 0.48 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.96\n 0.00 0.00 0.25 0.20 0.31 0.48 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.83\n 0.00 0.00 0.25 0.20 0.31 0.48 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.86\n 0.45 0.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00\n 0.45 0.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00\n 0.00 0.00 0.25 0.20 0.31 0.37 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.37\n 0.00 0.00 0.25 0.20 0.31 0.48 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.89\n 0.00 0.00 0.25 0.20 0.28 0.28 0.20 0.09 0.34 0.68 0.09 0.34 0.00 0.20 0.28\n 0.00 0.00 0.25 0.20 0.55 0.31 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.31\n 0.00 0.00 0.25 0.20 0.36 0.31 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.31\n 0.45 0.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00\n 0.00 0.00 0.43 0.20 0.25 0.25 0.20 0.09 0.25 0.25 0.09 0.25 0.00 0.20 0.25\n 0.00 0.00 0.78 0.20 0.25 0.25 0.20 0.09 0.25 0.25 0.09 0.25 0.00 0.20 0.25\n 0.00 0.00 0.25 0.20 0.55 0.31 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.31\n 0.00 0.00 0.25 0.20 0.31 0.48 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.81\n 0.45 0.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00\n 0.00 0.00 0.25 0.20 0.55 0.31 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.31\n 0.00 0.00 0.25 0.20 0.28 0.28 0.20 0.09 0.52 0.34 0.09 0.51 0.00 0.20 0.28\n 0.00 0.00 0.20 0.30 0.20 0.20 0.78 0.09 0.20 0.20 0.09 0.20 0.00 0.30 0.20\n 0.00 0.00 0.25 0.20 0.40 0.31 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.31\n 0.00 0.00 0.25 0.20 0.31 0.48 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.81\n 0.00 0.00 0.25 0.20 0.31 0.37 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.37\n 0.00 0.00 0.25 0.20 0.36 0.31 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.31\n 0.00 0.00 0.25 0.20 0.31 0.48 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.86\n 0.00 0.00 0.22 0.20 0.22 0.22 0.20 0.09 0.22 0.22 0.09 0.22 0.00 0.20 0.22\n 0.00 0.00 0.25 0.20 0.40 0.31 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.31\n 0.00 0.00 0.78 0.20 0.25 0.25 0.20 0.09 0.25 0.25 0.09 0.25 0.00 0.20 0.25\n 0.00 0.00 0.78 0.20 0.25 0.25 0.20 0.09 0.25 0.25 0.09 0.25 0.00 0.20 0.25\n 0.00 0.00 0.20 0.31 0.20 0.20 0.30 0.09 0.20 0.20 0.09 0.20 0.00 0.40 0.20\n 0.00 0.00 0.09 0.09 0.09 0.09 0.09 0.64 0.09 0.09 0.54 0.09 0.00 0.09 0.09\n 0.45 0.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00\n 0.75 0.75 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00\n 0.45 0.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00\n 0.00 0.00 0.25 0.20 0.28 0.28 0.20 0.09 0.34 0.75 0.09 0.34 0.00 0.20 0.28\n 0.45 0.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00\n 0.00 0.00 0.25 0.20 0.77 0.31 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.31\n 0.00 0.00 0.25 0.20 0.31 0.48 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.81\n 0.45 0.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.17 0.00 0.00\n 0.00 0.00 0.25 0.20 0.55 0.31 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.31\n 0.00 0.00 0.25 0.20 0.31 0.48 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.83\n 0.00 0.00 0.25 0.20 0.31 0.48 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.81\n 0.00 0.00 0.25 0.20 0.28 0.28 0.20 0.09 0.46 0.34 0.09 0.46 0.00 0.20 0.28\n 0.00 0.00 0.25 0.20 0.31 0.48 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.81\n 0.00 0.00 0.20 0.30 0.20 0.20 0.34 0.09 0.20 0.20 0.09 0.20 0.00 0.30 0.20\n 0.11 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00\n 0.00 0.00 0.25 0.20 0.55 0.31 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.31\n 0.00 0.00 0.25 0.20 0.40 0.31 0.20 0.09 0.28 0.28 0.09 0.28 0.00 0.20 0.31\n 0.00 0.00 0.09 0.09 0.09 0.09 0.09 0.68 0.09 0.09 0.54 0.09 0.00 0.09 0.09\n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n 0.31                                                                      \n 0.00 0.00                                                                 \n 0.31 0.40 0.00                                                            \n 0.28 0.28 0.00 0.28                                                       \n 0.09 0.09 0.00 0.09 0.09                                                  \n 0.20 0.20 0.00 0.20 0.20 0.09                                             \n 0.25 0.25 0.00 0.25 0.25 0.09 0.20                                        \n 0.28 0.28 0.00 0.28 0.34 0.09 0.20 0.25                                   \n 0.31 0.55 0.00 0.40 0.28 0.09 0.20 0.25 0.28                              \n 0.20 0.20 0.00 0.20 0.20 0.09 0.21 0.20 0.20 0.20                         \n 0.98 0.31 0.00 0.31 0.28 0.09 0.20 0.25 0.28 0.31 0.20                    \n 0.83 0.31 0.00 0.31 0.28 0.09 0.20 0.25 0.28 0.31 0.20 0.83               \n 0.86 0.31 0.00 0.31 0.28 0.09 0.20 0.25 0.28 0.31 0.20 0.86 0.83          \n 0.00 0.00 0.94 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00     \n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.88\n 0.37 0.31 0.00 0.31 0.28 0.09 0.20 0.25 0.28 0.31 0.20 0.37 0.37 0.37 0.00\n 0.89 0.31 0.00 0.31 0.28 0.09 0.20 0.25 0.28 0.31 0.20 0.89 0.83 0.86 0.00\n 0.28 0.28 0.00 0.28 0.68 0.09 0.20 0.25 0.34 0.28 0.20 0.28 0.28 0.28 0.00\n 0.31 0.55 0.00 0.40 0.28 0.09 0.20 0.25 0.28 0.55 0.20 0.31 0.31 0.31 0.00\n 0.31 0.36 0.00 0.36 0.28 0.09 0.20 0.25 0.28 0.36 0.20 0.31 0.31 0.31 0.00\n 0.00 0.00 0.90 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.90\n 0.25 0.25 0.00 0.25 0.25 0.09 0.20 0.43 0.25 0.25 0.20 0.25 0.25 0.25 0.00\n 0.25 0.25 0.00 0.25 0.25 0.09 0.20 0.79 0.25 0.25 0.20 0.25 0.25 0.25 0.00\n 0.31 0.55 0.00 0.40 0.28 0.09 0.20 0.25 0.28 0.55 0.20 0.31 0.31 0.31 0.00\n 0.81 0.31 0.00 0.31 0.28 0.09 0.20 0.25 0.28 0.31 0.20 0.81 0.81 0.81 0.00\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.88\n 0.31 0.55 0.00 0.40 0.28 0.09 0.20 0.25 0.28 0.55 0.20 0.31 0.31 0.31 0.00\n 0.28 0.28 0.00 0.28 0.34 0.09 0.20 0.25 0.51 0.28 0.20 0.28 0.28 0.28 0.00\n 0.20 0.20 0.00 0.20 0.20 0.09 0.21 0.20 0.20 0.20 0.34 0.20 0.20 0.20 0.00\n 0.31 0.40 0.00 0.78 0.28 0.09 0.20 0.25 0.28 0.40 0.20 0.31 0.31 0.31 0.00\n 0.81 0.31 0.00 0.31 0.28 0.09 0.20 0.25 0.28 0.31 0.20 0.81 0.81 0.81 0.00\n 0.37 0.31 0.00 0.31 0.28 0.09 0.20 0.25 0.28 0.31 0.20 0.37 0.37 0.37 0.00\n 0.31 0.36 0.00 0.36 0.28 0.09 0.20 0.25 0.28 0.36 0.20 0.31 0.31 0.31 0.00\n 0.86 0.31 0.00 0.31 0.28 0.09 0.20 0.25 0.28 0.31 0.20 0.86 0.83 0.91 0.00\n 0.22 0.22 0.00 0.22 0.22 0.09 0.20 0.22 0.22 0.22 0.20 0.22 0.22 0.22 0.00\n 0.31 0.40 0.00 0.78 0.28 0.09 0.20 0.25 0.28 0.40 0.20 0.31 0.31 0.31 0.00\n 0.25 0.25 0.00 0.25 0.25 0.09 0.20 0.79 0.25 0.25 0.20 0.25 0.25 0.25 0.00\n 0.25 0.25 0.00 0.25 0.25 0.09 0.20 0.79 0.25 0.25 0.20 0.25 0.25 0.25 0.00\n 0.20 0.20 0.00 0.20 0.20 0.09 0.21 0.20 0.20 0.20 0.30 0.20 0.20 0.20 0.00\n 0.09 0.09 0.00 0.09 0.09 0.64 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.00\n 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.88\n 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.45\n 0.00 0.00 0.69 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.69\n 0.28 0.28 0.00 0.28 0.83 0.09 0.20 0.25 0.34 0.28 0.20 0.28 0.28 0.28 0.00\n 0.00 0.00 0.94 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.96\n 0.31 0.83 0.00 0.40 0.28 0.09 0.20 0.25 0.28 0.55 0.20 0.31 0.31 0.31 0.00\n 0.81 0.31 0.00 0.31 0.28 0.09 0.20 0.25 0.28 0.31 0.20 0.81 0.81 0.81 0.00\n 0.00 0.00 0.91 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.91\n 0.31 0.55 0.00 0.40 0.28 0.09 0.20 0.25 0.28 0.55 0.20 0.31 0.31 0.31 0.00\n 0.83 0.31 0.00 0.31 0.28 0.09 0.20 0.25 0.28 0.31 0.20 0.83 0.94 0.83 0.00\n 0.81 0.31 0.00 0.31 0.28 0.09 0.20 0.25 0.28 0.31 0.20 0.81 0.81 0.81 0.00\n 0.28 0.28 0.00 0.28 0.34 0.09 0.20 0.25 0.46 0.28 0.20 0.28 0.28 0.28 0.00\n 0.81 0.31 0.00 0.31 0.28 0.09 0.20 0.25 0.28 0.31 0.20 0.81 0.81 0.81 0.00\n 0.20 0.20 0.00 0.20 0.20 0.09 0.21 0.20 0.20 0.20 0.74 0.20 0.20 0.20 0.00\n 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.11\n 0.31 0.55 0.00 0.40 0.28 0.09 0.20 0.25 0.28 0.55 0.20 0.31 0.31 0.31 0.00\n 0.31 0.40 0.00 0.53 0.28 0.09 0.20 0.25 0.28 0.40 0.20 0.31 0.31 0.31 0.00\n 0.09 0.09 0.00 0.09 0.09 0.68 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.00\n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n 0.00                                                                      \n 0.00 0.37                                                                 \n 0.00 0.28 0.28                                                            \n 0.00 0.31 0.31 0.28                                                       \n 0.00 0.31 0.31 0.28 0.36                                                  \n 0.88 0.00 0.00 0.00 0.00 0.00                                             \n 0.00 0.25 0.25 0.25 0.25 0.25 0.00                                        \n 0.00 0.25 0.25 0.25 0.25 0.25 0.00 0.43                                   \n 0.00 0.31 0.31 0.28 0.85 0.36 0.00 0.25 0.25                              \n 0.00 0.37 0.81 0.28 0.31 0.31 0.00 0.25 0.25 0.31                         \n 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.00 0.00                    \n 0.00 0.31 0.31 0.28 0.65 0.36 0.00 0.25 0.25 0.65 0.31 0.00               \n 0.00 0.28 0.28 0.34 0.28 0.28 0.00 0.25 0.25 0.28 0.28 0.00 0.28          \n 0.00 0.20 0.20 0.20 0.20 0.20 0.00 0.20 0.20 0.20 0.20 0.00 0.20 0.20     \n 0.00 0.31 0.31 0.28 0.40 0.36 0.00 0.25 0.25 0.40 0.31 0.00 0.40 0.28 0.20\n 0.00 0.37 0.81 0.28 0.31 0.31 0.00 0.25 0.25 0.31 0.86 0.00 0.31 0.28 0.20\n 0.00 0.38 0.37 0.28 0.31 0.31 0.00 0.25 0.25 0.31 0.37 0.00 0.31 0.28 0.20\n 0.00 0.31 0.31 0.28 0.36 0.91 0.00 0.25 0.25 0.36 0.31 0.00 0.36 0.28 0.20\n 0.00 0.37 0.86 0.28 0.31 0.31 0.00 0.25 0.25 0.31 0.81 0.00 0.31 0.28 0.20\n 0.00 0.22 0.22 0.22 0.22 0.22 0.00 0.22 0.22 0.22 0.22 0.00 0.22 0.22 0.20\n 0.00 0.31 0.31 0.28 0.40 0.36 0.00 0.25 0.25 0.40 0.31 0.00 0.40 0.28 0.20\n 0.00 0.25 0.25 0.25 0.25 0.25 0.00 0.43 0.79 0.25 0.25 0.00 0.25 0.25 0.20\n 0.00 0.25 0.25 0.25 0.25 0.25 0.00 0.43 0.80 0.25 0.25 0.00 0.25 0.25 0.20\n 0.00 0.20 0.20 0.20 0.20 0.20 0.00 0.20 0.20 0.20 0.20 0.00 0.20 0.20 0.30\n 0.00 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.09\n 0.88 0.00 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.00 0.00 0.93 0.00 0.00 0.00\n 0.45 0.00 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.00 0.00 0.45 0.00 0.00 0.00\n 0.69 0.00 0.00 0.00 0.00 0.00 0.69 0.00 0.00 0.00 0.00 0.69 0.00 0.00 0.00\n 0.00 0.28 0.28 0.68 0.28 0.28 0.00 0.25 0.25 0.28 0.28 0.00 0.28 0.34 0.20\n 0.88 0.00 0.00 0.00 0.00 0.00 0.90 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.00\n 0.00 0.31 0.31 0.28 0.55 0.36 0.00 0.25 0.25 0.55 0.31 0.00 0.55 0.28 0.20\n 0.00 0.37 0.81 0.28 0.31 0.31 0.00 0.25 0.25 0.31 0.88 0.00 0.31 0.28 0.20\n 0.88 0.00 0.00 0.00 0.00 0.00 0.90 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.00\n 0.00 0.31 0.31 0.28 0.63 0.36 0.00 0.25 0.25 0.63 0.31 0.00 0.63 0.28 0.20\n 0.00 0.37 0.83 0.28 0.31 0.31 0.00 0.25 0.25 0.31 0.81 0.00 0.31 0.28 0.20\n 0.00 0.37 0.81 0.28 0.31 0.31 0.00 0.25 0.25 0.31 0.88 0.00 0.31 0.28 0.20\n 0.00 0.28 0.28 0.34 0.28 0.28 0.00 0.25 0.25 0.28 0.28 0.00 0.28 0.46 0.20\n 0.00 0.37 0.81 0.28 0.31 0.31 0.00 0.25 0.25 0.31 0.88 0.00 0.31 0.28 0.20\n 0.00 0.20 0.20 0.20 0.20 0.20 0.00 0.20 0.20 0.20 0.20 0.00 0.20 0.20 0.34\n 0.11 0.00 0.00 0.00 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.11 0.00 0.00 0.00\n 0.00 0.31 0.31 0.28 0.87 0.36 0.00 0.25 0.25 0.85 0.31 0.00 0.65 0.28 0.20\n 0.00 0.31 0.31 0.28 0.40 0.36 0.00 0.25 0.25 0.40 0.31 0.00 0.40 0.28 0.20\n 0.00 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.00 0.09 0.09 0.09\n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n                                                                           \n 0.31                                                                      \n 0.31 0.37                                                                 \n 0.36 0.31 0.31                                                            \n 0.31 0.81 0.37 0.31                                                       \n 0.22 0.22 0.22 0.22 0.22                                                  \n 0.86 0.31 0.31 0.36 0.31 0.22                                             \n 0.25 0.25 0.25 0.25 0.25 0.22 0.25                                        \n 0.25 0.25 0.25 0.25 0.25 0.22 0.25 0.79                                   \n 0.20 0.20 0.20 0.20 0.20 0.20 0.20 0.20 0.20                              \n 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09                         \n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00                    \n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.45               \n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.69 0.45          \n 0.28 0.28 0.28 0.28 0.28 0.22 0.28 0.25 0.25 0.20 0.09 0.00 0.00 0.00     \n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.88 0.45 0.69 0.00\n 0.40 0.31 0.31 0.36 0.31 0.22 0.40 0.25 0.25 0.20 0.09 0.00 0.00 0.00 0.28\n 0.31 0.86 0.37 0.31 0.81 0.22 0.31 0.25 0.25 0.20 0.09 0.00 0.00 0.00 0.28\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.88 0.45 0.69 0.00\n 0.40 0.31 0.31 0.36 0.31 0.22 0.40 0.25 0.25 0.20 0.09 0.00 0.00 0.00 0.28\n 0.31 0.81 0.37 0.31 0.83 0.22 0.31 0.25 0.25 0.20 0.09 0.00 0.00 0.00 0.28\n 0.31 0.86 0.37 0.31 0.81 0.22 0.31 0.25 0.25 0.20 0.09 0.00 0.00 0.00 0.28\n 0.28 0.28 0.28 0.28 0.28 0.22 0.28 0.25 0.25 0.20 0.09 0.00 0.00 0.00 0.34\n 0.31 0.86 0.37 0.31 0.81 0.22 0.31 0.25 0.25 0.20 0.09 0.00 0.00 0.00 0.28\n 0.20 0.20 0.20 0.20 0.20 0.20 0.20 0.20 0.20 0.30 0.09 0.00 0.00 0.00 0.20\n 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.11 0.11 0.11 0.00\n 0.40 0.31 0.31 0.36 0.31 0.22 0.40 0.25 0.25 0.20 0.09 0.00 0.00 0.00 0.28\n 0.53 0.31 0.31 0.36 0.31 0.22 0.53 0.25 0.25 0.20 0.09 0.00 0.00 0.00 0.28\n 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.09 0.64 0.00 0.00 0.00 0.09\n                                                                           \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n                                                                  \n 0.00                                                             \n 0.00 0.31                                                        \n 0.91 0.00 0.00                                                   \n 0.00 0.55 0.31 0.00                                              \n 0.00 0.31 0.81 0.00 0.31                                         \n 0.00 0.31 0.97 0.00 0.31 0.81                                    \n 0.00 0.28 0.28 0.00 0.28 0.28 0.28                               \n 0.00 0.31 0.94 0.00 0.31 0.81 0.94 0.28                          \n 0.00 0.20 0.20 0.00 0.20 0.20 0.20 0.20 0.20                     \n 0.11 0.00 0.00 0.11 0.00 0.00 0.00 0.00 0.00 0.00                \n 0.00 0.55 0.31 0.00 0.63 0.31 0.31 0.28 0.31 0.20 0.00           \n 0.00 0.40 0.31 0.00 0.40 0.31 0.31 0.28 0.31 0.20 0.00 0.40      \n 0.00 0.09 0.09 0.00 0.09 0.09 0.09 0.09 0.09 0.09 0.00 0.09 0.09 \n                                                                  \nNumber of obs: 1729, groups:  Site, 17; Species, 152; dummy, 1\n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   1.6634     1.3865   1.200  0.23024   \nsSeedMass    -0.3205     0.1166  -2.748  0.00599 **\nsSeedShape   -0.3606     0.1331  -2.709  0.00674 **\nsSeedN       -0.2142     0.1144  -1.872  0.06119 . \nsAltitude    -0.6441     0.3159  -2.039  0.04144 * \nsHum          0.1031     0.3041   0.339  0.73450   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDispersion model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.03941    0.03744  27.759  &lt; 2e-16 ***\nsAltitude   -0.24168    0.04828  -5.006 5.57e-07 ***\nsHum        -0.01986    0.05260  -0.378    0.706    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsimulateResiduals(model5, plot = TRUE) # Unconditional simulations\n\n\n\n\nObject of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. \n \nScaled residual values: 0.836 0.808 0.916 0.956 0.848 0.852 0.252 0.296 0.264 0.256 0.248 0.264 0.24 0.244 0.232 0.184 0.216 0.288 0.796 0.796 ...\n\n\n\n\n\n\n\n\n\n\n\nSolution for SBPA (glmm)\n\n\n\n\n\nPrepare+scale data:\n\ndata = as.data.frame(EcoData::seedBank)\ndata$sAltitude = scale(data$Altitude)\ndata$sSeedMass = scale(data$SeedMass)\ndata$sSeedShape = scale(data$SeedShape)\ndata$sSeedN = scale(data$SeedN)\ndata$sSeedPr = scale(data$SeedPr)\ndata$sDormRank = scale(data$DormRank)\ndata$sTemp = scale(data$Temp)\ndata$sHum = scale(data$Humidity)\ndata$sNitro = scale(data$Nitrogen)\ndata$sGrazing = scale(data$Grazing)\ndata$sMGT = scale(data$MGT)\ndata$sJwidth = scale(data$Jwidth)\ndata$sEpiStein = scale(data$EpiStein)\ndata$sMGR = scale(data$MGR)\ndata$sT95 = scale(data$T95)\n\n# Let's remove NAs beforehand:\nrows = rownames(model.matrix(SBDensity~sAltitude + sSeedMass + sSeedShape + sSeedN +\n                               sSeedPr + sDormRank + sTemp + sHum + sNitro + sMGT + \n                               sMGR + sEpiStein + sT95 +\n                               sJwidth + sGrazing + Site + Species, data = data))\ndata = data[rows, ]\n\n\nmodel1 = glmer(SBPA~\n                sSeedMass + sSeedShape + sSeedN + \n                 sAltitude + sHum +\n                (1|Site) + (sAltitude|Species),\n              data = data, family = binomial())\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.0108795 (tol = 0.002, component 1)\n\nsummary(model1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: SBPA ~ sSeedMass + sSeedShape + sSeedN + sAltitude + sHum + (1 |  \n    Site) + (sAltitude | Species)\n   Data: data\n\n     AIC      BIC   logLik deviance df.resid \n  1368.9   1423.4   -674.4   1348.9     1719 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5936 -0.2658 -0.1017  0.2001  3.3351 \n\nRandom effects:\n Groups  Name        Variance Std.Dev. Corr \n Species (Intercept) 8.16305  2.8571        \n         sAltitude   4.39255  2.0958   -0.13\n Site    (Intercept) 0.06796  0.2607        \nNumber of obs: 1729, groups:  Species, 152; Site, 17\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.6286     0.4066  -6.465 1.01e-10 ***\nsSeedMass    -1.3671     0.6432  -2.126   0.0335 *  \nsSeedShape   -0.5806     0.3099  -1.873   0.0610 .  \nsSeedN       -2.1362     1.3165  -1.623   0.1047    \nsAltitude    -1.3930     0.3365  -4.140 3.47e-05 ***\nsHum         -0.1021     0.1308  -0.781   0.4349    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) sSdMss sSdShp sSeedN sAlttd\nsSeedMass   0.221                            \nsSeedShape  0.033  0.062                     \nsSeedN      0.336  0.061 -0.066              \nsAltitude   0.132  0.142  0.079  0.012       \nsHum        0.023  0.002 -0.003 -0.011  0.229\noptimizer (Nelder_Mead) convergence code: 0 (OK)\nModel failed to converge with max|grad| = 0.0108795 (tol = 0.002, component 1)\n\n\nModel did not converge, but there is a trick which often helps. The default optimizer in lme4 is not the best optimizer, changing it to ‘bobyqa’ often helps with convergence issues\n\nmodel1 = glmer(SBPA~\n                sSeedMass + sSeedShape + sSeedN + \n                sAltitude + sHum +\n                (1|Site) + (sAltitude|Species),\n              data = data, family = binomial(),\n              control = glmerControl('bobyqa'))\nsummary(model1)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: SBPA ~ sSeedMass + sSeedShape + sSeedN + sAltitude + sHum + (1 |  \n    Site) + (sAltitude | Species)\n   Data: data\nControl: glmerControl(\"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n  1368.9   1423.4   -674.4   1348.9     1719 \n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-2.5938 -0.2658 -0.1018  0.2001  3.3353 \n\nRandom effects:\n Groups  Name        Variance Std.Dev. Corr \n Species (Intercept) 8.16321  2.8571        \n         sAltitude   4.38183  2.0933   -0.13\n Site    (Intercept) 0.06786  0.2605        \nNumber of obs: 1729, groups:  Species, 152; Site, 17\n\nFixed effects:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.6284     0.4066  -6.465 1.01e-10 ***\nsSeedMass    -1.3693     0.6438  -2.127   0.0334 *  \nsSeedShape   -0.5816     0.3100  -1.876   0.0606 .  \nsSeedN       -2.1420     1.3180  -1.625   0.1041    \nsAltitude    -1.3920     0.3362  -4.141 3.46e-05 ***\nsHum         -0.1022     0.1307  -0.781   0.4345    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) sSdMss sSdShp sSeedN sAlttd\nsSeedMass   0.222                            \nsSeedShape  0.033  0.062                     \nsSeedN      0.337  0.061 -0.066              \nsAltitude   0.132  0.142  0.079  0.013       \nsHum        0.023  0.002 -0.003 -0.011  0.229\n\n\nSuccess, it converged!\nResidual checks:\nCheck residuals:\n\nres = simulateResiduals(model1, re.form=NULL, plot=TRUE)\n\n\n\n\nResiduals look good!\nBonus: With phylogenetic correlation structure:\n\ndist_phylo = ape::cophenetic.phylo(phyl.upd2) # create distance matrix\ncorrelation_matrix = vcv(phyl.upd2)[unique(data$Species), unique(data$Species)]\n\n###\n#the following code was taken from https://github.com/glmmTMB/glmmTMB/blob/master/misc/fixcorr.rmd\nas.theta.vcov &lt;- function(Sigma,corrs.only=FALSE) {\n    logsd &lt;- log(diag(Sigma))/2\n    cr &lt;- cov2cor(Sigma)\n    cc &lt;- chol(cr)\n    cc &lt;- cc %*% diag(1 / diag(cc))\n    corrs &lt;- cc[upper.tri(cc)]\n    if (corrs.only) return(corrs)\n    ret &lt;- c(logsd,corrs)\n    return(ret)\n}\ncorrs = as.theta.vcov(correlation_matrix, corrs.only=TRUE)\n#####\n\ndata$dummy = factor(rep(0, nrow(data)))\nnsp = length(unique(data$Species))\nmodel6 = glmmTMB(SBPA~\n                sSeedMass + sSeedShape + sSeedN + \n                sAltitude + sHum +\n                (1|Site) + (sAltitude|Species) +\n                (1+Species|dummy),\n              map=list(theta=factor(c(rep(0, 4), rep(1,nsp),rep(NA,length(corrs))) )),\n              start=list(theta=c(rep(0, 4), rep(0,nsp),corrs)),\n              family = binomial,\n              data = data)\n\nWarning: '.T2Cmat' is deprecated.\nUse '.T2CR' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\nsimulateResiduals(model6, plot = TRUE)\n\n\n\n\nObject of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. \n \nScaled residual values: 0.5024676 0.5329717 0.5854716 0.9968902 0.7858493 0.5647619 0.1162222 0.1334808 0.1071773 0.3725763 0.2077203 0.3276229 0.02545432 0.06809947 0.3315729 0.3429219 0.6135937 0.4816801 0.6575116 0.8497154 ...\n\n\nConditional simulations:\n\npred = predict(model6, re.form = NULL, type = \"response\")\nsimulations = sapply(1:1000, function(i) rbinom(length(pred),1, pred))\nres = createDHARMa(simulations, model.frame(model6)[,1], pred)\nplot(res)"
  },
  {
    "objectID": "6C-CaseStudies.html#snouter",
    "href": "6C-CaseStudies.html#snouter",
    "title": "Appendix C — Case studies",
    "section": "C.7 Snouter",
    "text": "C.7 Snouter\nFit one of the responses in the snouter datset against the predictors rain + djungle (see ?snouter). Check for spatial autocorrelation and proceed to fitting a spatial model if needed. See the data set’s help for details on the variables.\n\nlibrary(EcoData)\nstr(snouter)\n\n\n\n\n\n\n\nSolution"
  },
  {
    "objectID": "1A-GettingStarted.html#your-r-system",
    "href": "1A-GettingStarted.html#your-r-system",
    "title": "1  Getting Started",
    "section": "1.2 Your R System",
    "text": "1.2 Your R System\nIn this course, we work with the combination of R + RStudio.\n\nR is the calculation engine that performs the computations.\nRStudio is the editor that helps you sending inputs to R and collect outputs.\n\nMake sure you have a recent version of R + RStudio installed on your computer. If you have never used RStudio, here is a good video introducing the basic system and how R and RStudio interact."
  },
  {
    "objectID": "3B-CausalInference.html#correlations-causality",
    "href": "3B-CausalInference.html#correlations-causality",
    "title": "6  Causal inference",
    "section": "6.1 Correlations != Causality",
    "text": "6.1 Correlations != Causality\nThe most fundamental distinction in strategies for model choice is if we want to estimate effects, or if we want to predict. If we want to estimate effects, we nearly always want to estimate causal effects.\nLet me first define what we mean by “a causal effect”: if we have a system with a number of variables, the causal effect of A on B is the change in B that would happen if we changed A, but kept all other aspects of this system constant.\nAssume we look at the effect of coffee consumption on Lung Cancer. Assume further that there is no such effect. However, there is an effect of smoking on lung cancer, and for some reason, smoking also affects coffee consumption.\nIt is common to visualize and analyze such relationships in a causal graph. Here, I use the ggdag package\n\nlibrary(ggdag)\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nlibrary(ggplot2)\ntheme_set(theme_dag())\ndag = confounder_triangle(x = \"Coffee\", y = \"Lung Cancer\", z = \"Smoking\") \nggdag(dag, text = FALSE, use_labels = \"label\")\n\n\n\n\nWe can use the ggdag package (or to be more exact: the underlying daggitty package) to explore the implications of this graph. The way I created the graph already includes the assumption that we are interested in the effect of Coffee on Lung Cancer. I can use the function ggdag_dconnected() to explore if those two are d-connected, which is just a fancy word for correlated.\n\nggdag_dconnected(dag, text = FALSE, use_labels = \"label\")\n\n\n\n\nIn this case, the plot highlights me to the fact that Coffee and Lung cancer d-connected. What that means is: if I plot coffee against lung cancer, I will see a correlation between them, even though coffee consumption does not influence lung cancer at all. We can easily confirm via a simulation that this is true:\n\nsmoking <- runif(50)\nCoffee <- smoking + rnorm(50, sd = 0.2)\nLungCancer <- smoking + rnorm(50, sd =0.2)\nfit <- lm(LungCancer ~ Coffee)\nplot(LungCancer ~ Coffee)\nabline(fit)\n\n\n\n\nSo, if we would fit a regression of LungCancer ~ Coffee, we would at least conclude that there is a correlation between the two variables. Many people would even go a step further, and conclude that Coffee consumption affects lung cancer. This is a classical misinterpretation created by the confounder smoking. Realizing this, I could ask the ggdag_dconnected function what would happen if I control for the effect of smoking.\n\nggdag_dconnected(dag, text = FALSE, use_labels = \"label\", controlling_for = \"z\")\n\n\n\n\nThe result conforms with our intuition - once we control for smoking, there will be no correlation between Coffee and Lung Cancer (because there is no causal effect between them). In causal lingo, we say the two are now d-separated."
  },
  {
    "objectID": "3B-CausalInference.html#double-ml",
    "href": "3B-CausalInference.html#double-ml",
    "title": "6  Causal inference",
    "section": "6.5 Double ML",
    "text": "6.5 Double ML\nThere is another trick to remove confounders, which is often used in machine learning. Consider the following example, where we have a main effect x->y, and a confounder c. \n\nc = runif(500) # confounder\nx = 0.5 * c + 0.5 *runif(500) # main predictor\ny = 1.2* x + c + rnorm(500, sd = 0.2)\n\nA multiple regression identifies the correct effect for x.\n\nsummary(lm(y ~ x + c ))\n\n\nCall:\nlm(formula = y ~ x + c)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5785 -0.1453 -0.0037  0.1263  0.6079 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.04007    0.02480   1.616    0.107    \nx            1.07328    0.06709  15.998   <2e-16 ***\nc            1.01756    0.04692  21.688   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2102 on 497 degrees of freedom\nMultiple R-squared:  0.8316,    Adjusted R-squared:  0.831 \nF-statistic:  1227 on 2 and 497 DF,  p-value: < 2.2e-16\n\n\nAnother way to get those effects is the following, where we fit all variables against the confounder. This essentially conditions on the confounder. If we then regress the residuals without the confounder, we get the orignal effect.\n\nresX.C = resid(lm(x~c))\nresY.C = resid(lm(y~c))\n\nsummary(lm(resY.C ~ resX.C))\n\n\nCall:\nlm(formula = resY.C ~ resX.C)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5785 -0.1453 -0.0037  0.1263  0.6079 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.119e-18  9.390e-03    0.00        1    \nresX.C       1.073e+00  6.702e-02   16.02   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.21 on 498 degrees of freedom\nMultiple R-squared:  0.3399,    Adjusted R-squared:  0.3386 \nF-statistic: 256.5 on 1 and 498 DF,  p-value: < 2.2e-16\n\n\nYou can also do this with more variables."
  },
  {
    "objectID": "4A-GLMs.html#introduction-to-glms",
    "href": "4A-GLMs.html#introduction-to-glms",
    "title": "8  GL(M)Ms",
    "section": "8.1 Introduction to GLMs",
    "text": "8.1 Introduction to GLMs\nGeneralized linear models (GLMs) extend the linear model (LM) to other (i.e. non-normal) distributions. The idea is the following:\n\nWe want to keep the regression formula \\(y \\sim f(x)\\) of the lm with all it’s syntax, inlcuding splines, random effects and all that. In the GLM world, this is called the “linear predictor”.\nHowever, in the GLM, we now allow other residual distributions than normal, e.g. binomial, Poisson, Gamma, etc.\nSome families require a particular range of the predictions (e.g. binomial requires predictions between zero and one). To achieve this, we use a so-called link function to bring the results of the linear predictor to the desired range.\n\nIn R, the distribution is specified via the family() function, which distinguishes the glm from the lm function. If you look at the help of family, you will see that the link function is an argument of family(). If no link is provided, the default link for the respective family is chosen.\n\n?family\n\nA full GLM structure is thus:\n\\[\ny \\sim family[link^{-1}(f(x) + RE)]\n\\]\n\n\n\n\n\n\nNote\n\n\n\nAs you see, in noted the link function as \\(link^{-1}\\) in this equation. The reason is that traditionally, the link function is applied to the left hand side of the equation, i.e.\n\\[\nlink(y) \\sim x\n\\]\nThis is important to keep in mind when interpreting the names of the link function - the log link, for example, means that \\(log(y) = x\\), which actually means that we assume \\(y = exp(x)\\), i.e. the result of the linear predictor enters the exponential function, which assures that we have strictly positive predictions.\n\n\nThe function \\(f(x)\\) itself can have all components that we discussed before, in particular\n\nYou can add random effects as before (using functions lme4::glmer or glmmTMB::glmmTMB)\nYou can also use splines using mgcv::gam"
  },
  {
    "objectID": "4C-CorrelationStructures.html#general-idea",
    "href": "4C-CorrelationStructures.html#general-idea",
    "title": "10  Correlation structures",
    "section": "10.1 General Idea",
    "text": "10.1 General Idea\nExcept for the random effects, we have so far assumed that residual errors are independent. However, that must not always be the case - we may find that residuals show autocorrelation.\n\n\n\n\n\n\nNote\n\n\n\nCorrelation means that one variable correlates with another. Autocorrelation means that data points of one variable that are close to each other have similar values. This implies that autocorrelation is only defined if there is a “distance relationship” between observations.\n\n\nAutocorrelation can always occur if we have a distance relationship between observations. Apart from random effects, where distance is expressed by group, common examples of continuous distance relationships include:\n\nRandom effects (distance = group)\nSpatial distance.\nTemporal distance.\nPhylogenetic distance.\n\nHere a visualization from Roberts et al., 2016 (reproduced as OA, copyright: the authors).\n\n\n\n\n\n\n10.1.1 Models to deal with autocorrelation\nIf we find autocorrelation in the residuals of our model, there can be several reasons, which we can address by different structures.\n\n\n\n\n\n\nImportant\n\n\n\nIn the context of regression models, we are never interested in the autocorrelation of the response / predictors per se, but only in the residuals. Thus, it doesn’t make sense to assume that you need a spatial model only because you have a spatially autocorrelated signal.\n\n\n\nAutocorrelation can occur because we have a spatially correlated misfit, i.e. there is a trend in the given space (e.g. time, space, phylogeny). If this is the case, de-trending the model (with a linear regression term or a spline) will remove the residual autocorrelation. We should always de-trend first because we consider moving to a model with a residual correlation structure.\nOnly after accounting for the trend, we should test if there is a residual spatial / temporal / phylogenetic autocorrelation. If that is the case, we would usually use a so-called conditional autoregressive (CAR) structures. In these models, we make parametric assumptions for how the correlation between data points falls off with distance. When we speak about spatial / temporal / phylogenetic regressions, we usually refer to these or similar models.\n\n\n\n10.1.2 R implementation\nTo de-trend, you can just use standard regression terms or splines on time or space. For the rest of this chapter, we will concentrate on how to specify “real” correlation structures. However, in the case studies, you should always de-trend first.\nTo account for “real” autocorrelation of residuals, similar as for the variance modelling, we can add correlation structures\n\nfor normal responses in nlme::gls, see https://stat.ethz.ch/R-manual/R-devel/library/nlme/html/corClasses.html\nfor GLMs using glmmTMB, see https://cran.r-project.org/web/packages/glmmTMB/vignettes/covstruct.html.\n\nThe following pages provide examples and further comments on how to do this.\n\n\n\n\n\n\nNote\n\n\n\nEspecially for spatial models, both nlme and glmmTMB are relatively slow. Therea are a large number of specialized packages that deal in particular with the problem of spatial models, including MASS::glmmPQL, BRMS, INLA, spaMM, and many more. To keep things simple and concentrate on the principles, however, we will stick with the packages you already know."
  },
  {
    "objectID": "6B-CheatSheet.html#formula-syntax",
    "href": "6B-CheatSheet.html#formula-syntax",
    "title": "Appendix B — Cheat sheet",
    "section": "B.1 Formula syntax",
    "text": "B.1 Formula syntax\n\n\nTable B.1: Formula syntax\n\n\n\n\n\n\nFormula\nInterpretation\n\n\n\n\ny ~ x\nIntercept + slope\n\n\ny ~ 1\nintercept only\n\n\ny ~ x - 1 or x + 0\nonly slope, for categorical\n\n\nlog(x) ~ sqrt(x)\npredictor and response variable can be transformed\n\n\ny ~ x1 + x2\nmultiple predictors can be added\n\n\ny ~ x1 * x2\ninteraction and main effects\n\n\ny ~ x1:x2\nonly interaction\n\n\ny ~ (x1 + x2)^2\nall 2nd order interactions, works also with 3rd order ^3 etc\n\n\ny ~ I(x^2)\nI() forces mathematical interpretation of whatever is in the parenthesis\n\n\ny ~ s(x)\nspline (works only for mgcv)\n\n\ny ~ te(x1, x2)\ntesnsor spline (works only for mgcv)\n\n\ny ~.\ninclude all variables as main effects\n\n\ny~.^2\ninclude all variables as main effects and their 2nd order interactions"
  },
  {
    "objectID": "2A-LinearRegression.html#fitting-a-simple-linear-regression",
    "href": "2A-LinearRegression.html#fitting-a-simple-linear-regression",
    "title": "2  Linear Regression",
    "section": "2.1 Fitting a simple linear regression",
    "text": "2.1 Fitting a simple linear regression\nLet’s start with the basics. For this chapter, we will rely a lot on the airquality data, which is one of the built-in datasets in R.\n\n\n\n\n\n\nMore info on the airquality data\n\n\n\n\n\nMost datasets in R have a help file. Thus, if you don’t know the data set, have a look at the description via pressing F1 with the curses on the word, or via typing\n\n?airquality\n\nAs you can read, the datasets comprises daily air quality measurements in New York, May to September 1973. You can see the structure of the data via\n\nstr(airquality)\n\nVariables are described in the help. Note that Month / Day are currently coded as integers, would be better coded as date or (ordered) factors (something you could do later, if you use this dataset).\n\n\n\nLet’s say we want to examine the relationship between Ozone and Wind in these data.\n\nplot(Ozone ~ Wind, data = airquality)\n\n\n\n\nOK, I would say there is some dependency here. To quantify this numerically, we want to fit a linear regression model through the data with the lm() function of R. We can do this in R by typing\n\nfit = lm(Ozone ~ Wind, data = airquality)\n\nThis command creates a linear regression model that fits a straight line, adjusting slope and intercept to get the best fit through the data. We can see the fitted coefficients if we print the model object\n\nfit\n\n\nCall:\nlm(formula = Ozone ~ Wind, data = airquality)\n\nCoefficients:\n(Intercept)         Wind  \n     96.873       -5.551  \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function name lm is short for “linear model”. Remember from your basic stats course: This model is not called linear because we necessarily fit a linear function. It’s called linear as long as we express the response (in our case Ozone) as a polynomial of the predictor(s). A polynomial ensures that when estimating the unknown parameters (we call them “effects”), they all affect predictions linearly, and can thus be solved as a system of linear equations.\nSo, an lm is any regression of the form \\(y = \\operatorname{f}(x) + \\mathcal{N}(0, \\sigma)\\), where \\(\\operatorname{f}\\) is a polynomial, e.g. \\({a}_{0} + {a}_{1} \\cdot x + {a}_{2} \\cdot {x}^{2}\\), and \\(\\mathcal{N}(0, \\sigma)\\) means that we assume the data scattering as a normal (Gaussian) distribution with unknown standard deviation \\(\\sigma\\) around \\(\\operatorname{f}(x)\\).\n\n\n\n\n\n\n\n\nUnderstanding how the lm works\n\n\n\n\n\nTo understand how the lm works, we will estimate a lm using a custom likelihood function:\n\nCreate some toy data:\n\n\nXobs = rnorm(100, sd = 1.0)\nY = 0.0+1.0*Xobs + rnorm(100,sd = 0.2)\n\n\nDefine the likelihood function and optimize the parameters (slope+intercept):\n\n\nlikelihood = function(par) { # three parameters now\n  lls = dnorm(Y, mean = Xobs*par[2]+par[1], sd = par[3], log = TRUE) \n  # calculate for each observation the probability to observe the data given our model\n  # we use the logLikilihood because of numerical reasons\n  return(sum(lls))\n}\n\nlikelihood(c(0, 0, 0.2))\n\n[1] -1328.254\n\nopt = optim(c(0.0, 0.0, 1.0), fn = function(par) -likelihood(par), hessian = TRUE )\n\nopt$par\n\n[1] -0.01770037  1.00544470  0.17984788\n\n\n\nTest the estimated parameters against 0 (standard errors can be derived from the hessian of the MLE): Our true parameters are 0.0 for the intercept, 1.0 for the slope, and 0.22 for the sd of the observational error:\n\ncalculate standard error\ncalculate t-statistic\ncalculate p-value\n\n\n\nst_errors = sqrt(diag(solve(opt$hessian)))\nst_errors[2]\n\n[1] 0.01735754\n\nt_statistic = opt$par[2] / st_errors[2]\npt(t_statistic, df = 100-3, lower.tail = FALSE)*2\n\n[1] 4.712439e-77\n\n\nThe p-value is smaller than \\(\\alpha\\), so the effect is significant! Let’s compare it against the output of the lm function:\n\nmodel = lm(Y~Xobs) # 1. Get estimates, MLE\nmodel\n\n\nCall:\nlm(formula = Y ~ Xobs)\n\nCoefficients:\n(Intercept)         Xobs  \n   -0.01767      1.00543  \n\nsummary(model) # 2. Calculate standard errors, CI, and p-values\n\n\nCall:\nlm(formula = Y ~ Xobs)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.37768 -0.10133  0.00245  0.10821  0.57248 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.01767    0.01818  -0.972    0.333    \nXobs         1.00543    0.01753  57.345   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1817 on 98 degrees of freedom\nMultiple R-squared:  0.9711,    Adjusted R-squared:  0.9708 \nF-statistic:  3288 on 1 and 98 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "2B-ANOVA.html#the-idea-of-anova",
    "href": "2B-ANOVA.html#the-idea-of-anova",
    "title": "3  ANOVA and other tests on the LM",
    "section": "3.1 The idea of ANOVA",
    "text": "3.1 The idea of ANOVA\nANOVA stands for ANalysis Of VAriance. The basic idea is to find out how much of the signal (variance) is explained by different factors. We had already short introduced ANOVA in the section on categorical predictors.\nThe problem with explaining ANOVA is that the term is overfraught with historical meanings and explanations that are no further relevant. It used to be that ANOVA is a stand-alone method that you use for experimental designs with different treatments, that ANOVA assumes normal distribution and partitions sum of squares, that there are repeated-measure ANOVAS and all that, and those varieties of ANOVA partly still exist, but in general, there is a much simpler and general explanation of ANOVA:\nModern explanation: ANOVA is not a statistical model, but a hypothesis test that can be performed on top of any regression model. What this test is doing is to measure how much model fit improves when a predictor is added, and if this improvement is significant.\n\n3.1.1 An example\nAs an example, here is a ANOVA (function aov()) performed on the fit of a linear model\n\nfit = lm(Ozone ~ Wind + Temp, data = airquality)\nsummary(aov(fit))\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nWind          1  45284   45284   94.81  &lt; 2e-16 ***\nTemp          1  25886   25886   54.20 3.15e-11 ***\nResiduals   113  53973     478                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n37 observations deleted due to missingness\n\n\nIn the standard ANOVA for the LM, model fit is measured by the reduction in residual sum of squares. Let’s look at the example above. In the ANOVA table above, we (virtually) start with an intercept only model. Now, what the table tells us is that adding Wind to the model reduces the Sum Sq. by 45284, and adding also Temp reduces the Sum Sq by another 25886, which leaves us with 53973 residual sum sq. From this, we can also conclude that the total variance of the response is 53973 + 25886 + 45284 = 125143. Let’s check this:\n\nsum((fit$model$Ozone - mean(fit$model$Ozone))^2)\n\n[1] 125143.1\n\n\nThus, we can conclude that the R2 explained by each model component is\n\n53973/125143 # Wind\n\n[1] 0.4312906\n\n25886/125143 # Temp\n\n[1] 0.2068514\n\n45284/125143 # Residual\n\n[1] 0.361858\n\n\nMoreover, the ANOVA table performs tests to see if the improvement of model fit is significant against a null model. This is important because, as mentioned before (particular in the chapter on model selection), adding a predictor always improves model fit.\nTo interpret the p-values, consider that H0 = the simpler model is true, thus we test if the improvement of model fit is higher than what we would expect if the predictor has no effect.\n\n\n3.1.2 Customizing the partitioning\nThere are a number of cases where it can make sense to perform an ANOVA for larger parts of the model. Consider, for example, the following regression:\n\nfit = lm(Ozone ~ Wind + I(Wind^2) + Temp + I(Temp^2), data = airquality)\n\nMaybe, we would like to ask how much variance is explained by Wind + Wind^2, and how much by Temp + Temp^2. In this case, we can perform custom ANOVA, using the anova() function that we already introduced in the section on model selection via likelihood ratio tests (LRTs).\n\nm0 = lm(Ozone ~ 1, data = airquality)\nm1 = lm(Ozone ~ Wind + I(Wind^2) , data = airquality)\nm2 = lm(Ozone ~ Wind + I(Wind^2) + Temp + I(Temp^2), data = airquality)\nanova(m0, m1, m2)\n\nAnalysis of Variance Table\n\nModel 1: Ozone ~ 1\nModel 2: Ozone ~ Wind + I(Wind^2)\nModel 3: Ozone ~ Wind + I(Wind^2) + Temp + I(Temp^2)\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    115 125143                                  \n2    113  64360  2     60783 81.397 &lt; 2.2e-16 ***\n3    111  41445  2     22915 30.686 2.464e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "2B-ANOVA.html#relationship-between-lm-and-other-hypotheses-tests",
    "href": "2B-ANOVA.html#relationship-between-lm-and-other-hypotheses-tests",
    "title": "3  ANOVA and other tests on the LM",
    "section": "3.3 Relationship between lm and other hypotheses tests",
    "text": "3.3 Relationship between lm and other hypotheses tests\nANOVA is just one example of the close relationship between the lm and other null hypothesis significance tests. ANOVA can either be seen as a special test (traditional view), or as a special evaluation of a fitted lm (modern view). I would recommend taking the modern view, as it easily generalizes to GLMs and other more advanced regression models such as GLMMs.\nIt is interesting to consider more generally which hypothesis tests can be obtained by looking at certain results or tests on the linear model. As you can see in the table below, there are many classical hypothesis tests that are equivalently or nearly equivalently tested by a linear model.\n\nCommon statistical tests with linear models\n\n\n\n\n\n\n\nHypothesis\nR function\nEquivalent linear model\n\n\n\n\nTwo groups differ in their means\nunpaired t.test\nt.test(y_1, y_2)\nlm(c(y_1, y_2)~c(g_1, g_2))\n\n\nTwo groups differ in their means\npaired t.test\nt.test(y_1, y_2, paired = TRUE)\nlm(y_2-y_1~1)\n\n\ny depends on continuous x\nPearson correlation\ncor.test(x, y)\nlm(y~x) note: only near-equivalent\n\n\ny depends on continuous x\nSpearman correlation\ncor.test(x, y, method = \"spearman\")\nlm(rank(y)~rank(x)) note: only near-equivalent.\n\n\ny differs within a group\nOne-way ANOVA\naov(y~group)\nlm(y~group) with subsequent ANOVA\n\n\n\nBased on this, one could think it doesn’t matter then if we use a regression model or a hypothesis test. However, the advantage of the regression model is that\n\nIt provides a richer output\nIt provides the same output for all cases, with clear ideas about how to check residuals etc.\nIt allows to adjust for the effects of covariates"
  },
  {
    "objectID": "3C-ModelSelection.html#the-bias-variance-trade-off",
    "href": "3C-ModelSelection.html#the-bias-variance-trade-off",
    "title": "7  Model selection",
    "section": "7.1 The Bias-Variance Trade-off",
    "text": "7.1 The Bias-Variance Trade-off\nApart from causality, the arguably most fundamental idea about modelling choice is the bias-variance trade-off, which applies regardless of whether we are interested in causal effects or predictive models. The idea is the following:\n\nThe more variables / complexity we include in the model, the better it can (in principle) adjust to the true relationship, thus reducing model error from bias.\nThe more variables / complexity we include in the model, the larger our error (variance) on the fitted coefficients, thus increasing model error from variance. This means, the model adopts to the given data but no longer to the underlying relationship.\n\nIf we sum both terms up, we see that at the total error of a model that is too simple will be dominated by bias (underfitting), and the total error of a model that is too complex will be dominated by variance (overfitting):\n\n\n\n\n\nLet’s confirm this with a small simulation - let’s assume I have a simple relationship between x and y:\n\nset.seed(123)\n\nx = runif(100)\ny = 0.25 * x + rnorm(100, sd = 0.3)\n\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.67139 -0.18397 -0.00592  0.17890  0.66517 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept) -0.002688   0.058816  -0.046    0.964  \nx            0.223051   0.102546   2.175    0.032 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2908 on 98 degrees of freedom\nMultiple R-squared:  0.04605,   Adjusted R-squared:  0.03632 \nF-statistic: 4.731 on 1 and 98 DF,  p-value: 0.03203\n\n\nAs you see, the effect is significant. Now, I add 80 new variables to the model which are just noise\n\nxNoise = matrix(runif(8000), ncol = 80)\ndat = data.frame(y=y,x=x, xNoise)\n\nfullModel = lm(y~., data = dat)\nsummary(fullModel)\n\n\nCall:\nlm(formula = y ~ ., data = dat)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.263130 -0.086990 -0.007075  0.095086  0.293818 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  0.818772   1.309581   0.625   0.5397  \nx            0.187245   0.275626   0.679   0.5056  \nX1           0.065985   0.288601   0.229   0.8217  \nX2          -0.205767   0.226155  -0.910   0.3749  \nX3          -0.241677   0.264493  -0.914   0.3729  \nX4          -0.112305   0.259382  -0.433   0.6702  \nX5          -0.007687   0.278844  -0.028   0.9783  \nX6           0.016095   0.194323   0.083   0.9349  \nX7           0.210212   0.288974   0.727   0.4763  \nX8           0.248545   0.361409   0.688   0.5004  \nX9          -0.197172   0.441156  -0.447   0.6602  \nX10          0.206384   0.246107   0.839   0.4127  \nX11         -0.253228   0.249239  -1.016   0.3231  \nX12         -0.146196   0.218184  -0.670   0.5113  \nX13         -0.191434   0.629232  -0.304   0.7644  \nX14          0.079965   0.261570   0.306   0.7633  \nX15         -0.340678   0.338473  -1.007   0.3275  \nX16         -0.565212   0.291535  -1.939   0.0684 .\nX17          0.179863   0.510105   0.353   0.7285  \nX18          0.102642   0.242366   0.424   0.6769  \nX19         -0.097458   0.286541  -0.340   0.7377  \nX20          0.030439   0.278389   0.109   0.9141  \nX21         -0.144404   0.378193  -0.382   0.7071  \nX22          0.099958   0.207603   0.481   0.6360  \nX23          0.261590   0.299691   0.873   0.3942  \nX24          0.307017   0.290396   1.057   0.3044  \nX25         -0.261906   0.273628  -0.957   0.3512  \nX26          0.348920   0.243841   1.431   0.1696  \nX27         -0.036098   0.228266  -0.158   0.8761  \nX28          0.287817   0.270679   1.063   0.3017  \nX29          0.033879   0.307206   0.110   0.9134  \nX30          0.062557   0.362398   0.173   0.8649  \nX31          0.172122   0.265290   0.649   0.5247  \nX32          0.105700   0.201368   0.525   0.6061  \nX33         -0.147496   0.252870  -0.583   0.5669  \nX34         -0.167456   0.263864  -0.635   0.5337  \nX35          0.100169   0.178501   0.561   0.5816  \nX36          0.088712   0.295791   0.300   0.7677  \nX37          0.216354   0.323429   0.669   0.5120  \nX38          0.291084   0.317911   0.916   0.3720  \nX39         -0.012332   0.240035  -0.051   0.9596  \nX40          0.243437   0.367832   0.662   0.5165  \nX41          0.380828   0.319131   1.193   0.2482  \nX42         -0.074005   0.253109  -0.292   0.7733  \nX43          0.098013   0.227937   0.430   0.6723  \nX44          0.198824   0.302047   0.658   0.5187  \nX45          0.199577   0.219516   0.909   0.3753  \nX46         -0.421600   0.369403  -1.141   0.2687  \nX47          0.154510   0.230492   0.670   0.5111  \nX48         -0.413987   0.262710  -1.576   0.1325  \nX49         -0.077130   0.228585  -0.337   0.7397  \nX50         -0.096674   0.347191  -0.278   0.7838  \nX51         -0.200855   0.236771  -0.848   0.4074  \nX52         -0.043446   0.242367  -0.179   0.8597  \nX53         -0.131982   0.369716  -0.357   0.7253  \nX54         -0.195446   0.361531  -0.541   0.5954  \nX55         -0.236722   0.256146  -0.924   0.3676  \nX56         -0.368766   0.265788  -1.387   0.1822  \nX57         -0.334641   0.257425  -1.300   0.2100  \nX58          0.231276   0.237317   0.975   0.3427  \nX59         -0.096390   0.273072  -0.353   0.7282  \nX60          0.311881   0.271568   1.148   0.2658  \nX61          0.242950   0.217372   1.118   0.2784  \nX62         -0.084825   0.442712  -0.192   0.8502  \nX63          0.029619   0.298468   0.099   0.9220  \nX64         -0.028104   0.261794  -0.107   0.9157  \nX65         -0.525963   0.361913  -1.453   0.1634  \nX66          0.144431   0.228127   0.633   0.5346  \nX67         -0.128939   0.258665  -0.498   0.6242  \nX68          0.112637   0.267554   0.421   0.6787  \nX69          0.348617   0.248531   1.403   0.1777  \nX70          0.006882   0.257210   0.027   0.9789  \nX71         -0.246016   0.316239  -0.778   0.4467  \nX72          0.320674   0.376736   0.851   0.4058  \nX73         -0.088619   0.248323  -0.357   0.7253  \nX74         -0.258652   0.245951  -1.052   0.3069  \nX75          0.304042   0.284942   1.067   0.3001  \nX76         -0.414025   0.290564  -1.425   0.1713  \nX77         -0.087306   0.272738  -0.320   0.7526  \nX78         -0.072697   0.250050  -0.291   0.7746  \nX79         -0.138113   0.280764  -0.492   0.6287  \nX80         -0.439736   0.259721  -1.693   0.1077  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3149 on 18 degrees of freedom\nMultiple R-squared:  0.7946,    Adjusted R-squared:  -0.1299 \nF-statistic: 0.8595 on 81 and 18 DF,  p-value: 0.6891\n\n\nThe effect estimates are relatively unchanged, but the CI has increased, and the p-values are n.s.\n\n\n\n\n\n\nPrecision variables\n\n\n\nWe have learned that adding variables increases the complexity of the model and thus the variance/uncertainty of the model. However, there are so-called precision variables that, when added to the model, actually reduce the variance/uncertainty of the model! Technically, they explain the response variable so well that they help to improve the overall fit of the model and thus help to reduce the overall variance of the model.\nHere’s a small example based on toy data:\n\nset.seed(42)\nX = runif(100)\nP = runif(100)\nY = 0.8*X + 10*P + rnorm(100, sd = 0.5)\n\nWithout the precision variable (P):\n\nsummary(lm(Y~X))\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.3935 -2.7579  0.3701  2.2876  4.8379 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   5.3465     0.5826   9.177 7.39e-15 ***\nX             0.4364     0.9639   0.453    0.652    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.896 on 98 degrees of freedom\nMultiple R-squared:  0.002088,  Adjusted R-squared:  -0.008095 \nF-statistic: 0.205 on 1 and 98 DF,  p-value: 0.6517\n\n\nEffect of X is n.s. (standard error = 0.96)\nWith the precision variable (P):\n\nsummary(lm(Y~X+P))\n\n\nCall:\nlm(formula = Y ~ X + P)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.97608 -0.28268  0.00618  0.27548  1.38684 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.04947    0.12646  -0.391    0.697    \nX            0.75999    0.15197   5.001 2.54e-06 ***\nP           10.05137    0.16200  62.045  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4563 on 97 degrees of freedom\nMultiple R-squared:  0.9755,    Adjusted R-squared:  0.975 \nF-statistic:  1929 on 2 and 97 DF,  p-value: < 2.2e-16\n\n\nThe effect of X on Y is now significant (standard error = 0.15)!"
  },
  {
    "objectID": "4A-GLMs.html#case-studies",
    "href": "4A-GLMs.html#case-studies",
    "title": "8  GL(M)Ms",
    "section": "8.6 Case Studies",
    "text": "8.6 Case Studies\n\n8.6.1 Binomial GLM - Snails\nThe following dataset contains information about the occurrences of 3 freshwater snail species and their infection rate with schistosomiasis (feshwater snails are intermediate hosts). The data and the analysis is from Rabone et al. 2019.\n\nlibrary(EcoData)\n?snails\n\nHelp on topic 'snails' was found in the following packages:\n\n  Package               Library\n  EcoData               /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n  MASS                  /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n\nUsing the first match ...\n\n\nThe dataset has data on three snail species: Biomphalaria pfeifferi (BP), Bulinus forskalii (BF), and Bulinus truncatus (BT). For this task, we will focus only on Bulinus truncatus (BT).\nWe want to answer two questions:\n\nWhat are the variables that explain the occurrence of the Bulinus truncatus species (=Species distribution model)\nWhat are the variables that influence the infection of the snails with Schistosomiasis\n\nTasks:\n\nBuild two binomial GLMs (see hints) to explain the presence and the infection rate of Bulinus truncatus (BT)\nAdd environmental variables and potential confounders (see ?snails for an overview of the variables and if you want, you can also take a loot at the original paper)\nCheck residuals with DHARMa, check for misfits (plot residuals against variables)\nWhich variables explain the presence of BT and which have an effect on the infection rates?\n\n\n\n\n\n\n\nHints\n\n\n\nPrepare dataset before the analysis:\n\ndata = EcoData::snails\ndata$sTemp_Water = scale(data$Temp_Water)\ndata$spH = scale(data$pH)\ndata$swater_speed_ms = scale(data$water_speed_ms)\ndata$swater_depth = scale(data$water_depth)\ndata$sCond = scale(data$Cond)\ndata$swmo_prec = scale(data$wmo_prec)\ndata$syear = scale(data$year)\ndata$sLat = scale(data$Latitude)\ndata$sLon = scale(data$Longitude)\ndata$sTemp_Air = scale(data$Temp_Air)\n# Remove NAs\nrows = rownames(model.matrix(~sTemp_Water + spH + sLat + sLon + sCond + seas_wmo+ swmo_prec + swater_speed_ms + swater_depth +sTemp_Air+ syear + duration + locality + site_irn + coll_date, data = data))\ndata = data[rows, ]\n\n\nFactors which drive the occurrence of BT\nMinimal model:\n\nmodel1 = glm(bt_pres~ sTemp_Water,\n             data = data,  family = binomial)\n\nAdd environmental variables such as Temperature, pH, water metrics (e.g. water speed etc). Which variables could be potential confounders (year? locality? Season?….)\nFactors which affect the infection rate\nWe use a k/n binomial model:\n\nmodel2 = glm(cbind(BT_pos_tot, BT_tot - BT_pos_tot )~ sTemp_Water ,\n                 data = data[data$BT_tot > 0, ],  family = binomial)\n\nAdd environmental variables such as Temperature, pH, water metrics (e.g. water speed etc). Which variables could be potential confounders (year? locality? Season?….)\n\n\n\n\n\n\n\n\n\nSolution - Species distribution model\n\n\n\n\n\nEnvironmental variables (part of our hypothesis): Temp_water, Temp_Air, pH, Cond, swmo_prec, water_speed_ms, and water_depth.\nPotential confounders: site_type, year, seas_wmo (season) (, and locality)\nOur model:\n\nmodel1 = glm(bt_pres~ site_type + sTemp_Water + spH +\n               sCond + swmo_prec + swater_speed_ms  + duration + \n               sTemp_Air + seas_wmo + syear + swater_depth,\n               data = data,  family = binomial)\nsummary(model1)\n\n\nCall:\nglm(formula = bt_pres ~ site_type + sTemp_Water + spH + sCond + \n    swmo_prec + swater_speed_ms + duration + sTemp_Air + seas_wmo + \n    syear + swater_depth, family = binomial, data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.3275  -1.0924   0.6059   0.9616   2.4113  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        0.815224   0.194147   4.199 2.68e-05 ***\nsite_typecanal.3  -0.332009   0.121838  -2.725  0.00643 ** \nsite_typepond     -1.256373   0.198187  -6.339 2.31e-10 ***\nsite_typerice.p   -2.549160   0.226259 -11.267  < 2e-16 ***\nsite_typeriver    -0.937624   0.178915  -5.241 1.60e-07 ***\nsite_typerivulet  -0.655550   0.214947  -3.050  0.00229 ** \nsite_typespillway -2.948800   0.410426  -7.185 6.73e-13 ***\nsTemp_Water       -0.147118   0.089085  -1.651  0.09865 .  \nspH               -0.058152   0.050195  -1.159  0.24665    \nsCond              0.226405   0.057109   3.964 7.36e-05 ***\nswmo_prec         -0.156915   0.059027  -2.658  0.00785 ** \nswater_speed_ms   -0.129785   0.054930  -2.363  0.01814 *  \nduration          -0.007267   0.006853  -1.060  0.28899    \nsTemp_Air         -0.207746   0.075345  -2.757  0.00583 ** \nseas_wmowet       -0.070866   0.134537  -0.527  0.59838    \nsyear             -0.487076   0.063083  -7.721 1.15e-14 ***\nswater_depth      -0.050282   0.051463  -0.977  0.32854    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2862.7  on 2071  degrees of freedom\nResidual deviance: 2469.6  on 2055  degrees of freedom\nAIC: 2503.6\n\nNumber of Fisher Scoring iterations: 4\n\n\nIntercept corresponds to seas_wmo == wet and site_type == can.2\nYear (our confounder) has a very strong effect on the occurrence rate. Conducitivity has a strong positive effect. Let’s look at the effects:\n\nplot(allEffects(model1))\n\n\n\n\nit appears that site_type and conductivity have the largest positive effects on occurrence rates. It is reasonable to assume that there are interactions between freshwater (site_type) and factors such as conductivity:\n\nmodel1 = glm(bt_pres~ site_type * sCond -sCond + swater_speed_ms+spH + sTemp_Water + duration + \n                 sTemp_Air + seas_wmo + syear + swater_depth,\n             data = data,  family = binomial)\nsummary(model1)\n\n\nCall:\nglm(formula = bt_pres ~ site_type * sCond - sCond + swater_speed_ms + \n    spH + sTemp_Water + duration + sTemp_Air + seas_wmo + syear + \n    swater_depth, family = binomial, data = data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4717  -1.1029   0.5995   0.9587   2.4166  \n\nCoefficients:\n                         Estimate Std. Error z value Pr(>|z|)    \n(Intercept)              0.859045   0.195491   4.394 1.11e-05 ***\nsite_typecanal.3        -0.341943   0.122316  -2.796 0.005181 ** \nsite_typepond           -1.445413   0.216706  -6.670 2.56e-11 ***\nsite_typerice.p         -2.731996   0.251334 -10.870  < 2e-16 ***\nsite_typeriver          -1.128968   0.238779  -4.728 2.27e-06 ***\nsite_typerivulet        -0.744423   0.288857  -2.577 0.009962 ** \nsite_typespillway       -2.699163   0.419229  -6.438 1.21e-10 ***\nswater_speed_ms         -0.132721   0.054953  -2.415 0.015727 *  \nspH                     -0.081288   0.050445  -1.611 0.107084    \nsTemp_Water             -0.166515   0.089683  -1.857 0.063353 .  \nduration                -0.008406   0.006896  -1.219 0.222867    \nsTemp_Air               -0.186726   0.075486  -2.474 0.013374 *  \nseas_wmowet             -0.089821   0.134777  -0.666 0.505130    \nsyear                   -0.478198   0.063467  -7.535 4.90e-14 ***\nswater_depth            -0.054116   0.051882  -1.043 0.296922    \nsite_typecan.2:sCond     0.285611   0.116639   2.449 0.014338 *  \nsite_typecanal.3:sCond  -0.040904   0.116426  -0.351 0.725342    \nsite_typepond:sCond      0.482954   0.133330   3.622 0.000292 ***\nsite_typerice.p:sCond    0.731702   0.205377   3.563 0.000367 ***\nsite_typeriver:sCond    -0.186359   0.348331  -0.535 0.592646    \nsite_typerivulet:sCond   0.069200   0.414818   0.167 0.867511    \nsite_typespillway:sCond  0.030526   0.148147   0.206 0.836751    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2862.7  on 2071  degrees of freedom\nResidual deviance: 2456.7  on 2050  degrees of freedom\nAIC: 2500.7\n\nNumber of Fisher Scoring iterations: 4\n\n\n“-sCond” allows us to test the slopes for the different site_types against 0 (no reference level)\nResidual checks:\n\nlibrary(DHARMa)\nsim = simulateResiduals(model1, plot = TRUE)\n\n\n\n\nCheck misfits by plotting predictors against residuals:\n\nplotResiduals(sim, data$sCond)\n\n\n\nplotResiduals(sim, data$sTemp_Air)\n\n\n\n\nThey look okay!\nBonus - Spatial autocorrelation:\nA common problem with spatial data is spatial autocorrelation. We can use DHARMa to test if our residuals are spatially autocorrelated\n\nres2 = recalculateResiduals(sim, group = c(data$site_irn))\ngroupLocations = aggregate(cbind(data$sLat, data$sLon ), list( data$site_irn), mean)\ntestSpatialAutocorrelation(res2, x = groupLocations$V1, y = groupLocations$V2)\n\n\n\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res2\nobserved = 0.242175, expected = -0.011364, sd = 0.067256, p-value =\n0.0001634\nalternative hypothesis: Distance-based autocorrelation\n\n\nThe test is significant, i.e. we should assume that the data is spatially autocorrelated. Addressing this issue, however, is part of the Advanced course\n\n\n\n\n\n\n\n\n\nSolution - Infection rate\n\n\n\n\n\nEnvironmental variables (part of our hypothesis): Temp_water, Temp_Air, pH, Cond, swmo_prec, water_speed_ms, and water_depth.\nPotential confounders: site_type, year, seas_wmo (season), duration, ( and locality)\nOur model:\n\ndata_inf = data[data$BT_tot > 0 , ] # only observations \n\nmodel2 = glm(cbind(BT_pos_tot, BT_tot - BT_pos_tot )~ site_type + sTemp_Water + spH +\n               sCond + log(swmo_prec+1) + swater_speed_ms  + log(duration) + \n               sTemp_Air + seas_wmo + syear + swater_depth,\n               data = data_inf,  family = binomial)\nsummary(model2)\n\n\nCall:\nglm(formula = cbind(BT_pos_tot, BT_tot - BT_pos_tot) ~ site_type + \n    sTemp_Water + spH + sCond + log(swmo_prec + 1) + swater_speed_ms + \n    log(duration) + sTemp_Air + seas_wmo + syear + swater_depth, \n    family = binomial, data = data_inf)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-4.1600  -0.6178  -0.3103  -0.1493   8.6079  \n\nCoefficients:\n                   Estimate Std. Error z value Pr(>|z|)    \n(Intercept)        -2.84336    0.52079  -5.460 4.77e-08 ***\nsite_typecanal.3    0.06733    0.14076   0.478 0.632399    \nsite_typepond       2.42003    0.20151  12.010  < 2e-16 ***\nsite_typerice.p     1.06829    0.39734   2.689 0.007175 ** \nsite_typeriver      0.11099    0.46215   0.240 0.810207    \nsite_typerivulet    1.16396    0.33047   3.522 0.000428 ***\nsite_typespillway   1.20405    1.02591   1.174 0.240542    \nsTemp_Water         0.23679    0.10178   2.327 0.019991 *  \nspH                 0.03519    0.06160   0.571 0.567794    \nsCond              -0.10822    0.05731  -1.888 0.059000 .  \nlog(swmo_prec + 1) -1.14756    0.33413  -3.434 0.000594 ***\nswater_speed_ms     0.09253    0.04145   2.232 0.025595 *  \nlog(duration)      -0.90117    0.17268  -5.219 1.80e-07 ***\nsTemp_Air          -0.29302    0.09338  -3.138 0.001702 ** \nseas_wmowet         0.17373    0.16907   1.028 0.304160    \nsyear              -0.11184    0.06612  -1.691 0.090746 .  \nswater_depth       -0.16272    0.06577  -2.474 0.013353 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1532.4  on 1106  degrees of freedom\nResidual deviance: 1264.9  on 1090  degrees of freedom\nAIC: 1597.1\n\nNumber of Fisher Scoring iterations: 6\n\n\nIntercept corresponds to seas_wmo == wet and site_type == can.2\nYear (our confounder) has a very strong effect on the occurrence rate. Syte_type, swmo_prec (Precipitation), sTemp_Water, and sTemp_Air have strong effects:\n\nplot(allEffects(model2))\n\n\n\n\nWith partial residuals:\n\nplot(allEffects(model2, partial.residuals = TRUE))\n\n\n\n\nReminder: The partial residuals don’t tell us anything because the variance of the residuals (remember binomial) depends on the probability of the binomial process AND on k (k/n binomial model). We need DHARMa to check the residuals:\n\nlibrary(DHARMa)\nsim = simulateResiduals(model2, plot = TRUE)\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\n\nResiduals look okayish for a k/n model.\nCheck misfits by plotting predictors against residuals:\n\nplotResiduals(sim, log(data_inf$swmo_prec+1))\n\n\n\nplotResiduals(sim, data_inf$sTemp_Water)\n\n\n\n\nswmo_prec doesn’t look good but this is caused by the many zeros in the variable\n\n\n\n\n\n8.6.2 Making spatial predictions with a logistic regression - Elephant SDM\nTo demonstrate how to fit a species distribution model and extrapolate it in space, let’s have a look at the elephant dataset, which is contained in EcoData\n\nlibrary(EcoData)\n?elephant\n\nThe object elephant contains two subdatasets\n\nelephant$occurenceData contains presence / absence data as well as bioclim variables (environmental predictors) for the African elephant\nelephant$predictionData data with environmental predictors for spatial predictions\n\nThe environmental data consists of 19 environmental variables, called bio1 through bio19, which are public and globally available bioclimatic variables (see https://www.worldclim.org/data/bioclim.html for a description of the variables). For example, bio1 is the mean annual temperature. No understanding of these variables is required for the task, the only difficulty is that many of them are highly correlated because they encode similar information (e.g. there are several temperature variables).\nThe goal of this exercise is to fit a logistic regression model based on the observed presence / absences, and then make new predictions of habitat suitability in space across Africa based on the fitted model. Thus, our workflow consists of two steps:\n\nbuilding and optimizing the predictive model, and\nusing the predictive model to make predictions on new data and visualizing the results.\n\nHere an example of how you could do this\nBuild predictive model:\n\nmodel = glm(Presence~bio1, data = elephant$occurenceData, family = binomial()) # minimum model\n\nTo check the predictive power of the model, you can either use\n\nAIC(model) \n\n[1] 3511.176\n\nlibrary(pROC)\nauc(elephant$occurenceData$Presence, predict(model, type = \"response\"))\n\nArea under the curve: 0.7053\n\n\nThe AUC is a common measure of goodness of fit for binary classification. However, it doesn’t penalize for complexity, as the AIC. Better to look at it under cross-validaton\n\n    library(boot)\n    res = cv.glm(elephant$occurenceData, model, cost = auc, K=5)\n\n\n    res$delta\n\n[1] 0.706149 0.706149\n\n\nCurrently, fit and validation AUC are nearly identical, because we have a very simple model. However, for more complex models, this could change.\n\nDrop some of the highly correlated variables (don’t use all of them).\nUse quadratic effects\nUse stepAIC or the dredge function to optimize the predictive model\n\nMake new predictions\nThe data for making spatial predictions is in elephant$predictionData. This new dataset is not a data.frame but a raster object, which is a special data class for spatial data. You can plot one of the predictors in the following way.\n\nlibrary(sp)\nlibrary(raster)\nplot(elephant$predictionData$bio1)\n\n\n\n\nAs our new_data object is not a typical data.frame, we are not using the standard predict function for a glm, which is ?predict.glm, but the predict function from the raster object (which internally transforms the new_data into a classical data.frame, pass then the data.frame to our model, and then transforms the output back to a raster object). Therefore, the syntax is slightly different to how we previously used predict().\n\npredictions =  predict(elephant$predictionData, model = model, type = \"response\")\nhead(as.data.frame(predictions))\n\n      layer\n1 0.1195466\n2 0.1148339\n3 0.1102836\n4 0.1080683\n5 0.1037547\n6 0.1016556\n\n\nThe advantage of the raster object is that we can directly use it to create a map (the raster object has coordinates for each observation):\n\nspplot(predictions, colorkey = list(space = \"left\") )\n\n\n\n\nTask: play around with the logistic regression to improve predictive accuracy. You can check predictive accuracy by looking at AIC or by taking out a bit of the data for validation, e.g. by AUC. When improving the predictive power of the model, does the map change?"
  },
  {
    "objectID": "2C-RandomEffects.html#motivation",
    "href": "2C-RandomEffects.html#motivation",
    "title": "4  Linear mixed models",
    "section": "4.1 Motivation",
    "text": "4.1 Motivation\nRandom effects are a very common addition to regression models that are used to account for grouping (categorical) variables such as subject, year, location. To explain the motivation for these models, as well as the basic syntax, we will use an example data set containing exam scores of 4,059 students from 65 schools in Inner London. This data set is located in the R package mlmRev.\n\nResponse: “normexam” (Normalized exam score).\nPredictor 1: “standLRT” (Standardised LR test score; Reading test taken when they were 11 years old).\nPredictor 2: “sex” of the student (F / M).\nGrouping factor: school\n\nIf we analyze this with a simple LM, we see that reading ability and sex have the expected effects on the exam score.\n\nlibrary(mlmRev)\nlibrary(effects)\n\nmod0 = lm(normexam ~ standLRT + sex , data = Exam)\nplot(allEffects(mod0))\n\n\n\n\nHowever, it is reasonable to assume that not all schools are equally good, which has two possible consequences:\n\nIf school identity correlates with sex or standLRT, school could be a confounder (more on this later)\nResiduals will be correlated in school, thus they are not iid (pseudo-replication)\n\nWe could solve this problem by fitting a different mean per school\n\nmod0b = lm(normexam ~ standLRT + sex + school , data = Exam)\n\nhowever, with 65 schools this would cost 64 degrees of freedom, which is a high cost just for correcting the possibility of confounding and a minor residual problem.\nThe solution: Mixed / random effect models. In a mixed model, we assume (differently to a fixed effect model) that the variance between schools originates from a normal distribution. There are different interpretations of a random effect (more on this later), but one interpretation is to view the random effect as an additional error, so that such a mixed model has two levels of error:\n\nFirst, the random effect, which is a normal “error” acting on an entire group of data points (in this case school).\nAnd second, the residual error, which is a normal error per observation and acts on top of the random effect\n\nBecause of this hierarchical structure, these models are also called “multi-level models” or “hierarchical models”.\n\n\n\n\n\n\nNote\n\n\n\nBecause grouping naturally occurs in any type of experimental data (batches, blocks, observer, subject etc.), random and mixed effect models are the de-facto default for most experimental data! Naming conventions:\n\nNo random effect = fixed effect model\nOnly random effects = random effect model\nRandom effects + fixed effects = mixed model\n\nMost models that are used in practice are mixed models"
  },
  {
    "objectID": "2D-NonlinearRegression.html",
    "href": "2D-NonlinearRegression.html",
    "title": "5  Nonlinear regressions",
    "section": "",
    "text": "s"
  },
  {
    "objectID": "5A-Summary.html#reminder-modelling-strategy",
    "href": "5A-Summary.html#reminder-modelling-strategy",
    "title": "12  Summary and concluding thoughts",
    "section": "12.1 Reminder: Modelling Strategy",
    "text": "12.1 Reminder: Modelling Strategy\n\n\n\n\n\nThings to note:\n\nFor an lm, the link function is the identity function.\nFixed effects \\(\\operatorname{f}(x)\\) can be either a polynomial \\(\\left( a \\cdot x = b \\right)\\) = linear regression, a nonlinear function = nonlinear regression, or a smooth spline = generalized additive model (GAM).\nRandom effects assume normal distribution for groups.\nRandom effects can also act on fixed effects (random slope).\nFor an lm with correlation structure, C is integrated in Dist. For all other GLMMs, there is another distribution, plus the additional multivariate normal on the linear predictor.\n\nStrategy for analysis:\n\nDefine formula via scientific questions + confounders.\nDefine type of GLM (lm, logistic, Poisson).\nBlocks in data -&gt; Random effects, start with random intercept.\n\nFit this base model, then do residual checks for\n\nWrong functional form -&gt; Change fitted function.\nWrong distribution-&gt; Transformation or GLM adjustment.\n(Over)dispersion -&gt; Variable dispersion GLM.\nHeteroskedasticity -&gt; Model dispersion.\nZero-inflation -&gt; Add ZIP term.\nCorrelation -&gt; Add correlation structure.\n\nAnd adjust the model accordingly.\nPackages to fit thes models (see Apppedix: syntax cheat sheet):\n\nbaseR: lm, glm.\nlme4: mixed models, lmer, glmer.\nmgcv: GAM.\nnlme: Variance and correlations structure modelling for linear (mixed) models, using gls + lme.\nglmmTMB: Generalized linear mixed models with variance / correlation modelling and zip term.\n\nAlternatively, you could move to Bayesian estimates with brms, which allows all that, easier inclusion of shrinkage, and p-value / df problems less visible"
  },
  {
    "objectID": "5A-Summary.html#experimental-design-and-power-analysis",
    "href": "5A-Summary.html#experimental-design-and-power-analysis",
    "title": "12  Summary and concluding thoughts",
    "section": "12.3 Experimental design and power analysis",
    "text": "12.3 Experimental design and power analysis\nAll we have learned in this course directly informs experimental and research design, may it be of controlled experiments, quasi-experiments or observational studies.\nAs a start, note our discussions of causality and model selection:\n\nFirst of all, research design needs a goal: do you want to design for prediction or (causal) inference\nIf you are after causal inference, try to control confounders, and if not, try to measure them. It is often possible even in observational settings to control confounders, e.g. by selecting the observations in a way that confouders are at the same value. This can be useful because, as we discussed, adjusting for collinear variable is possible, but cost power.\nIn experimental settings, blocked designs where blocks will be later treated as a RE are standard\n\nIf you have decided on the selection of variables, treat your analysis as fixed. Perform a pilot study and/or a power analysis for the entire study [things to be added]"
  },
  {
    "objectID": "5A-Summary.html#next-steps",
    "href": "5A-Summary.html#next-steps",
    "title": "12  Summary and concluding thoughts",
    "section": "12.4 Next steps",
    "text": "12.4 Next steps"
  },
  {
    "objectID": "2D-NonlinearRegression.html#what-is-a-nonlinear-regression",
    "href": "2D-NonlinearRegression.html#what-is-a-nonlinear-regression",
    "title": "5  Nonlinear regressions",
    "section": "5.1 What is a nonlinear regression?",
    "text": "5.1 What is a nonlinear regression?\nThe most important thing first: the distinction between a linear and a nonlinear regression is NOT if you fit a linear function. Quadratic, cubic and other polynomial functional forms are all linear regressions. A regression is called a “linear regression” if the function of the mean can be represented as a polynmial of the form\n\\[ y ~ a_0 + a_1 \\cdot x + a_2 \\cdot x^2 + ...\\]\nIf your model can be expressed in this way by transforming y or x (e.g. log(y), logit(y)) it is always preferable because fitting linear regressions is numerically easier and more stable, and consequently, there is much more support for linear regressions in R.\nHowever, there are a number of functions that cannot be expressed as a linear regression. Typical examples in ecology are growth or density dependence models, or in biology certain reaction or dose-response curves. In these an other case, you will have to run a nonlinear regression. The simplest case is a nonlinear regression with a normal residuals, which is known as nls(nonlinear least squares)."
  },
  {
    "objectID": "2D-NonlinearRegression.html#y-a_0-a_1-cdot-x-a_2-cdot-x2-...",
    "href": "2D-NonlinearRegression.html#y-a_0-a_1-cdot-x-a_2-cdot-x2-...",
    "title": "5  Nonlinear regressions",
    "section": "5.2 \\[ y ~ a_0 + a_1 \\cdot x + a_2 \\cdot x^2 + ...\\]",
    "text": "5.2 \\[ y ~ a_0 + a_1 \\cdot x + a_2 \\cdot x^2 + ...\\]\nIf your model can be expressed in this way by transforming y or x (e.g. log(y), logit(y)) it is always preferable because fitting linear regressions is numerically easier and more stable, and consequently, there is much more support for linear regressions in R.\nHowever, there are a number of functions that cannot be expressed as a linear regression. Typical examples in ecology are growth or density dependence models, or in biology certain reaction or dose-response curves. In these an other case, you will have to run a nonlinear regression. The simplest case is a nonlinear regression with a normal residuals, which is known as nls(nonlinear least squares)."
  },
  {
    "objectID": "2D-NonlinearRegression.html#fitting-a-nonlinear-regression-using-nls",
    "href": "2D-NonlinearRegression.html#fitting-a-nonlinear-regression-using-nls",
    "title": "5  Nonlinear regressions",
    "section": "5.2 Fitting a nonlinear regression using nls",
    "text": "5.2 Fitting a nonlinear regression using nls\nHere, I will use the Michaelis-Menten model as an example. In this example, we are interested in the development of a reaction rate over time. The data we have are measurements of the reaction rate and substrate concentration\n\nlibrary(nlstools)\n\ndata(vmkm)\nplot(vmkm)\n\n\n\n\nFrom the MM model, we can derive the evolution of the reaction rate (v) as a function of the concentration of substrate (S) (For details, see ?nlstoools::michaelismodels ). The function we want to fit is of the form:\n\\[ v = \\frac{S}{S + K_m} \\cdot V_{max}\\]\nThe base package to do this is the nls package, which allows you to specify\n\nmm &lt;- nls(v ~ S/(S + Km) * Vmax,data = vmkm, \n          start = list(Km=1,Vmax=1)) \n\nAlternatively, the helpful nlstools package already provides many functions that you may want to fit, including this particular function under the name ‘michaelis’. We have already loaded this package, so you could can just write\n\nmm &lt;- nls(michaelis,data = vmkm, start = list(Km=1,Vmax=1)) \n\nNote that in unlike in a lm, the nls function requires you to provide starting values for the parameters. This helps the optimizer to find the MLE for the model, which can sometimes be problematic. In this case, you may also want to consider changing the algorithm and some of its settings (via algorithm and control, see ?nls).\nThe model provides a standard summary table.\n\nsummary(mm)\n\n\nFormula: v ~ S/(S + Km) * Vmax\n\nParameters:\n     Estimate Std. Error t value Pr(&gt;|t|)    \nKm     2.5861     0.3704   6.981 8.94e-07 ***\nVmax   1.5460     0.1454  10.635 1.11e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02572 on 20 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 2.51e-06\n\n\nThe nlstools package has a bit extended summary table that provides a few useful additional information\n\noverview(mm)\n\n\n------\nFormula: v ~ S/(S + Km) * Vmax\n\nParameters:\n     Estimate Std. Error t value Pr(&gt;|t|)    \nKm     2.5861     0.3704   6.981 8.94e-07 ***\nVmax   1.5460     0.1454  10.635 1.11e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02572 on 20 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 2.51e-06\n\n------\nResidual sum of squares: 0.0132 \n\n------\nt-based confidence interval:\n         2.5%    97.5%\nKm   1.813418 3.358860\nVmax 1.242790 1.849303\n\n------\nCorrelation matrix:\n            Km      Vmax\nKm   1.0000000 0.9917486\nVmax 0.9917486 1.0000000\n\n\nA plot can be generated with\n\nnlstools::plotfit(mm, smooth = TRUE)"
  },
  {
    "objectID": "2D-NonlinearRegression.html#v-fracss-k_m-cdot-v_max",
    "href": "2D-NonlinearRegression.html#v-fracss-k_m-cdot-v_max",
    "title": "5  Nonlinear regressions",
    "section": "5.4 \\[ v = \\frac{S}{S + K_m} \\cdot V_{max}\\]",
    "text": "5.4 \\[ v = \\frac{S}{S + K_m} \\cdot V_{max}\\]\nThe base package to do this is the nls package, which allows you to specify\n\nmm &lt;- nls(v ~ S/(S + Km) * Vmax,data = vmkm, start = list(Km=1,Vmax=1)) \n\nAlternatively, the helpful nlstools package already provides many functions that you may want to fit, including this particular function under the name ‘michaelis’. We have already loaded this package, so you could can just write\n\nmm &lt;- nls(michaelis,data = vmkm, start = list(Km=1,Vmax=1)) \n\nNote that in unlike in a lm, the nls function requires you to provide starting values for the parameters. This helps the optimizer to find the MLE for the model, which can sometimes be problematic. In this case, you may also want to consider changing the algorithm and some of its settings (via algorithm and control, see ?nls).\nThe model provides a standard summary table.\n\nsummary(mm)\n\n\nFormula: v ~ S/(S + Km) * Vmax\n\nParameters:\n     Estimate Std. Error t value Pr(&gt;|t|)    \nKm     2.5861     0.3704   6.981 8.94e-07 ***\nVmax   1.5460     0.1454  10.635 1.11e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02572 on 20 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 2.51e-06\n\n\nThe nlstools package has a bit extended summary table that provides a few useful additional information\n\noverview(mm)\n\n\n------\nFormula: v ~ S/(S + Km) * Vmax\n\nParameters:\n     Estimate Std. Error t value Pr(&gt;|t|)    \nKm     2.5861     0.3704   6.981 8.94e-07 ***\nVmax   1.5460     0.1454  10.635 1.11e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02572 on 20 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 2.51e-06\n\n------\nResidual sum of squares: 0.0132 \n\n------\nt-based confidence interval:\n         2.5%    97.5%\nKm   1.813418 3.358860\nVmax 1.242790 1.849303\n\n------\nCorrelation matrix:\n            Km      Vmax\nKm   1.0000000 0.9917486\nVmax 0.9917486 1.0000000\n\n\nA plot can be generated with\n\nnlstools::plotfit(mm, smooth = TRUE)\n\n\n\n\nResidual plots from the nlstools package\n\nres &lt;- nlsResiduals(mm)\nplot(res, which = 0)"
  },
  {
    "objectID": "2D-NonlinearRegression.html#interactions-in-nls",
    "href": "2D-NonlinearRegression.html#interactions-in-nls",
    "title": "5  Nonlinear regressions",
    "section": "5.6 Interactions in NLS",
    "text": "5.6 Interactions in NLS\nWhat about interactions, i.e. one variable influencing the parameter value of another variable. Numeric interactions in nls will just be coded as multiplications in the formula. Factor interactions are not explicitly supported by nls but you can create them via dummy variables (i.e. introduce a variable per factor level and put a 1 if the observation corresponds to the factor level, zero otherwise).\nHere I created an exampel with the previous dataset, which I randomly split between wild type and mutant observations. I want to know if\n\nvmkm1 = vmkm\nvmkm1$type = sample(c(0,1), nrow(vmkm), replace = T) # wild type and mutant\nhead(vmkm1)\n\n    S    v type\n1 0.3 0.17    1\n2 0.3 0.15    1\n3 0.4 0.21    1\n4 0.4 0.23    1\n5 0.5 0.26    1\n6 0.5 0.23    1\n\n\nNow I can fit the model\n\nmm1 &lt;- nls(v ~ S/(S + Km + type*DeltaMutantKm) * Vmax,data = vmkm1, start = list(Km=1,Vmax=1, DeltaMutantKm = 0)) \nsummary(mm1)\n\n\nFormula: v ~ S/(S + Km + type * DeltaMutantKm) * Vmax\n\nParameters:\n              Estimate Std. Error t value Pr(&gt;|t|)    \nKm             2.44124    0.37664   6.482 3.28e-06 ***\nVmax           1.50852    0.14329  10.528 2.28e-09 ***\nDeltaMutantKm  0.09166    0.09443   0.971    0.344    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02579 on 19 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 1.958e-06\n\n\nWe see here that there is no significant difference between wild type and mutant, which is reasonable as I just randomly put the observations in one or the other type."
  },
  {
    "objectID": "2D-NonlinearRegression.html#uncertainties-and-cis-on-parameters",
    "href": "2D-NonlinearRegression.html#uncertainties-and-cis-on-parameters",
    "title": "5  Nonlinear regressions",
    "section": "5.4 Uncertainties and CIs on parameters",
    "text": "5.4 Uncertainties and CIs on parameters\nIn nonlinear models, the MLE curvature will often not be approximately multivariate normal, which means that approximation errors can be large when calculating CIs based on the the se. The nlstools package allows you to calculate CIs based the normal approximation (asymptotic) or on profiling.\n\nnlstools::confint2(mm)\n\n        2.5 %   97.5 %\nKm   1.813418 3.358860\nVmax 1.242790 1.849303\n\n\nWe can also visualize the likelihood contour\n\ncont &lt;- nlsContourRSS(mm)\n\n\b\b0%\b\b\b100%\u0007\n RSS contour surface array returned \n\nplot(cont)\n\n\n\n\nAlternatively (if you assume the normal approximation is bad), you can bootstrap the model, which can also be used to generate CIs on predictions\n\nmmboot &lt;- nlstools::nlsBoot(mm, niter = 200)\nplot(mmboot)\n\n\n\nsummary(mmboot)\n\n\n------\nBootstrap statistics\n     Estimate Std. error\nKm   2.643809  0.3941447\nVmax 1.567486  0.1540766\n\n------\nMedian of bootstrap estimates and percentile confidence intervals\n       Median     2.5%    97.5%\nKm   2.597163 2.042456 3.577164\nVmax 1.547605 1.337787 1.942798\n\n\nThe most complete solution to calculating confidence intervals for complex models with non-normal likelihood surfaces would be a Bayesian approach, for example by using the BayesianTools R package (see the package vignette about how to do this)."
  },
  {
    "objectID": "2D-NonlinearRegression.html#uncertainties-and-cis-on-predictions",
    "href": "2D-NonlinearRegression.html#uncertainties-and-cis-on-predictions",
    "title": "5  Nonlinear regressions",
    "section": "5.5 Uncertainties and CIs on predictions",
    "text": "5.5 Uncertainties and CIs on predictions\nIn principle, predictions on an nls object (also for new data) can be made with the predict.nls function\n\npredict(mm)\n\n [1] 0.1591553 0.1591553 0.2051867 0.2051867 0.2521110 0.2521110 0.2930050\n [8] 0.2930050 0.3624145 0.3675233 0.4280083 0.4337041 0.4867385 0.4928727\n[15] 0.5396288 0.5396288 0.5942039 0.5942039 0.6379213 0.6379213 0.6708403\n[22] 0.6708403\n\n\nThe problem in calculating CIs on model predictions in nls is that a) likelihood surface (= uncertainty) of the parameters may not be multivariate normal b) the model itself is nonlinear, so it’s not straightforward to propagate parameter uncertainty to predictions or derived quantities.\nProblem b) (propagation) can be solved using the propagate package, which propagates uncertainties based on MC samples or Taylor expansion\n\nlibrary(propagate)\n\nLoading required package: MASS\n\n\nLoading required package: tmvtnorm\n\n\nLoading required package: mvtnorm\n\n\nLoading required package: Matrix\n\n\nLoading required package: stats4\n\n\nLoading required package: gmm\n\n\nLoading required package: sandwich\n\n\nLoading required package: Rcpp\n\n\nLoading required package: ff\n\n\nLoading required package: bit\n\n\n\nAttaching package: 'bit'\n\n\nThe following object is masked from 'package:base':\n\n    xor\n\n\nAttaching package ff\n\n\n- getOption(\"fftempdir\")==\"/var/folders/cn/9js1nnws5fjcqb6m0g1p5_rm0000gp/T//Rtmpntw5yw/ff\"\n\n\n- getOption(\"ffextension\")==\"ff\"\n\n\n- getOption(\"ffdrop\")==TRUE\n\n\n- getOption(\"fffinonexit\")==TRUE\n\n\n- getOption(\"ffpagesize\")==65536\n\n\n- getOption(\"ffcaching\")==\"mmnoflush\"  -- consider \"ffeachflush\" if your system stalls on large writes\n\n\n- getOption(\"ffbatchbytes\")==16777216 -- consider a different value for tuning your system\n\n\n- getOption(\"ffmaxbytes\")==536870912 -- consider a different value for tuning your system\n\n\n\nAttaching package: 'ff'\n\n\nThe following objects are masked from 'package:utils':\n\n    write.csv, write.csv2\n\n\nThe following objects are masked from 'package:base':\n\n    is.factor, is.ordered\n\n\nLoading required package: minpack.lm\n\nout = propagate::predictNLS(mm, nsim = 10000) # nsim must be set much higher for production\n\npredictNLS: Propagating predictor value #1...\n\n\npredictNLS: Propagating predictor value #2...\n\n\npredictNLS: Propagating predictor value #3...\n\n\npredictNLS: Propagating predictor value #4...\n\n\npredictNLS: Propagating predictor value #5...\n\n\npredictNLS: Propagating predictor value #6...\n\n\npredictNLS: Propagating predictor value #7...\n\n\npredictNLS: Propagating predictor value #8...\n\n\npredictNLS: Propagating predictor value #9...\n\n\npredictNLS: Propagating predictor value #10...\n\n\npredictNLS: Propagating predictor value #11...\n\n\npredictNLS: Propagating predictor value #12...\n\n\npredictNLS: Propagating predictor value #13...\n\n\npredictNLS: Propagating predictor value #14...\n\n\npredictNLS: Propagating predictor value #15...\n\n\npredictNLS: Propagating predictor value #16...\n\n\npredictNLS: Propagating predictor value #17...\n\n\npredictNLS: Propagating predictor value #18...\n\n\npredictNLS: Propagating predictor value #19...\n\n\npredictNLS: Propagating predictor value #20...\n\n\npredictNLS: Propagating predictor value #21...\n\n\npredictNLS: Propagating predictor value #22...\n\n\nThe nonparametric alternative, which also adresses problem A and can be more easily applied also to derived quantities such as an LD50 value is to bootstrap the entire pipeline. In this case, you should use the general boot function described at the end of section"
  },
  {
    "objectID": "2D-NonlinearRegression.html#extending-nls-to-mixed-effects",
    "href": "2D-NonlinearRegression.html#extending-nls-to-mixed-effects",
    "title": "5  Nonlinear regressions",
    "section": "5.6 Extending nls to mixed effects",
    "text": "5.6 Extending nls to mixed effects\nIf you want mixed effects to your model (which can often make sense, e.g. consider you would have data as above, but from 3 different experiments), we can use the nlme function in the package nlme. To create a simple example, I add a variable group to the data we used before, assuming that the data came out of 3 different experiments\n\nlibrary(nlme)\n\nvmkm2 = vmkm\nvmkm2$group = sample(c(1,2,3), nrow(vmkm), replace = T)\n\nvmkm2 &lt;- groupedData(v ~ 1|group, vmkm2) # transferring to a grouped data object recommended for nlme\n\nWe now want to assume that the parameter Km may be different in the 3 experiments. In this case, we use nlme, provide the data, formula and start values as before, and we can decide which of the parameters should be treated as fixed and which as random effects. Note that random effects in this case are effectively random slopes, as there is no\n\nmm2 = nlme(model = michaelis,data = vmkm2, \n           fixed = Vmax + Km ~ 1, random = Km ~ 1|group, \n          start = c(Km=1, Vmax=1))\n\nsummary(mm2)\n\nNonlinear mixed-effects model fit by maximum likelihood\n  Model: michaelis \n  Data: vmkm2 \n        AIC       BIC   logLik\n  -92.71828 -88.35411 50.35914\n\nRandom effects:\n Formula: Km ~ 1 | group\n                  Km   Residual\nStdDev: 7.693878e-06 0.02452676\n\nFixed effects:  Vmax + Km ~ 1 \n        Value Std.Error DF   t-value p-value\nVmax 1.546013 0.1453726 18 10.634833       0\nKm   2.586054 0.3704204 18  6.981403       0\n Correlation: \n   Vmax \nKm 0.992\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-1.84557906 -0.38103073 -0.01901336  0.57185719  2.39956859 \n\nNumber of Observations: 22\nNumber of Groups: 3"
  },
  {
    "objectID": "2D-NonlinearRegression.html#residual-checks",
    "href": "2D-NonlinearRegression.html#residual-checks",
    "title": "5  Nonlinear regressions",
    "section": "5.3 Residual checks",
    "text": "5.3 Residual checks\nResidual plots can be created using the nlstools package\n\nres &lt;- nlsResiduals(mm)\nplot(res, which = 0)\n\n\n\n\nOf course, all discussions about residuals from the chapter on linear regressions also applies here, so we are checking for iid normal residuals."
  },
  {
    "objectID": "2D-NonlinearRegression.html#uncertainties-and-cis-on-predictions-and-derived-quantities",
    "href": "2D-NonlinearRegression.html#uncertainties-and-cis-on-predictions-and-derived-quantities",
    "title": "5  Nonlinear regressions",
    "section": "5.5 Uncertainties and CIs on predictions and derived quantities",
    "text": "5.5 Uncertainties and CIs on predictions and derived quantities\nIn principle, predictions on an nls object (also for new data) can be made with the predict.nls function\n\npredict(mm)\n\n [1] 0.1607039 0.1607039 0.2070964 0.2070964 0.2504823 0.2504823 0.2911448\n [8] 0.2911448 0.3652647 0.3652647 0.4311173 0.4311173 0.4900126 0.4900126\n[15] 0.5429979 0.5429979 0.5909202 0.5909202 0.6344723 0.6344723 0.6742257\n[22] 0.6742257\n\n\nhowever, this function does not calculate confidence intervals. You can add those using the propagate package, which propagates uncertainties through the nonlinear model based on MC samples or Taylor expansion based on the normal approximation of the likelihood.\n\nlibrary(propagate)\nout = propagate::predictNLS(mm, nsim = 10000) # nsim must be set much higher for production\n\nFor complex nonlinear models with few data, the normal approximation will not be a good one. In this case, you can resort to the bootstrap, using the Igeneral boot function described at the end of the linear regression chapter. This can also be applied to any derived quantities (i.e. quantities that are calculated from a function of the model parameters such as an LD50 value). However, note that wherever possible, it is easier to fit those derived quantities by reshuffling the regression formula.\nThe most complete solution to calculating confidence intervals and uncertainties and predictions on derived quantities would be to fit the model fully Bayesian. This can be done, for example, using the BayesianTools R package (see the package vignette about how to do this)."
  },
  {
    "objectID": "2D-NonlinearRegression.html#mixed-effects-heteroskedasticity-and-correlations",
    "href": "2D-NonlinearRegression.html#mixed-effects-heteroskedasticity-and-correlations",
    "title": "5  Nonlinear regressions",
    "section": "5.8 Mixed effects, heteroskedasticity and correlations",
    "text": "5.8 Mixed effects, heteroskedasticity and correlations\nIf you want mixed effects to your model (which can often make sense, e.g. consider you would have data as above, but from 3 different experiments), we can use the nlme function in the package nlme. To create a simple example, I add a variable group to the data we used before, assuming that the data came out of 3 different experiments\n\nlibrary(nlme)\nvmkm2 = vmkm\nvmkm2$group = sample(c(1,2,3), nrow(vmkm), replace = T)\nvmkm2 &lt;- groupedData(v ~ 1|group, vmkm2) # transferring to a grouped data object recommended for nlme\nhead(vmkm2)\n\nGrouped Data: v ~ 1 | group\n    S    v group\n1 0.3 0.17     3\n2 0.3 0.15     1\n3 0.4 0.21     2\n4 0.4 0.23     2\n5 0.5 0.26     1\n6 0.5 0.23     2\n\n\nWe now want to assume that the parameter Km may be different in the 3 experiments. In this case, we use nlme, provide the data, formula and start values as before, and we can decide which of the parameters should be treated as fixed and which as random effects. Note that random effects in this case are effectively random slopes, as there is no\n\nmm2 = nlme(model = michaelis,data = vmkm2, \n           fixed = Vmax + Km ~ 1, random = Km ~ 1|group, \n          start = c(Km=1, Vmax=1))\n\nsummary(mm2)\n\nNonlinear mixed-effects model fit by maximum likelihood\n  Model: michaelis \n  Data: vmkm2 \n        AIC       BIC   logLik\n  -92.71828 -88.35411 50.35914\n\nRandom effects:\n Formula: Km ~ 1 | group\n                  Km   Residual\nStdDev: 2.608507e-06 0.02452676\n\nFixed effects:  Vmax + Km ~ 1 \n        Value Std.Error DF   t-value p-value\nVmax 1.546013 0.1453726 18 10.634832       0\nKm   2.586054 0.3704204 18  6.981403       0\n Correlation: \n   Vmax \nKm 0.992\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-1.84557903 -0.38103070 -0.01901336  0.57185718  2.39956863 \n\nNumber of Observations: 22\nNumber of Groups: 3"
  },
  {
    "objectID": "2D-NonlinearRegression.html#model-selection",
    "href": "2D-NonlinearRegression.html#model-selection",
    "title": "5  Nonlinear regressions",
    "section": "5.7 Model selection",
    "text": "5.7 Model selection\nYou can use all model selection methods described in the next chapter. If you want to compare, for example, if the model with the interaction as fitted above is better than the null model that doesn’t distinguis between types, we can either to a LRT\n\nanova(mm, mm1)\n\nAnalysis of Variance Table\n\nModel 1: v ~ S/(S + Km) * Vmax\nModel 2: v ~ S/(S + Km + type * DeltaMutantKm) * Vmax\n  Res.Df Res.Sum Sq Df     Sum Sq F value Pr(&gt;F)\n1     20   0.013234                             \n2     19   0.012635  1 0.00059899  0.9007 0.3545\n\n\nor an AIC comparison\n\nAIC(mm)\n\n[1] -94.71828\n\nAIC(mm1)\n\n[1] -93.73724\n\n\nBoth of which tell us the same thing our previous result told us, which is that the model that fits separate Km parameters for wild type and mutant is not bettr than the model that assumes that they have the same functional relationship."
  }
]